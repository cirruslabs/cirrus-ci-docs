{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"api/","title":"Cirrus CI API","text":"<p>Cirrus CI exposes GraphQL API for integrators to use through <code>https://api.cirrus-ci.com/graphql</code> endpoint. Please check Cirrus CI GraphQL Schema for a full list of  available types and methods. Or check built-in interactive GraphQL Explorer. Here is an example of how to get a build for a particular SHA of a given repository:</p> <pre><code>curl -X POST --data \\\n'{\n  \"query\": \"query BuildBySHAQuery($owner: String!, $name: String!, $SHA: String) { searchBuilds(repositoryOwner: $owner, repositoryName: $name, SHA: $SHA) { id } }\",\n  \"variables\": {\n    \"owner\": \"ORGANIZATION\",\n    \"name\": \"REPOSITORY NAME\",\n    \"SHA\": \"SOME SHA\"\n  }\n}' \\\nhttps://api.cirrus-ci.com/graphql | python -m json.tool\n</code></pre>"},{"location":"api/#authorization","title":"Authorization","text":"<p>In order for a tool to access Cirrus CI API, an organization admin should generate an access token through Cirrus CI settings page for a corresponding organization. Here is a direct link to the settings page: <code>https://cirrus-ci.com/settings/github/&lt;ORGANIZATION&gt;</code>. An access token will allow full write and read access to both public and private repositories of your organization on Cirrus CI: it will be possible to create new builds and perform any other GraphQL mutations. It is also possible to generate scoped access tokens for a subset of repositories to perform read or write operations on the same settings page.</p> <p>Note that if you only need read access to public repositories of your organization you can skip this step and don't provide <code>Authorization</code> header.</p> <p>Once an access token is generated and securely stored, it can be used to authorize API requests by setting <code>Authorization</code> header to <code>Bearer $TOKEN</code>.</p> <p>User API Token Permission Scope</p> <p>It is also possible to generate API tokens for personal accounts but they will be scoped only to access personal public and private repositories of a particular user. It won't be possible to access private repositories of an organization, even if they have access.</p>"},{"location":"api/#webhooks","title":"WebHooks","text":""},{"location":"api/#builds-and-tasks-webhooks","title":"Builds and Tasks WebHooks","text":"<p>It is possible to subscribe for updates of builds and tasks. If a WebHook URL is configured on Cirrus CI Settings page for  an organization, Cirrus CI will try to <code>POST</code> a webhook event payload to this URL.</p> <p><code>POST</code> request will contain <code>X-Cirrus-Event</code> header to specify if the update was made to a <code>build</code> or a <code>task</code>. The event  payload itself is pretty basic:</p> <pre><code>{\n  \"action\": \"created\" | \"updated\",\n  \"old_status\": \"FAILED\", # optional field for \"updated\" action in case a task or a build transitioned from one status to another\n  \"data\": ...\n}\n</code></pre> <p><code>data</code> field will be populated by executing the following GraphQL query:</p> <pre><code>repository(id: $repositoryId) {\n  id\n  owner\n  name\n  isPrivate\n}\nbuild(id: $buildId) {\n  id\n  branch\n  pullRequest\n  pullRequestDraft\n  durationInSeconds\n  changeIdInRepo\n  changeTimestamp\n  status\n}\ntask(id: $taskId) {\n  id\n  name\n  nameAlias\n  status\n  statusTimestamp\n  creationTimestamp\n  durationInSeconds\n  uniqueLabels\n  automaticReRun\n  automaticallyReRunnable\n  manualRerunCount\n  notifications {\n    level\n    message\n    link\n  }\n}\n</code></pre>"},{"location":"api/#audit-events-webhooks","title":"Audit Events WebHooks","text":"<p>In addition to updates to builds and tasks, Cirrus CI will also send <code>audit_event</code> events to the configured WebHook URL. <code>action</code> for these audit events will be always <code>\"create\"</code> and the <code>data</code> field will contain the following GraphQL fragment for the particular audit event:</p> <pre><code>fragment AuditEventWebhookPayload on AuditEventType {\n  id\n  type\n  timestamp\n  data\n  actor {\n    id\n  }\n  repository {\n    id\n    owner\n    name\n    isPrivate\n  }\n}\n</code></pre> <p>Here is an example of an audit event for when a user re-ran a task with an attached Terminal within your organization:</p> <pre><code>{\n  \"action\": \"created\" | \"updated\",\n  \"data\": {\n    \"id\": \"uuid-uuid-uuid-uuid\",\n    \"type\": \"graphql.mutation\",\n    \"timestamp\": ...,\n    \"data\": \"{\\n  \\\"mutationName\\\": \\\"TaskReRun\\\",\\n  \\\"taskId\\\": \\\"1\\\",\\n  \\\"attachTerminal\\\": true,\\n  \\\"newTaskId\\\": 2\\n}\",\n    \"actor\": {\n      \"id\": \"...\" // internal to Cirrus CI user Id\n    },\n    \"repository\": {\n      \"id\": \"123\",\n      \"owner\": \"ACME\",\n      \"name\": \"super-project\",\n      \"isPrivate\": true\n    }\n  }\n}\n</code></pre> <p>At the moment only two types of audit events are supported:</p> <ul> <li><code>graphql.mutation</code> - any GraphQL mutation was performed for your organization (re-running tasks, creating an encrypted variable, etc.).</li> <li><code>user.authenticated</code> - a user that has access to your organization logs in to Cirrus CI.</li> </ul>"},{"location":"api/#securing-webhooks","title":"Securing WebHooks","text":"<p>Imagine you've been given a <code>https://example.com/webhook</code> endpoint by your administrator, and for some reason there's no easy way to change that. This kind of URL is easily discoverable on the internet, and an attacker can take advantage of this by sending requests to this URL, thus pretending to be the Cirrus CI.</p> <p>To avoid such situations, set the secret token in the repository settings, and then validate the <code>X-Cirrus-Signature</code> for each WebHook request.</p> <p>Once configured, the secret token and the request's body are fed into the HMAC algorithm to generate the <code>X-Cirrus-Signature</code> for each request coming from the Cirrus CI.</p> <p>Missing X-Cirrus-Signature header</p> <p>When secret token is configured in the repository settings, all WebHook requests will contain the <code>X-Cirrus-Signature-Header</code>. Make sure to assert the presence of <code>X-Cirrus-Signature-Header</code> header and correctness of its value in your validation code.</p> <p>Using HMAC is pretty straightforward in many languages, here's an example of how to validate the <code>X-Cirrus-Signature</code> using Python's <code>hmac</code> module:</p> <pre><code>import hmac\n\ndef is_signature_valid(secret_token: bytes, body: bytes, x_cirrus_signature: str) -&gt; bool:\n    expected_signature = hmac.new(secret_token, body, \"sha256\").hexdigest()\n\n    return hmac.compare_digest(expected_signature, x_cirrus_signature)\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Here you can find example configurations per different programming languages/frameworks.</p>"},{"location":"examples/#android","title":"Android","text":"<p>Cirrus CI has a set of Docker images ready for Android development. If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI. For those images <code>.cirrus.yml</code> configuration file can look like:</p> <pre><code>container:\n  image: ghcr.io/cirruslabs/android-sdk:30\n\ncheck_android_task:\n  check_script: ./gradlew check connectedCheck\n</code></pre> <p>Or like this if a running hardware accelerated emulator is needed for the tests:</p> <pre><code>container:\n  image: ghcr.io/cirruslabs/android-sdk:30\n  cpu: 4\n  memory: 12G\n  kvm: true\n\ncheck_android_task:\n  install_emulator_script:\n    sdkmanager --install \"system-images;android-30;google_apis;x86\"\n  create_avd_script:\n    echo no | avdmanager create avd --force\n      -n emulator\n      -k \"system-images;android-30;google_apis;x86\"\n  start_avd_background_script:\n    $ANDROID_HOME/emulator/emulator\n      -avd emulator\n      -no-audio\n      -no-boot-anim\n      -gpu swiftshader_indirect\n      -no-snapshot\n      -no-window\n  # assemble while emulator is starting\n  assemble_instrumented_tests_script:\n    ./gradlew assembleDebugAndroidTest\n  wait_for_avd_script:\n    adb wait-for-device shell 'while [[ -z $(getprop sys.boot_completed) ]]; do sleep 3; done; input keyevent 82'\n  check_script: ./gradlew check connectedCheck\n</code></pre> <p>Info</p> <p>Please don't forget to setup Remote Build Cache for your Gradle project.</p>"},{"location":"examples/#android-lint","title":"Android Lint","text":"<p>The Cirrus CI annotator supports providing inline reports on PRs and can parse Android Lint reports. Here is an example of an Android Lint task that you can add to your <code>.cirrus.yml</code>:</p> <pre><code>task:\n  name: Android Lint\n  lint_script: ./gradlew lintDebug\n  always:\n    android-lint_artifacts:\n      path: \"**/reports/lint-results-debug.xml\"\n      type: text/xml\n      format: android-lint\n</code></pre>"},{"location":"examples/#bazel","title":"Bazel","text":"<p>Bazel Team provides a set of official Docker images with Bazel pre-installed. Here is an example of how <code>.cirrus.yml</code> can look like for Bazel:</p> amd64arm64 <pre><code>container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script: bazel build //...\n</code></pre> <pre><code>arm_container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script: bazel build //...\n</code></pre> <p>If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI.</p>"},{"location":"examples/#remote-cache","title":"Remote Cache","text":"<p>Cirrus CI has built-in HTTP Cache which is compatible with Bazel's remote cache.</p> <p>Here is an example of how Cirrus CI HTTP Cache can be used with Bazel:</p> amd64arm64 <pre><code>container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script:\n    bazel build\n      --spawn_strategy=sandboxed\n      --strategy=Javac=sandboxed\n      --genrule_strategy=sandboxed\n      --remote_http_cache=http://$CIRRUS_HTTP_CACHE_HOST\n      //...\n</code></pre> <pre><code>arm_container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script:\n    bazel build\n      --spawn_strategy=sandboxed\n      --strategy=Javac=sandboxed\n      --genrule_strategy=sandboxed\n      --remote_http_cache=http://$CIRRUS_HTTP_CACHE_HOST\n      //...\n</code></pre>"},{"location":"examples/#c","title":"C++","text":"<p>Official GCC Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64 <pre><code>container:\n  image: gcc:latest\ntask:\n  tests_script: make tests\n</code></pre> <pre><code>arm_container:\n  image: gcc:latest\ntask:\n  tests_script: make tests\n</code></pre>"},{"location":"examples/#crystal","title":"Crystal","text":"<p>Official Crystal Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that caches dependencies and runs tests:</p> <pre><code>container:\n  image: crystallang/crystal:latest\n\nspec_task:\n  shard_cache:\n    fingerprint_script: cat shard.lock\n    populate_script: shards install\n    folder: lib\n  spec_script: crystal spec\n</code></pre>"},{"location":"examples/#elixir","title":"Elixir","text":"<p>Official Elixir Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64 <pre><code>test_task:\n  container:\n    image: elixir:latest\n  mix_cache:\n    folder: deps\n    fingerprint_script: cat mix.lock\n    populate_script: mix deps.get\n  compile_script: mix compile\n  test_script: mix test\n</code></pre> <pre><code>test_task:\n  arm_container:\n    image: elixir:latest\n  mix_cache:\n    folder: deps\n    fingerprint_script: cat mix.lock\n    populate_script: mix deps.get\n  compile_script: mix compile\n  test_script: mix test\n</code></pre>"},{"location":"examples/#erlang","title":"Erlang","text":"<p>Official Erlang Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64 <pre><code>test_task:\n  container:\n    image: erlang:latest\n  rebar3_cache:\n    folder: _build\n    fingerprint_script: cat rebar.lock\n    populate_script: rebar3 compile --deps_only\n  compile_script: rebar3 compile\n  test_script: rebar3 ct\n</code></pre> <pre><code>test_task:\n  arm_container:\n    image: erlang:latest\n  rebar3_cache:\n    folder: _build\n    fingerprint_script: cat rebar.lock\n    populate_script: rebar3 compile --deps_only\n  compile_script: rebar3 compile\n  test_script: rebar3 ct\n</code></pre>"},{"location":"examples/#flutter","title":"Flutter","text":"<p>Cirrus CI provides a set of Docker images with Flutter and Dart SDK pre-installed. Here is an example of how <code>.cirrus.yml</code> can be written for Flutter:</p> <pre><code>container:\n  image: ghcr.io/cirruslabs/flutter:latest\n\ntest_task:\n  pub_cache:\n    folder: ~/.pub-cache\n  test_script: flutter test --machine &gt; report.json\n  always:\n    report_artifacts:\n      path: report.json\n      format: flutter\n</code></pre> <p>If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI.</p>"},{"location":"examples/#flutter-web","title":"Flutter Web","text":"<p>Our Docker images with Flutter and Dart SDK pre-installed have special <code>*-web</code> tags with Chromium pre-installed. You can use these tags to run Flutter Web</p> <p>First define a new <code>chromium</code> platform in your <code>dart_test.yaml</code>:</p> <pre><code>define_platforms:\n  chromium:\n    name: Chromium\n    extends: chrome\n    settings:\n      arguments: --no-sandbox\n      executable:\n        linux: chromium\n</code></pre> <p>Now you'll be able to run tests targeting web via <code>pub run test test -p chromium</code></p>"},{"location":"examples/#go","title":"Go","text":"<p>The best way to test Go projects is by using official Go Docker images. Here is an example of how <code>.cirrus.yml</code> can look like for a project using Go Modules:</p> amd64arm64 <pre><code>container:\n  image: golang:latest\n\ntest_task:\n  modules_cache:\n    fingerprint_script: cat go.sum\n    folder: $GOPATH/pkg/mod\n  get_script: go get ./...\n  build_script: go build ./...\n  test_script: go test ./...\n</code></pre> <pre><code>arm_container:\n  image: golang:latest\n\ntest_task:\n  modules_cache:\n    fingerprint_script: cat go.sum\n    folder: $GOPATH/pkg/mod\n  get_script: go get ./...\n  build_script: go build ./...\n  test_script: go test ./...\n</code></pre>"},{"location":"examples/#golangci-lint","title":"GolangCI Lint","text":"<p>We highly recommend to configure some sort of linting for your Go project. One of the options is GolangCI Lint. The Cirrus CI annotator supports providing inline reports on PRs and can parse GolangCI Lint reports. Here is an example of a GolangCI Lint task that you can add to your <code>.cirrus.yml</code>:</p> amd64arm64 <pre><code>task:\n  name: GolangCI Lint\n  container:\n    image: golangci/golangci-lint:latest\n  run_script: golangci-lint run -v --out-format json &gt; lint-report.json\n  always:\n    golangci_artifacts:\n      path: lint-report.json\n      type: text/json\n      format: golangci\n</code></pre> <pre><code>task:\n  name: GolangCI Lint\n  arm_container:\n    image: golangci/golangci-lint:latest\n  run_script: golangci-lint run -v --out-format json &gt; lint-report.json\n  always:\n    golangci_artifacts:\n      path: lint-report.json\n      type: text/json\n      format: golangci\n</code></pre>"},{"location":"examples/#gradle","title":"Gradle","text":"<p>We recommend use of the official Gradle Docker containers since they have Gradle specific configurations already set up. For example, standard Java containers don't have a pre-configured user and as a result don't have <code>HOME</code> environment variable presented which makes Gradle complain.</p>"},{"location":"examples/#caching","title":"Caching","text":"<p>To preserve caches between Gradle runs, add a cache instruction as shown below. The trick here is to clean up <code>~/.gradle/caches</code> folder in the very end of a build. Gradle creates some unique nondeterministic files in <code>~/.gradle/caches</code> folder on every run which makes Cirrus CI re-upload the cache every time. This way, you get faster builds!</p> amd64arm64 <pre><code>container:\n  image: gradle:jdk11\n\ncheck_task:\n  gradle_cache:\n    folder: ~/.gradle/caches\n  check_script: gradle check\n  cleanup_before_cache_script:\n    - rm -rf ~/.gradle/caches/$GRADLE_VERSION/\n    - rm -rf ~/.gradle/caches/transforms-1\n    - rm -rf ~/.gradle/caches/journal-1\n    - rm -rf ~/.gradle/caches/jars-3/*/buildSrc.jar\n    - find ~/.gradle/caches/ -name \"*.lock\" -type f -delete\n</code></pre> <pre><code>arm_container:\n  image: gradle:jdk11\n\ncheck_task:\n  gradle_cache:\n    folder: ~/.gradle/caches\n  check_script: gradle check\n  cleanup_before_cache_script:\n    - rm -rf ~/.gradle/caches/$GRADLE_VERSION/\n    - rm -rf ~/.gradle/caches/transforms-1\n    - rm -rf ~/.gradle/caches/journal-1\n    - rm -rf ~/.gradle/caches/jars-3/*/buildSrc.jar\n    - find ~/.gradle/caches/ -name \"*.lock\" -type f -delete\n</code></pre>"},{"location":"examples/#build-cache","title":"Build Cache","text":"<p>Here is how HTTP Cache can be used with Gradle by adding the following code to <code>settings.gradle</code>:</p> <pre><code>ext.isCiServer = System.getenv().containsKey(\"CIRRUS_CI\")\next.isMasterBranch = System.getenv()[\"CIRRUS_BRANCH\"] == \"master\"\next.buildCacheHost = System.getenv().getOrDefault(\"CIRRUS_HTTP_CACHE_HOST\", \"localhost:12321\")\n\nbuildCache {\n  local {\n    enabled = !isCiServer\n  }\n  remote(HttpBuildCache) {\n    url = \"http://${buildCacheHost}/\"\n    enabled = isCiServer\n    push = isMasterBranch\n  }\n}\n</code></pre> <p>If your project uses a <code>buildSrc</code> directory, the build cache configuration should also be applied to <code>buildSrc/settings.gradle</code>.</p> <p>To do this, put the build cache configuration above into a separate <code>gradle/buildCacheSettings.gradle</code> file, then apply it to both your <code>settings.gradle</code> and <code>buildSrc/settings.gradle</code>.</p> <p>In <code>settings.gradle</code>:</p> <pre><code>apply from: new File(settingsDir, 'gradle/buildCacheSettings.gradle')\n</code></pre> <p>In <code>buildSrc/settings.gradle</code>:</p> <pre><code>apply from: new File(settingsDir, '../gradle/buildCacheSettings.gradle')\n</code></pre> <p>Please make sure you are running Gradle commands with <code>--build-cache</code> flag or have <code>org.gradle.caching</code> enabled in <code>gradle.properties</code> file. Here is an example of a <code>gradle.properties</code> file that we use internally for all Gradle projects:</p> <pre><code>org.gradle.daemon=true\norg.gradle.caching=true\norg.gradle.parallel=true\norg.gradle.configureondemand=true\norg.gradle.jvmargs=-Dfile.encoding=UTF-8\n</code></pre>"},{"location":"examples/#junit","title":"JUnit","text":"<p>Here is a <code>.cirrus.yml</code> that, parses and uploads JUnit reports at the end of the build:</p> <pre><code>junit_test_task:\n  junit_script: &lt;replace this comment with instructions to run the test suites&gt;\n  always:\n    junit_result_artifacts:\n      path: \"**/test-results/**.xml\"\n      format: junit\n      type: text/xml\n</code></pre> <p>If it is running on a pull request, annotations will also be displayed in-line.</p>"},{"location":"examples/#maven","title":"Maven","text":"<p>Official Maven Docker images can be used for building and testing Maven projects:</p> amd64arm64 <pre><code>container:\n  image: maven:latest\n\ntask:\n  name: Cirrus CI\n  maven_cache:\n    folder: ~/.m2\n  test_script: mvn test -B\n</code></pre> <pre><code>arm_container:\n  image: maven:latest\n\ntask:\n  name: Cirrus CI\n  maven_cache:\n    folder: ~/.m2\n  test_script: mvn test -B\n</code></pre>"},{"location":"examples/#mysql","title":"MySQL","text":"<p>The Additional Containers feature makes it super simple to run the same Docker MySQL image as you might be running in production for your application. Getting a running instance of the latest GA version of MySQL can used with the following six lines in your <code>.cirrus.yml</code>:</p> amd64arm64 <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre> <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre> <p>With the configuration above MySQL will be available on <code>localhost:3306</code>. Use empty password to login as <code>root</code> user.</p>"},{"location":"examples/#node","title":"Node","text":"<p>Official NodeJS Docker images can be used for building and testing Node.JS applications.</p>"},{"location":"examples/#npm","title":"npm","text":"<p>Here is an example of a <code>.cirrus.yml</code> that caches <code>node_modules</code> based on contents of <code>package-lock.json</code> file and runs tests:</p> amd64arm64 <pre><code>container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat package-lock.json\n    populate_script: npm ci\n  test_script: npm test\n</code></pre> <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat package-lock.json\n    populate_script: npm ci\n  test_script: npm test\n</code></pre>"},{"location":"examples/#yarn","title":"Yarn","text":"<p>Here is an example of a <code>.cirrus.yml</code> that caches <code>node_modules</code> based on the contents of a <code>yarn.lock</code> file and runs tests:</p> amd64arm64 <pre><code>container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn run test\n</code></pre> <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn run test\n</code></pre>"},{"location":"examples/#yarn-2","title":"Yarn 2","text":"<p>Yarn 2 (also known as Yarn Berry), has a different package cache location (<code>.yarn/cache</code>). To run tests, it would look like this:</p> amd64arm64 <pre><code>container:\n  image: node:latest\n\ntest_task:\n  yarn_cache:\n    folder: .yarn/cache\n    fingerprint_script: cat yarn.lock\n  install_script:\n    - yarn set version berry\n    - yarn install\n  test_script: yarn run test\n</code></pre> <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  yarn_cache:\n    folder: .yarn/cache\n    fingerprint_script: cat yarn.lock\n  install_script:\n    - yarn set version berry\n    - yarn install\n  test_script: yarn run test\n</code></pre>"},{"location":"examples/#eslint-annotations","title":"ESLint Annotations","text":"<p>ESLint reports are supported by Cirrus CI Annotations. This way you can see all the linting issues without leaving the pull request you are reviewing! You'll need to generate an <code>ESLint</code> report file (for example, <code>eslint.json</code>) in one of your task's scripts. Then save it as an artifact in <code>eslint</code> format:</p> <pre><code>task:\n  # boilerplate\n  eslint_script: ...\n  always:\n    eslint_report_artifact:\n      path: eslint.json\n      format: eslint\n</code></pre>"},{"location":"examples/#protocol-buffers-linting","title":"Protocol Buffers Linting","text":"<p>Here is an example of how  <code>*.proto</code> files can be linted using Buf CLI.</p> amd64arm64 <pre><code>container:\n  image: bufbuild/buf:latest\n\ntask:\n  name: Buf Lint\n  lint_script: buf lint --error-format=json &gt; lint.report.json\n  on_failure:\n    report_artifacts:\n      path: lint.report.json\n      format: buf\n</code></pre> <pre><code>arm_container:\n  image: bufbuild/buf:latest\n\ntask:\n  name: Buf Lint\n  lint_script: buf lint --error-format=json &gt; lint.report.json\n  on_failure:\n    report_artifacts:\n      path: lint.report.json\n      format: buf\n</code></pre>"},{"location":"examples/#python","title":"Python","text":"<p>Official Python Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that caches installed packages based on contents of <code>requirements.txt</code> and runs <code>pytest</code>:</p> amd64arm64 <pre><code>container:\n  image: python:slim\n\ntest_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION &amp;&amp; cat requirements.txt\n    populate_script: pip install -r requirements.txt\n  test_script: pytest\n</code></pre> <pre><code>arm_container:\n  image: python:slim\n\ntest_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION &amp;&amp; cat requirements.txt\n    populate_script: pip install -r requirements.txt\n  test_script: pytest\n</code></pre>"},{"location":"examples/#building-pypi-packages","title":"Building PyPI Packages","text":"<p>Also using the Python Docker images, you can run tests if you are making packages for PyPI. Here is an example <code>.cirrus.yml</code> for doing so:</p> amd64arm64 <pre><code>container:\n  image: python:slim\n\nbuild_package_test_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION\n    populate_script: python3 -m pip install --upgrade setuptools wheel\n  build_package_test_script: python3 setup.py sdist bdist_wheel\n</code></pre> <pre><code>arm_container:\n  image: python:slim\n\nbuild_package_test_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION\n    populate_script: python3 -m pip install --upgrade setuptools wheel\n  build_package_test_script: python3 setup.py sdist bdist_wheel\n</code></pre>"},{"location":"examples/#linting","title":"Linting","text":"<p>You can easily set up linting with Cirrus CI and flake8, here is an example <code>.cirrus.yml</code>:</p> amd64arm64 <pre><code>lint_task:\n  container:\n    image: alpine/flake8:latest\n  script: flake8 *.py\n</code></pre> <pre><code>lint_task:\n  arm_container:\n    image: alpine/flake8:latest\n  script: flake8 *.py\n</code></pre>"},{"location":"examples/#unittest-annotations","title":"<code>Unittest</code> Annotations","text":"<p>Python Unittest reports are supported by Cirrus CI Annotations. This way you can see what tests are failing without leaving the pull request you are reviewing! Here is an example of a <code>.cirrus.yml</code> that produces and stores <code>Unittest</code> reports:</p> amd64arm64 <pre><code>unittest_task:\n  container:\n    image: python:slim\n  install_dependencies_script: |\n    pip3 install unittest_xml_reporting\n  run_tests_script: python3 -m xmlrunner tests\n  # replace 'tests' with the module,\n  # unittest.TestCase, or unittest.TestSuite\n  # that the tests are in\n  always:\n    upload_results_artifacts:\n      path: ./*.xml\n      format: junit\n      type: text/xml\n</code></pre> <pre><code>unittest_task:\n  arm_container:\n    image: python:slim\n  install_dependencies_script: |\n    pip3 install unittest_xml_reporting\n  run_tests_script: python3 -m xmlrunner tests\n  # replace 'tests' with the module,\n  # unittest.TestCase, or unittest.TestSuite\n  # that the tests are in\n  always:\n    upload_results_artifacts:\n      path: ./*.xml\n      format: junit\n      type: text/xml\n</code></pre> <p>Now you should get annotations for your test results.</p>"},{"location":"examples/#qodana","title":"Qodana","text":"<p>Qodana by JetBrains is a code quality monitoring tool that identifies and suggests fixes for bugs, security vulnerabilities, duplications, and imperfections. It brings all the smart features you love in the JetBrains IDEs.</p> <p>Here is an example of <code>.cirrus.yml</code> configuration file which will save Qodana's report as an artifact, will parse it and report as annotations:</p> <pre><code>task:\n  name: Qodana\n  container:\n    image: jetbrains/qodana:latest\n  env:\n    CIRRUS_WORKING_DIR: /data/project\n  generate_report_script:\n    - /opt/idea/bin/entrypoint --save-report --report-dir=report\n  always:\n    results_artifacts:\n      path: \"report/results/result-allProblems.json\"\n      format: qodana\n</code></pre>"},{"location":"examples/#release-assets","title":"Release Assets","text":"<p>Cirrus CI doesn't provide a built-in functionality to upload artifacts on a GitHub release but this functionality can be added via a script. For a release, Cirrus CI will provide <code>CIRRUS_RELEASE</code> environment variable along with <code>CIRRUS_TAG</code> environment variable. <code>CIRRUS_RELEASE</code> indicates release id which can be used to upload assets.</p> <p>Cirrus CI only requires write access to Check API and doesn't require write access to repository contents because of security reasons. That's why you need to create a personal access token with full access to <code>repo</code> scope. Once an access token is created, please create an encrypted variable from it and save it to <code>.cirrus.yml</code>:</p> <pre><code>env:\n  GITHUB_TOKEN: ENCRYPTED[qwerty]\n</code></pre> <p>Now you can use a script to upload your assets:</p> <pre><code>#!/usr/bin/env bash\n\nif [[ \"$CIRRUS_RELEASE\" == \"\" ]]; then\n  echo \"Not a release. No need to deploy!\"\n  exit 0\nfi\n\nif [[ \"$GITHUB_TOKEN\" == \"\" ]]; then\n  echo \"Please provide GitHub access token via GITHUB_TOKEN environment variable!\"\n  exit 1\nfi\n\nfile_content_type=\"application/octet-stream\"\nfiles_to_upload=(\n  # relative paths of assets to upload\n)\n\nfor fpath in $files_to_upload\ndo\n  echo \"Uploading $fpath...\"\n  name=$(basename \"$fpath\")\n  url_to_upload=\"https://uploads.github.com/repos/$CIRRUS_REPO_FULL_NAME/releases/$CIRRUS_RELEASE/assets?name=$name\"\n  curl -X POST \\\n    --data-binary @$fpath \\\n    --header \"Authorization: token $GITHUB_TOKEN\" \\\n    --header \"Content-Type: $file_content_type\" \\\n    $url_to_upload\ndone\n</code></pre>"},{"location":"examples/#ruby","title":"Ruby","text":"<p>Official Ruby Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that caches installed gems based on Ruby version, contents of <code>Gemfile.lock</code>, and runs <code>rspec</code>:</p> amd64arm64 <pre><code>container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_report_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre> <pre><code>arm_container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_report_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre> Repositories without <code>Gemfile.lock</code> <p>When you are not committing <code>Gemfile.lock</code> (in Ruby gems repositories, for example) you can run <code>bundle install</code> (or <code>bundle update</code>) in <code>install_script</code> instead of <code>populate_script</code> in <code>bundle_cache</code>. Cirrus Agent is clever enough to re-upload cache entry only if cached folder has been changed during task execution. Here is an example of a <code>.cirrus.yml</code> that always runs <code>bundle install</code>:</p> amd64arm64 <pre><code>container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile\n      - cat *.gemspec\n  install_script: bundle install # or `update` for the freshest bundle\n  rspec_script: bundle exec rspec\n</code></pre> <pre><code>arm_container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile\n      - cat *.gemspec\n  install_script: bundle install # or `update` for the freshest bundle\n  rspec_script: bundle exec rspec\n</code></pre> <p>Test Parallelization</p> <p>It's super easy to add intelligent test splitting by using Knapsack Pro and matrix modification. After setting up Knapsack Pro gem, you can add sharding like this:</p> <pre><code>task:\n  matrix:\n    name: rspec (shard 1)\n    name: rspec (shard 2)\n    name: rspec (shard 3)\n    name: rspec (shard 4)\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script: cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rake knapsack_pro:rspec\n</code></pre> <p>Which will create four shards that will theoretically run tests 4x faster by equally splitting all tests between these four shards.</p>"},{"location":"examples/#rspec-and-rubocop-annotations","title":"RSpec and RuboCop Annotations","text":"<p>Cirrus CI natively supports RSpec and RuboCop machine-parsable JSON reports.</p> <p>To get behavior-driven test annotations, generate and upload a <code>rspec</code> artifact from your lint task:</p> amd64arm64 <pre><code>container:\n  image: ruby:latest\n\ntask:\n  name: RSpec\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre> <pre><code>arm_container:\n  image: ruby:latest\n\ntask:\n  name: RSpec\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre> <p>Generate a <code>rubocop</code> artifact to quickly gain context for linter/formatter annotations:</p> amd64arm64 <pre><code>container:\n  image: ruby:latest\n\ntask:\n  name: RuboCop\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rubocop --format json --out rubocop.json\n  always:\n    rubocop_artifacts:\n      path: rubocop.json\n      type: text/json\n      format: rubocop\n</code></pre> <pre><code>arm_container:\n  image: ruby:latest\n\ntask:\n  name: RuboCop\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rubocop --format json --out rubocop.json\n  always:\n    rubocop_artifacts:\n      path: rubocop.json\n      type: text/json\n      format: rubocop\n</code></pre>"},{"location":"examples/#rust","title":"Rust","text":"<p>Official Rust Docker images can be used for builds. Here is a basic example of <code>.cirrus.yml</code> that caches crates in <code>$CARGO_HOME</code> based on contents of <code>Cargo.lock</code>:</p> amd64arm64 <pre><code>container:\n  image: rust:latest\n\ntest_task:\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre> <pre><code>arm_container:\n  image: rust:latest\n\ntest_task:\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre> <p>Caching Cleanup</p> <p>Please note <code>before_cache_script</code> that removes registry index from the cache before uploading it in the end of a successful task. Registry index is changing very rapidly making the cache invalid. <code>before_cache_script</code> deletes the index and leaves only the required crates for caching.</p>"},{"location":"examples/#rust-nightly","title":"Rust Nightly","text":"<p>It is possible to use nightly builds of Rust via an official <code>rustlang/rust:nightly</code> container. Here is an example of a <code>.cirrus.yml</code> to run tests against the latest stable and nightly versions of Rust:</p> amd64arm64 <pre><code>test_task:\n  matrix:\n    - container:\n        image: rust:latest\n    - allow_failures: true\n      container:\n        image: rustlang/rust:nightly\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre> <pre><code>test_task:\n  matrix:\n    - arm_container:\n        image: rust:latest\n    - allow_failures: true\n      arm_container:\n        image: rustlang/rust:nightly\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre> FreeBSD Caveats <p>Vanila FreeBSD VMs don't set some environment variables required by Cargo for effective caching. Specifying <code>HOME</code> environment variable to some arbitrarily location should fix caching:</p> <pre><code>freebsd_instance:\n  image-family: freebsd-14-0\n\ntask:\n  name: cargo test (stable)\n  env:\n    HOME: /tmp # cargo needs it\n  install_script: pkg install -y rust\n  cargo_cache:\n    folder: $HOME/.cargo/registry\n    fingerprint_script: cat Cargo.lock\n  build_script: cargo build --all\n  test_script: cargo test --all --all-targets\n  before_cache_script: rm -rf $HOME/.cargo/registry/index\n</code></pre>"},{"location":"examples/#xclogparser","title":"XCLogParser","text":"<p>XCLogParser is a CLI tool that parses Xcode and <code>xcodebuild</code>'s logs (<code>xcactivitylog</code> files) and produces reports in different formats.</p> <p>Here is an example of <code>.cirrus.yml</code> configuration file which will save XCLogParser's flat JSON report as an artifact, will parse it and report as annotations:</p> <pre><code>macos_instance:\n  image: big-sur-xcode\n\ntask:\n  name: XCLogParser\n  build_script:\n    - xcodebuild -scheme noapp -derivedDataPath ~/dd\n  always:\n    xclogparser_parse_script:\n      - brew install xclogparser\n      - xclogparser parse --project noapp --reporter flatJson --output xclogparser.json --derived_data ~/dd\n    xclogparser_upload_artifacts:\n      path: \"xclogparser.json\"\n      type: text/json\n      format: xclogparser\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#what-are-the-ip-addresses-of-cirrus-ci","title":"What are the IP addresses of Cirrus CI?","text":"<p>Cirrus CI control plane uses three IP addresses:</p> <ul> <li><code>34.117.12.6</code> - IP address of the Cirrus CI API and all <code>*.cirrus-ci.com</code> domains.</li> <li><code>34.27.109.83</code> - IP address for egress connections when evaluating Starlark configuration files.</li> <li><code>34.28.114.255</code> - IP addresses for egress connections that Cirrus CI uses to access APIs, deliver webhook events, etc.</li> </ul>"},{"location":"faq/#is-cirrus-ci-a-delivery-platform","title":"Is Cirrus CI a delivery platform?","text":"<p>Cirrus CI is not positioned as a delivery platform but can be used as one for many general use cases by having  Dependencies between tasks and using Conditional Task Execution or Manual Tasks:</p> <pre><code>lint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  only_if: $BRANCH == 'master'\n  trigger_type: manual\n  depends_on: \n    - test\n    - lint\n  script: yarn run publish\n</code></pre>"},{"location":"faq/#are-there-any-limits","title":"Are there any limits?","text":"<p>Cirrus CI has the following limitations on how many CPUs for different platforms a single user can run on Cirrus Cloud Clusters for public repositories for free:</p> <ul> <li>16.0 CPUs for Linux platform (Containers or VMs).</li> <li>16.0 CPUs for Arm Linux platform (Containers).</li> <li>8.0 CPUs for Windows platform (Containers or VMs)</li> <li>8.0 CPUs for FreeBSD VMs.</li> <li>4.0 CPUs macOS VM (1 VM).</li> </ul> <p>Note that a single task can't request more than 8 CPUs (except macOS VMs which are not configurable).</p> <p>Monthly CPU Minutes Limit</p> <p>Additionally there is an upper monthly limit on free usage equal to 50 compute credits (which is equal to 10,000 CPU-minutes for Linux tasks or 500 minutes for macOS tasks which always use 4 CPUs).</p> <p>If you are using Cirrus CI with your private personal repositories under the $10/month plan you'll have twice the limits:</p> <ul> <li>32.0 CPUs for Linux platform (Containers or VMs).</li> <li>16.0 CPUs for Windows platform (Containers or VMs)</li> <li>16.0 CPUs for FreeBSD VMs.</li> <li>8.0 CPUs macOS VM (2 VMs).</li> </ul> <p>There are no limits on how many VMs or Containers you can run in parallel if you bring your own infrastructure or use Compute Credits for either private or public repositories.</p> <p>Cache and Logs Redundancy</p> <p>By default Cirrus CI persists caches and logs for 90 days. If you bring your own compute services this period can be configured directly in your cloud provider's console.</p>"},{"location":"faq/#repository-is-blocked","title":"Repository is blocked","text":"<p>Free tier of Cirrus CI is intended for public OSS projects to run tests and other validations continuously. If your repository is configured to use Cirrus CI in a questionable way to just exploit Cirrus CI infrastructure, your repository might be blocked.</p> <p>Here are a few examples of such questionable activities we've seen so far:</p> <ul> <li>Use Cirrus CI as a powerhouse for arbitrary CPU-intensive calculations (including crypto mining).</li> <li>Use Cirrus CI to download a pirated movie, re-encode it, upload as a Cirrus artifact and distribute it.</li> <li>Use Cirrus CI distributed infrastructure to emulate user activity on a variety of websites to trick advertisers.</li> </ul>"},{"location":"faq/#ip-addresses-of-cirrus-cloud-clusters","title":"IP Addresses of Cirrus Cloud Clusters","text":"<p>Instances running on Cirrus Cloud Clusters are using dynamic IPs by default. It's possible to request a static <code>35.222.255.190</code> IP for all the \"managed-by-us\" instance types except macOS VMs via <code>use_static_ip</code> field. Here is an example of a Linux Docker container with a static IP:</p> <pre><code>task:\n  name: Test IP\n  container:\n    image: cirrusci/wget:latest\n    use_static_ip: true\n  script: wget -qO- ifconfig.co\n</code></pre>"},{"location":"faq/#ci-agent-stopped-responding","title":"CI agent stopped responding!","text":"<p>It means that Cirrus CI haven't heard from the agent for quite some time. In 99.999% of the cases  it happens because of two reasons:</p> <ol> <li> <p>Your task was executing on a Cirrus Cloud Cluster. Cirrus Cloud Cluster     is backed by Google Cloud's Spot VMs for cost efficiency reasons and    Google Cloud preempted back a VM your task was executing on. Cirrus CI is trying to minimize possibility of such cases     by constantly rotating VMs before Google Cloud preempts them, but there is still chance of such inconvenience.</p> </li> <li> <p>Your CI task used too much memory which led to a crash of a VM or a container.</p> </li> </ol>"},{"location":"faq/#agent-process-on-a-persistent-worker-exited-unexpectedly","title":"Agent process on a persistent worker exited unexpectedly!","text":"<p>This means that either an agent process or a VM with an agent process exited before reporting the last instruction of a task.</p> <p>If it's happening for a <code>macos_instance</code> then please contact support.</p>"},{"location":"faq/#instance-failed-to-start","title":"Instance failed to start!","text":"<p>It means that Cirrus CI has made a successful API call to a computing service  to allocate resources. But a requested resource wasn't created. </p> <p>If it happened for an OSS project, please contact support immediately. Otherwise check your cloud console first  and then contact support if it's still not clear what happened. </p>"},{"location":"faq/#instance-got-rescheduled","title":"Instance got rescheduled!","text":"<p>Cirrus CI is trying to be as efficient as possible and heavily uses spot VMs to run majority of workloads. It allows to drastically lower Cirrus CI's infrastructure bill and allows to provide the best pricing model with per-second billing and very generous limits for OSS projects, but it comes with a rare edge case... </p> <p>Spot VMs can be preempted which will require rescheduling and automatically restart tasks that were executing on these VMs.  This is a rare event since autoscaler is constantly rotating instances but preemption still happens occasionally.  All automatic re-runs and stateful tasks using compute credits are always executed on regular VMs.</p>"},{"location":"faq/#instance-timed-out","title":"Instance timed out!","text":"<p>By default, Cirrus CI has an execution limit of 60 minutes for each task. However, this default timeout duration can be changed by using <code>timeout_in</code> field in <code>.cirrus.yml</code> configuration file:</p> <pre><code>task: \n  timeout_in: 90m\n  ...\n</code></pre> <p>Maximum timeout</p> <p>There is a hard limit of 2 hours for free tasks. Use compute credits or compute service integration to avoid the limit.</p>"},{"location":"faq/#container-errored","title":"Container errored","text":"<p>It means that Cirrus CI has made a successful API call to a computing service to start a container but unfortunately container runtime or the corresponding computing service had an internal error.</p>"},{"location":"faq/#only-github-support","title":"Only GitHub Support?","text":"<p>At the moment Cirrus CI only supports GitHub via a GitHub Application. We are planning to support BitBucket next. </p>"},{"location":"faq/#any-discounts","title":"Any discounts?","text":"<p>Cirrus CI itself doesn't provide any discounts except Cirrus Cloud Cluster  which is free for open source projects. But since Cirrus CI delegates execution of builds to different computing services, it means that discounts from your cloud provider will be applied to Cirrus CI builds.</p>"},{"location":"features/","title":"Features","text":""},{"location":"features/#free-for-open-source","title":"Free for Open Source","text":"<p>To support the Open Source community, Cirrus CI provides Linux, Windows, macOS and FreeBSD services free of charge up to a cap of 50 compute credits a month to OSS projects.</p> <p>Here is a list of all instance types available for free for Open Source Projects:</p> Instance Type Managed by Description <code>container</code> us Linux Docker Container <code>arm_container</code> us Linux Arm Docker Container <code>windows_container</code> us Windows Docker Container <code>docker_builder</code> us Full-fledged VM pre-configured for running Docker <code>macos_instance</code> us macOS Virtual Machines <code>freebsd_instance</code> us FreeBSD Virtual Machines <code>compute_engine_instance</code> us Full-fledged custom VM <code>persistent_worker</code> you Use any host on any platform and architecture"},{"location":"features/#per-second-billing","title":"Per-second billing","text":"<p>Use compute credits to run as many parallel tasks as you want and pay only for CPU time used by these tasks. Another approach is to bring your own infrastructure and pay directly to your cloud provider within your current billing.</p>"},{"location":"features/#no-concurrency-limit-no-queues","title":"No concurrency limit. No queues","text":"<p>Cirrus CI leverages elasticity of the modern clouds to always have available resources to process your builds. Engineers should never wait for builds to start.</p>"},{"location":"features/#bring-your-own-infrastructure","title":"Bring Your Own Infrastructure","text":"<p>Cirrus CI supports bringing your own infrastructure (BYO) for full control over security and for easy integration with your current workflow.</p> <p> </p>"},{"location":"features/#flexible-runtime-environment","title":"Flexible runtime environment","text":"<p>Cirrus CI allows you to use any Unix or Windows VMs, any Docker containers, any amount of CPUs, optional SSDs and GPUs.</p>"},{"location":"features/#basic-but-very-powerful-configuration-format","title":"Basic but very powerful configuration format","text":"<p>Learn more about how to configure tasks here. Configure things like:</p> <ul> <li>Matrix Builds</li> <li>Dependencies between tasks</li> <li>Conditional Task Execution</li> <li>Local HTTP Cache</li> <li>Dockerfile as a CI environment</li> <li>Monorepo Support</li> </ul> <p>Check the Quick Start guide for more features.</p> <p>Feel free to contact support if you have questions for your particular case.</p>"},{"location":"pricing/","title":"Pricing","text":"<p>Cirrus CI is free for Open Source projects with some limitations. For private projects, Cirrus CI has couple of options depending on your needs:</p> <ol> <li>For private personal repositories there is a very affordable $10 a month plan with     access to Cirrus Cloud Clusters for Linux, Windows and macOS workloads.</li> <li>Buy compute credits to access managed and pre-configured Cirrus Cloud Clusters for Linux, FreeBSD, Windows, and macOS workloads.</li> <li>Configure access to your own infrastructure and pay $10/seat/month.</li> </ol> <p>Here is a comparison table of available Cirrus CI plans:</p> User Free Public Repositories Private Personal Repository Private Organization Repositories Person <ul><li>Free access to Cirrus Cloud Clusters for public repositories</li><li>Bring your own infrastructure for public repositories</li><li>Configure persistent workers for public repositories</li></ul> <ul><li>Access to community clusters for public and private repositories</li><li>Bring your own infrastructure for public and private repositories</li><li>Configure persistent workers for public and private repositories</li></ul> Not Applicable Organization <ul><li>Free access to Cirrus Cloud Clusters for public repositories</li><li>Use compute credits to access community clusters for private repositories and/or to avoid the limits on public repositories</li><li>Bring your own infrastructure for public repositories</li><li>Configure persistent workers for public repositories</li></ul> Not Applicable <ul><li>Free access to community clusters for public repositories</li><li>Use compute credits to access community clusters for private repositories and/or to avoid the limits on public repositories</li><li>Bring your own infrastructure for public and private repositories</li><li>Configure persistent workers for public and private repositories</li></ul>"},{"location":"pricing/#compute-credits","title":"Compute Credits","text":"<p>Sometimes configuring your own compute services isn't worth it. It takes time and effort to maintain them. For such cases there is a way to use the Cirrus Cloud Clusters for your organization.</p> <p>1 compute credit can be bought for 1 US dollar. Here is how much 1000 minutes of CPU time will cost for different platforms:</p> <ul> <li>1000 minutes of 1 virtual CPU for Linux platform for 3 compute credits</li> <li>1000 minutes of 1 virtual CPU for FreeBSD platform for 3 compute credits</li> <li>1000 minutes of 1 virtual CPU for Windows platform for 4 compute credits</li> <li>1000 minutes of 1 Apple Silicon CPU for 15 compute credits</li> </ul> <p>All tasks using compute credits are charged on per-second basis. 2 CPU Linux task takes 5 minutes? Pay 3 cents.</p> <p>Note: orchestration costs are included in compute credits and there is no need to purchase additional seats on your organization's plan.</p> <p>Works for OSS projects</p> <p>Compute credits can be used for commercial OSS projects to avoid concurrency limits. Note that only collaborators for the project will be able to use organization's compute credits.</p> <p>Benefits of this approach:</p> <ul> <li>Use the same pre-configured infrastructure that we fine tune and constantly upgrade/improve.</li> <li>No need to configure anything. Let Cirrus CI's team manage and upgrade infrastructure for you.</li> <li>Per-second billing with no additional monthly fees for storage and traffic.</li> <li>Cost efficient for small to medium teams. </li> </ul> <p>Cons of this approach:</p> <ul> <li>No support for exotic use cases like GPUs, SSDs and 100+ cores machines.</li> <li>Not that cost efficient for big teams.</li> </ul>"},{"location":"pricing/#buying-compute-credits","title":"Buying Compute Credits","text":"<p>To see your current balance, recent transactions and to buy more compute credits, go to your organization's settings page:</p> <pre><code>https://cirrus-ci.com/settings/github/MY-ORGANIZATION\n</code></pre>"},{"location":"pricing/#configuring-compute-credits","title":"Configuring Compute Credits","text":"<p>Compute credits can be used with any of the following instance types: <code>container</code>, <code>windows_container</code> and <code>macos_instance</code>. No additional configuration needed.</p> amd64arm64 <pre><code>task:\n  container:\n    image: node:latest\n  ...\n</code></pre> <pre><code>task:\n  arm_container:\n    image: node:latest\n  ...\n</code></pre> <p>Using compute credits for public or personal private repositories</p> <p>If you willing to boost Cirrus CI for public or your personal private repositories you need to explicitly mark a task to use compute credits with <code>use_compute_credits</code> field.</p> <p>Here is an example of how to enable compute credits for internal and external collaborators of a public repository:</p> <pre><code>task:\n  use_compute_credits: $CIRRUS_USER_COLLABORATOR == 'true'\n</code></pre> <p>Here is another example of how to enable compute credits for master branch of a personal private project to make sure all of the master builds are executed as fast as possible by skipping free usage limits:</p> <pre><code>task:\n  use_compute_credits: $CIRRUS_BRANCH == 'master'\n</code></pre>"},{"location":"pricing/#compute-services","title":"Compute Services","text":"<p>Configure and connect one or more compute services and/or persistent workers to Cirrus CI for orchestrating CI workloads on them. It's free for your public repositories and costs $10/seat/month to use with private repositories unless your organization has Priority Support Subscription.</p> <p>Benefits of this approach:</p> <ul> <li>Full control of underlying infrastructure. Use any type of VMs and containers with any amount of CPUs and memory.</li> <li>More secure. Setup any firewall and access rules.</li> <li>Pay for CI within your existing cloud and GitHub bills. </li> </ul> <p>Cons of this approach:</p> <ul> <li>Need to configure and connect one or several compute services.</li> <li>Might not be worth the effort for a small team.</li> <li>Need to pay $10/seat/month plan.</li> </ul> <p>What is a seat?</p> <p>A seat is a user that initiates CI builds by pushing commits and/or creating pull requests in a private repository.  It can be a real person or a bot. If you are using Cron Builds or creating builds through Cirrus's API it will be counted as an additional seat (like a bot).</p> <p>For example, if there are 10 people in your GitHub Organization and only 5 of them are working on private repositories  where Cirrus CI is configured, the remaining 5 people are not counted as seats, given that they aren't pushing to the private repository.  Let's say Dependabot is also configured for these private repositories. </p> <p>In that case there are <code>5 + 1 = 6</code> seats you need to purchase Cirrus CI plan for.</p>"},{"location":"security/","title":"Security Policy","text":""},{"location":"security/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>If you find a security vulnerability in the Cirrus CI platform (the backend, web interface, etc.), please follow the steps below.</p> <ol> <li>Do NOT comment about the vulnerability publicly.</li> <li> <p>Please email <code>hello@cirruslabs.org</code> with the following format:</p> <pre><code>Subject: Platform Security Risk\n\nHOW TO EXPLOIT\n\nGive exact details so our team can replicate it.\n\nOTHER INFORMATION\n\nIf anything else needs to be said, put it here.\n</code></pre> </li> <li> <p>Please be patient. You will get an email back soon.</p> </li> </ol> <p>Thank you! </p>"},{"location":"support/","title":"General Support","text":"<p>The best way to ask general questions about particular use cases is to email our support team at support+ci@cirruslabs.org. Our support team is trying our best to respond ASAP, but there is no guarantee on a response time unless your organization enrolls in Priority Support.</p> <p>If you have a feature request or noticed lack of some documentation please feel free to create a GitHub issue. Our support team will answer it by replying to the issue or by updating the documentation.</p>"},{"location":"support/#priority-support","title":"Priority Support","text":"<p>In addition to the general support we provide a Priority Support option with guaranteed response times. But most importantly we'll be doing regular checkins to make sure roadmap for Cirrus CI and other services/software under <code>cirruslabs</code> organization is aligned with your company's needs. You'll be helping to shape the future of software developed by Cirrus Labs!</p> Severity Support Impact First Response Time SLA Hours How to Submit 1 Emergency (service is unavailable or completely unusable). 30 minutes 24x7 Please use urgent email address. 2 Highly Degraded (Important features unavailable or extremely slow; No acceptable workaround). 4 hours 24x5 Please use priority email address. 3 Medium Impact. 8 hours 24x5 Please use priority email address. 4 Low Impact. 24 hours 24x5 Please use regular support email address. Make sure to send the email from your corporate email. <p><code>24x5</code> means period of time from 9AM on Monday till 5PM on Friday in EST timezone.</p> Support Impact Definitions <ul> <li>Severity 1 - Cirrus CI or other services is unavailable or completely unusable. An urgent issue can be filed and   our On-Call Support Engineer will respond within 30 minutes. Example: Cirrus CI showing 502 errors for all users.</li> <li>Severity 2 - Cirrus CI or other services is Highly Degraded. Significant Business Impact. Important Cirrus CI features are unavailable   or extremely slowed, with no acceptable workaround.</li> <li>Severity 3 - Something is preventing normal service operation. Some Business Impact. Important features of Cirrus CI or other services   are unavailable or somewhat slowed, but a workaround is available. Cirrus CI use has a minor loss of operational functionality.</li> <li>Severity 4 - Questions or Clarifications around features or documentation. Minimal or no Business Impact.    Information, an enhancement, or documentation clarification is requested, but there is no impact on the operation of Cirrus CI or other services/software.</li> </ul> <p>How to submit a priority or an urgent issue</p> <p>Once your organization signs the Priority Support Subscription contract, members of your organization will get access to separate support emails specified in your subscription contract.</p>"},{"location":"support/#priority-support-pricing","title":"Priority Support Pricing","text":"<p>As a company grows, engineering team tend to accumulate knowledge operating and working with Cirrus CI and other services/software provided by Cirrus Labs, therefore there is less effort needed to support each new seat from our side. On the other hand, Cirrus CI allows to bring your own infrastructure which increases complexity of the support. As a result we reflected the above challenges in a tiered pricing model based on a seat amount and a type of infrastructure used:</p> Seat Amount Only managed by us instance types Bring your own infrastructure 20-100 $60/seat/month $100/seat/month 101-300 $45/seat/month $75/seat/month 301-500 $30/seat/month $50/seat/month 500+ $15/seat/month $25/seat/month <p>Note that Priority Support Subscription requires a purchase of a minimum of 20 seats even if some of them will be unused.</p> What is a seat? <p>A seat is a user that initiates CI builds by pushing commits and/or creating pull requests in a private repository. It can be a real person or a bot. If you are using Cron Builds or creating builds through Cirrus's API it will be counted as an additional seat (like a bot).</p> <p>If you'd like to get a priority support for your public repositories then the amount of seats will be equal to the amount of members in your organization.</p>"},{"location":"support/#how-to-purchase-priority-support-subscription","title":"How to purchase Priority Support Subscription","text":"<p>Please email sales@cirruslabs.org, so we can get a support contract in addition to TOC. The contract will contain a special priority email address for your organization and other helpful information. Sales team will also schedule a check-in meeting to make sure your engineering team is set for success and Cirrus Labs roadmap aligns with your needs.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"guide/FreeBSD/","title":"FreeBSD VMs","text":""},{"location":"guide/FreeBSD/#freebsd-virtual-machines","title":"FreeBSD Virtual Machines","text":"<p>It is possible to run FreeBSD Virtual Machines the same way one can run Linux containers on the FreeBSD Cloud Cluster. To accomplish this, use <code>freebsd_instance</code> in your <code>.cirrus.yml</code>:</p> <pre><code>freebsd_instance:\n  image_family: freebsd-14-2\n\ntask:\n  install_script: pkg install -y ...\n  script: ...\n</code></pre> <p>Under the Hood</p> <p>Under the hood, a basic integration with Google Compute Engine is used and <code>freebsd_instance</code> is a syntactic sugar for the following <code>compute_engine_instance</code> configuration:</p> <pre><code>compute_engine_instance:\n  image_project: freebsd-org-cloud-dev\n  image: family/freebsd-14-2\n  platform: freebsd\n</code></pre>"},{"location":"guide/FreeBSD/#list-of-available-image-families","title":"List of available image families","text":"<p>Any of the official FreeBSD VMs on Google Cloud Platform are supported. Here are a few of them which are self explanatory:</p> <ul> <li><code>freebsd-15-0-snap</code> (15.0-SNAP)</li> <li><code>freebsd-14-2</code>      (14.2-RELEASE)</li> </ul> <p>It's also possible to specify a concrete version of an image by name via <code>image_name</code> field. To get a full list of available images please run the following gcloud command:</p> <pre><code>gcloud compute images list --project freebsd-org-cloud-dev --no-standard-images\n</code></pre>"},{"location":"guide/build-life/","title":"Life of a Build","text":"<p>Any build starts with a change pushed to GitHub. Since Cirrus CI is a GitHub Application, a webhook event  will be triggered by GitHub. From the webhook event, Cirrus CI will parse a Git branch and the SHA  for the change. Based on said information, a new build will be created.</p> <p>After build creation Cirrus CI will use GitHub's APIs to download a content of <code>.cirrus.yml</code> file for the SHA. Cirrus CI will evaluate it and create corresponding tasks.</p> <p>These tasks (defined in the <code>.cirrus.yml</code> file) will be dispatched within Cirrus CI to different services responsible for scheduling on a supported computing service. Cirrus CI's scheduling service will use appropriate APIs to create and manage a VM instance or a Docker container on the particular computing service.  The scheduling service will also configure start-up script that downloads the Cirrus CI agent, configures it to send logs back and starts it. Cirrus CI agent is a self-contained executable written in Go which means it can be executed anywhere.</p> <p>Cirrus CI's agent will request commands to execute for a particular task and will stream back logs, caches, artifacts and exit codes of the commands upon execution. Once the task finishes, the scheduling service will clean up the used VM or container.</p> <p></p> <p>This is a diagram of how Cirrus CI schedules a task on Google Cloud Platform. The blue arrows represent API calls and the green arrows represent unidirectional communication between an agent inside a VM or a container and Cirrus CI. Other chores such as health checking of the agent and GitHub status reporting happen in real time as a task is running.</p> <p>Straight forward and nothing magical. </p> <p>For any questions, feel free to contact us.</p>"},{"location":"guide/custom-vms/","title":"Custom VMs","text":""},{"location":"guide/custom-vms/#custom-compute-engine-vms","title":"Custom Compute Engine VMs","text":"<p>Cirrus CI supports many different compute services when you bring your own infrastructure,  but internally at Cirrus Labs we use Google Cloud Platform for running all managed by us instances except <code>macos_instance</code>. Already things like Docker Builder and <code>freebsd_instance</code> are basically a syntactic sugar for launching Compute Engine instances from a particular limited set of images.</p> <p>With <code>compute_engine_instance</code> it is possible to use any publicly available image for running your Cirrus tasks in. Such instances are particularly useful when you can't use Docker containers, for example, when you need to test things against newer versions of the Linux kernel than the Docker host has.</p> <p>Here is an example of using a <code>compute_engine_instance</code> to run a VM with KVM available:</p> <pre><code>compute_engine_instance:\n  image_project: cirrus-images # GCP project.\n  image: family/docker-kvm # family or a full image name.\n  platform: linux\n  architecture: arm64 # optional. By default, amd64 is assumed.\n  cpu: 4 # optional. Defaults to 2 CPUs.\n  memory: 16G # optional. Defaults to 4G.\n  disk: 100 # optional. By default, uses the smallest disk size required by the image.\n  nested_virtualization: true # optional. Whether to enable Intel VT-x. Defaults to false.\n</code></pre> Nested Virtualization License <p>Make sure that your source image already has a necessary license. Otherwise, nested virtualization won't work.</p>"},{"location":"guide/custom-vms/#building-custom-image-for-compute-engine","title":"Building custom image for Compute Engine","text":"<p>We recommend to use Packer for building your custom images. As an example, please take a look at our Packer templates used for building Docker Builder VM image.</p> <p>After building your image, please make sure the image publicly available:</p> <pre><code>gcloud compute images add-iam-policy-binding $IMAGE_NAME \\\n    --member='allAuthenticatedUsers' \\\n    --role='roles/compute.imageUser'\n</code></pre>"},{"location":"guide/docker-builder-vm/","title":"Docker Builder on VM","text":""},{"location":"guide/docker-builder-vm/#docker-builder-vm","title":"Docker Builder VM","text":"<p>\"Docker Builder\" tasks are a way to build and publish Docker Images to Docker Registries of your choice using a VM as build environment. In essence, a <code>docker_builder</code> is basically a <code>task</code> that is executed in a VM with pre-installed Docker.  A <code>docker_builder</code> can be defined the same way as a <code>task</code>:</p> amd64arm64 <pre><code>docker_builder:\n  build_script: docker build --tag myrepo/foo:latest .\n</code></pre> <pre><code>docker_builder:\n  env:\n    CIRRUS_ARCH: arm64\n  build_script: docker build --tag myrepo/foo:latest .\n</code></pre> <p>Leveraging features such as Task Dependencies, Conditional Execution and Encrypted Variables with a Docker Builder can help building relatively complex pipelines. It can also be used to execute builds which need special privileges.</p> <p>In the example below, a <code>docker_builder</code> will be only executed on a tag creation, once both <code>test</code> and <code>lint</code>  tasks have finished successfully:</p> <pre><code>test_task: ...\nlint_task: ...\n\ndocker_builder:\n  only_if: $CIRRUS_TAG != ''\n  depends_on: \n    - test\n    - lint\n  env:\n    DOCKER_USERNAME: ENCRYPTED[...]\n    DOCKER_PASSWORD: ENCRYPTED[...]\n  build_script: docker build --tag myrepo/foo:$CIRRUS_TAG .\n  login_script: docker login --username $DOCKER_USERNAME --password $DOCKER_PASSWORD\n  push_script: docker push myrepo/foo:$CIRRUS_TAG\n</code></pre> <p>Example</p> <p>For more examples please check how we use Docker Builder to build and publish Cirrus CI's Docker Images for Android.</p>"},{"location":"guide/docker-builder-vm/#multi-arch-builds","title":"Multi-arch builds","text":"<p>Docker Builder VM has QEMU pre-installed and is able to execute multi-arch builds via <code>buildx</code>. Add the following <code>setup_script</code> to enable <code>buildx</code> and then use <code>docker buildx build</code> instead of the regular <code>docker build</code>:</p> <pre><code>docker_builder:\n  setup_script:\n    - docker buildx create --name multibuilder\n    - docker buildx use multibuilder\n    - docker buildx inspect --bootstrap\n  build_script: docker buildx build --platform linux/amd64,linux/arm64 --tag myrepo/foo:$CIRRUS_TAG .\n</code></pre>"},{"location":"guide/docker-builder-vm/#pre-installed-packages","title":"Pre-installed Packages","text":"<p>For your convenience, a Docker Builder VM has some common packages pre-installed:</p> <ul> <li>AWS CLI</li> <li>Docker Compose</li> <li>OpenJDK</li> <li>Python</li> <li>Ruby with Bundler</li> </ul>"},{"location":"guide/docker-builder-vm/#under-the-hood","title":"Under the hood","text":"<p>Under the hood a simple integration with Google Compute Engine is used and basically <code>docker_builder</code> is a syntactic sugar for the following <code>compute_engine_instance</code> configuration:</p> amd64arm64 <pre><code>task:\n  compute_engine_instance:\n    image_project: cirrus-images\n    image: family/docker-builder\n    platform: linux\n    cpu: 4\n    memory: 16G\n</code></pre> <pre><code>task:\n  compute_engine_instance:\n    image_project: cirrus-images\n    image: family/docker-builder-arm64\n    architecture: arm64\n    platform: linux\n    cpu: 4\n    memory: 16G\n</code></pre> <p>You can check Packer templates of the VM image in <code>cirruslabs/vm-images</code> repository.</p>"},{"location":"guide/docker-builder-vm/#layer-caching","title":"Layer Caching","text":"<p>Docker has the <code>--cache-from</code> flag which allows using a previously built image as a cache source. This way only changed layers will be rebuilt which can drastically improve performance of the <code>build_script</code>. Here is a snippet that uses  the <code>--cache-from</code> flag:</p> <pre><code># pull an image if available\ndocker pull myrepo/foo:latest || true\ndocker build --cache-from myrepo/foo:latest \\\n  --tag myrepo/foo:$CIRRUS_TAG \\\n  --tag myrepo/foo:latest .\n</code></pre>"},{"location":"guide/docker-builder-vm/#dockerfile-as-a-ci-environment","title":"Dockerfile as a CI environment","text":"<p>With Docker Builder there is no need to build and push custom containers so they can be used as an environment to run CI tasks in.  Cirrus CI can do it for you! Just declare a path to a <code>Dockerfile</code> with the <code>dockerfile</code> field for your <code>container</code> or <code>arm_container</code> declarations in your <code>.cirrus.yml</code> like this:</p> amd64arm64 <pre><code>efficient_task:\n  container:\n    dockerfile: ci/Dockerfile\n    docker_arguments:\n      foo: bar\n  test_script: ...\n\ninefficient_task:\n  container:\n    image: node:latest\n  setup_script:\n    - apt-get update\n    - apt-get install build-essential\n  test_script: ...\n</code></pre> <pre><code>efficient_task:\n  arm_container:\n    dockerfile: ci/Dockerfile\n    docker_arguments:\n      foo: bar\n  test_script: ...\n\ninefficient_task:\n  arm_container:\n    image: node:latest\n  setup_script:\n    - apt-get update\n    - apt-get install build-essential\n  test_script: ...\n</code></pre> <p>Cirrus CI will build a container and cache the resulting image based on <code>Dockerfile</code>\u2019s content. On the next build,  Cirrus CI will check if a container was already built, and if so, Cirrus CI will instantly start a CI task using the cached image.</p> <p>Under the hood, for every <code>Dockerfile</code> that is needed to be built, Cirrus CI will create a Docker Builder task as a dependency.  You will see such <code>build_docker_image_HASH</code> tasks in the UI.</p> Danger of using <code>COPY</code> and <code>ADD</code> instructions <p>Cirrus only includes files directly added or copied into a container image in the cache key. But Cirrus is not  recursively  waking contents of folders that are being included into the image. This means that for a public repository a potential bad actor  can create a PR with malicious scripts included into a container, wait for it to be cached and then reset the PR, so it looks harmless.</p> <p>Please try to only <code>COPY</code> files by full path, e.g.:</p> <pre><code>FROM python:3\n\nCOPY requirements.txt /tmp/\nRUN pip install --requirement /tmp/requirements.txt\n</code></pre> Using with private GKE clusters <p>To use <code>dockerfile</code> with <code>gke_container</code> you first need to create a VM with Docker installed within your GCP project. This image will be used to perform building of Docker images for caching. Once this image is available, for example, by  <code>MY_DOCKER_VM</code> name, you can use it like this:</p> <pre><code>gke_container:\n  dockerfile: .ci/Dockerfile\n  builder_image_name: MY_DOCKER_VM\n  cluster_name: cirrus-ci-cluster\n  zone: us-central1-a\n  namespace: default\n</code></pre> <p>Please make sure your buidler image has <code>gcloud</code> configured as a credential helper.</p> <p>If your builder image is stored in another project you can also specify it by using <code>builder_image_project</code> field. By default, Cirrus CI assumes builder image is stored within the same project as the GKE cluster.</p> Using with private EKS clusters <p>To use <code>dockerfile</code> with <code>eks_container</code> you need three things:</p> <ol> <li>Either create an AMI with Docker installed or use one like ECS-optimized AMIa. For example, <code>MY_DOCKER_AMI</code>.</li> <li>Create a role which has <code>AmazonEC2ContainerRegistryFullAccess</code> policy. For example, <code>cirrus-builder</code>.</li> <li>Create <code>cirrus-cache</code> repository in your Elastic Container registry and make sure user that <code>aws_credentials</code> are associated with has <code>ecr:DescribeImages</code> access to it.</li> </ol> <p>Once all of the above requirement are met you can configure <code>eks_container</code> like this:</p> <pre><code>eks_container:\n  region: us-east-2\n  cluster_name: my-company-arm-cluster\n  dockerfile: .ci/Dockerfile\n  builder_image: MY_DOCKER_AMI\n  builder_image_owners: # optional, defaults to self, amazon and aws-marketplace\n    - self\n  builder_role: cirrus-builder # role for builder instance profile\n  builder_instance_type: c7g.xlarge # should match the architecture below\n  builder_subnet_ids: # optional, list of subnets from your default VPC to randomly choose from for scheduling the instance\n    - ...\n  builder_subnet_filters: # optional, map of filters to use for DescribeSubnets API call. Note to make sure Cirrus is given `ec2:DescribeSubnets`  \n    - name: tag:Name\n      values:\n        - subnet1\n        - subnet2\n  architecture: arm64 # default is amd64\n</code></pre> <p>This will make Cirrus CI to check whether <code>cirrus-cache</code> repository in <code>us-east-2</code> region contains a precached image for <code>.ci/Dockerfile</code> of this repository. </p>"},{"location":"guide/docker-builder-vm/#windows-support","title":"Windows Support","text":"<p>Docker builders also support building Windows Docker containers - use the <code>platform</code> and <code>os_version</code> fields:</p> <pre><code>docker_builder:\n  platform: windows\n  ...\n</code></pre>"},{"location":"guide/docker-builds-on-kubernetes/","title":"Docker Builds on GKE","text":""},{"location":"guide/docker-builds-on-kubernetes/#docker-builds-on-kubernetes","title":"Docker Builds on Kubernetes","text":"<p>Besides the ability to build docker images using a dedicated <code>docker_builder</code> task which runs on VMs, it is also possible to run docker builds on Kubernetes. To do so we are leveraging the <code>additional_containers</code> and <code>docker-in-docker</code> functionality.</p> <p>Currently Cirrus CI supports running builds on these Kubernetes distributions:</p> <ul> <li>Google Kubernetes Engine (GKE)</li> <li>AWS Elastic Kubernetes Service (EKS)</li> </ul> <p>For Generic Kubernetes Support follow this issue.</p>"},{"location":"guide/docker-builds-on-kubernetes/#comparison-of-docker-builds-on-vms-vs-kubernetes","title":"Comparison of docker builds on VMs vs Kubernetes","text":"<ul> <li>VMs<ul> <li>complex builds are potentially faster than <code>docker-in-docker</code></li> <li>safer due to better isolation between builds</li> </ul> </li> <li>Kubernetes<ul> <li>much faster start - creating a new container usually takes few seconds vs creating a VM which takes usually about a minute on GCP and even longer on AWS.</li> <li>ability to use an image with your custom tools image (e.g. containing Skaffold) to invoke docker instead of using a fixed VM image.</li> </ul> </li> </ul>"},{"location":"guide/docker-builds-on-kubernetes/#how-to","title":"How to","text":"<p>This a full example of how to build a docker image on GKE using docker and pushing it to GCR. While not required, the script section in this example also has some best practice cache optimizations and pushes the image to GCR.</p> <p>AWS EKS support</p> <p>While the steps below are specifically written for and tested with GKE (Google Kubernetes Engine), it should work equally on AWS EKS.</p> <pre><code>docker_build_task:\n  gke_container: # for AWS, replace this with `aks_container`\n    image: docker:latest # This image can be any custom image. The only hard requirement is that it needs to have `docker-cli` installed.\n    cluster_name: cirrus-ci-cluster # your gke cluster name\n    zone: us-central1-b # zone of the cluster\n    namespace: cirrus-ci # namespace to use\n    cpu: 1\n    memory: 1500Mb\n    additional_containers:\n      - name: dockerdaemon\n        privileged: true # docker-in-docker needs to run in privileged mode\n        cpu: 4\n        memory: 3500Mb\n        image: docker:dind\n        port: 2375\n        env:\n          DOCKER_DRIVER: overlay2 # this speeds up the build\n          DOCKER_TLS_CERTDIR: \"\" # disable TLS to preserve the old behavior\n  env:\n    DOCKER_HOST: tcp://localhost:2375 # this is required so that docker cli commands connect to the \"additional container\" instead of `docker.sock`.\n    GOOGLE_CREDENTIALS: ENCRYPTED[qwerty239abc] # this should contain the json key for a gcp service account with the `roles/storage.admin` role on the `artifacts.&lt;your_gcp_project&gt;.appspot.com` bucket as described here https://cloud.google.com/container-registry/docs/access-control. This is only required if you want to pull / push to gcr. If we use dockerhub you need to use different credentials.\n  login_script:\n    echo $GOOGLE_CREDENTIALS | docker login -u _json_key --password-stdin https://gcr.io\n  build_script:\n    - docker pull gcr.io/my-project/my-app:$CIRRUS_LAST_GREEN_CHANGE || true\n    - docker build\n      --cache-from=gcr.io/my-project/my-app:$CIRRUS_LAST_GREEN_CHANGE\n      -t gcr.io/my-project/my-app:$CIRRUS_CHANGE_IN_REPO \n      .   \n  push_script:\n    - docker push gcr.io/my-project/my-app:$CIRRUS_CHANGE_IN_REPO \n</code></pre>"},{"location":"guide/docker-builds-on-kubernetes/#caveats","title":"Caveats","text":"<p>Since the <code>additional_container</code> needs to run in privileged mode, the isolation between the Docker build and the host are somewhat limited, you should create a separate cluster for Cirrus CI builds ideally. If this a concern you can also try out Kaniko or Makisu to run builds in unprivileged containers.</p>"},{"location":"guide/docker-pipe/","title":"Docker Pipe","text":"<p>Docker Pipe is a way to execute each instruction in its own Docker container while persisting working directory between each of the containers. For example, you can build your application in  one container, run some lint tools in another containers and finally deploy your app via CLI from another container.</p> <p>No need to create huge containers with every single tool pre-installed!</p> <p>A <code>pipe</code> can be defined the same way as a <code>task</code> with the only difference that instructions should be grouped under the <code>steps</code> field defining a Docker <code>image</code> for each step to be executed in. Here is an example of how we build and validate links for the Cirrus CI documentation that you are reading right now:</p> <pre><code>pipe:\n  name: Build Site and Validate Links\n  steps:\n    - image: squidfunk/mkdocs-material:latest\n      build_script: mkdocs build\n    - image: raviqqe/liche:latest # links validation tool in a separate container\n      validate_script: /liche --document-root=site --recursive site/\n</code></pre> <p>Amount of CPU and memory that a pipe has access to can be configured with <code>resources</code> field:</p> <pre><code>pipe:\n  resources:\n    cpu: 2.5\n    memory: 5G\n  # ...\n</code></pre>"},{"location":"guide/linux/","title":"Linux Containers","text":""},{"location":"guide/linux/#linux-containers","title":"Linux Containers","text":"<p>Cirrus CI supports <code>container</code> and <code>arm_container</code> instances in order to run your CI workloads on <code>amd64</code> and <code>arm64</code> platforms respectively. Cirrus CI uses Kubernetes clusters running in different clouds that are the most suitable for running each platform:</p> <ul> <li>For <code>container</code> instances Cirrus CI uses a GKE cluster of compute-optimized instances running in Google Cloud.</li> <li>For <code>arm_container</code> instances Cirrus CI uses a EKS cluster of Graviton2 instances running in AWS.</li> </ul> <p>Cirrus Cloud Clusters are configured the same way as anyone can configure a private Kubernetes cluster for their own repository. Cirrus CI supports connecting managed Kubernetes clusters from most of the cloud providers. Please check out all the supported computing services Cirrus CI can integrate with.</p> <p>By default, a container is given 2 CPUs and 4 GB of memory, but it can be configured in <code>.cirrus.yml</code>:</p> amd64arm64 <pre><code>container:\n  image: openjdk:latest\n  cpu: 4\n  memory: 12G\n\ntask:\n  script: ...\n</code></pre> <pre><code>arm_container:\n  image: openjdk:latest\n  cpu: 4\n  memory: 12G\n\ntask:\n  script: ...\n</code></pre> <p>Containers on Cirrus Cloud Cluster can use maximum 8.0 CPUs and up to 32 GB of memory. Memory limit is tied to the amount of CPUs requested. For each CPU you can't get more than 4G of memory.</p> <p>Tasks using Compute Credits has higher limits and can use up to 28.0 CPUs and 112G of memory respectively.</p> Using in-memory disks <p>Some I/O intensive tasks may benefit from using a <code>tmpfs</code> disk mounted as a working directory. Set <code>use_in_memory_disk</code> flag to enable in-memory disk for a container:</p> amd64arm64 <pre><code>task:\n  name: Much I/O\n  container:\n    image: alpine:latest\n    use_in_memory_disk: true\n</code></pre> <pre><code>task:\n  name: Much I/O\n  arm_container:\n    image: alpine:latest\n    use_in_memory_disk: true\n</code></pre> <p>Note: any files you write including cloned repository will count against your task's memory limit.</p> Privileged Access <p>If you need to run privileged docker containers, take a look at the docker builder.</p> Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>"},{"location":"guide/linux/#kvm-enabled-privileged-containers","title":"KVM-enabled Privileged Containers","text":"<p>It is possible to run containers with KVM enabled. Some types of CI tasks can tremendously benefit from native virtualization. For example, Android related tasks can benefit from running hardware accelerated emulators instead of software emulated ARM emulators.</p> <p>In order to enable KVM module for your <code>container</code>s, add <code>kvm: true</code> to your <code>container</code> declaration. Here is an example of a task that runs hardware accelerated Android emulators:</p> <pre><code>task:\n  name: Integration Tests (x86)\n  container:\n    image: ghcr.io/cirruslabs/android-sdk:30\n    kvm: true\n  accel_check_script: emulator -accel-check\n</code></pre> <p>Limitations of KVM-enabled Containers</p> <p>Because of the additional virtualization layer, it takes about a minute to acquire the necessary resources to start such tasks. KVM-enabled containers are backed by dedicated VMs which restrict the amount of CPU resources that can be used. The value of <code>cpu</code> must be <code>1</code> or an even integer. Values like <code>0.5</code> or <code>3</code> are not supported for KVM-enabled containers </p>"},{"location":"guide/linux/#working-with-private-registries","title":"Working with Private Registries","text":"<p>It is possible to use private Docker registries with Cirrus CI to pull containers. To provide an access to a private registry  of your choice you'll need to obtain a JSON Docker config file for your registry and create an encrypted variable for Cirrus CI to use.</p> Using Kubernetes secrets with private clusters <p>Alternatively, if you are using Cirrus CI with your private Kubernetes cluster you can create a <code>kubernetes.io/dockerconfigjson</code> secret and just use it's name for <code>registry_config</code>:</p> <pre><code>task:\n  gke_container:\n    image: secret:image\n    registry_config: myregistrykey\n</code></pre> <p>Let's check an example of setting up Oracle Container Registry in order to use Oracle Database in tests.</p> <p>First, you'll need to login with the registry by running the following command:</p> <pre><code>docker login container-registry.oracle.com\n</code></pre> <p>After a successful login, Docker config file located in <code>~/.docker/config.json</code> will look something like this:</p> <pre><code>{\n  \"auths\": {\n    \"container-registry.oracle.com\": {\n      \"auth\": \"....\"\n    }\n  }\n}\n</code></pre> <p>If you don't see <code>auth</code> for your registry, it means your Docker installation is using a credentials store. In this case you can manually auth using a Base64 encoded string of your username and your PAT (Personal Access Token). Here's how to generate that:</p> <pre><code>echo $USERNAME:$PAT | base64\n</code></pre> <p>Create an encrypted variable from the Docker config and put in <code>.cirrus.yml</code>:</p> <pre><code>registry_config: ENCRYPTED[...]\n</code></pre> <p>Now Cirrus CI will be able to pull images from Oracle Container Registry:</p> <pre><code>registry_config: ENCRYPTED[...]\n\ntest_task:\n  container:\n    image: bigtruedata/sbt:latest\n    additional_containers:\n      - name: oracle\n        image: container-registry.oracle.com/database/standard:latest\n        port: 1521\n        cpu: 1\n        memory: 8G\n   build_script: ./build/build.sh\n</code></pre>"},{"location":"guide/macOS/","title":"macOS VMs","text":""},{"location":"guide/macOS/#macos-virtual-machines","title":"macOS Virtual Machines","text":"<p>It is possible to run M1 macOS Virtual Machines (like how one can run Linux containers) on the Cirrus Cloud macOS Cluster.  Use <code>macos_instance</code> in your <code>.cirrus.yml</code> files:</p> <pre><code>macos_instance:\n  image: ghcr.io/cirruslabs/macos-runner:sonoma\n\ntask:\n  script: echo \"Hello World from macOS!\"\n</code></pre>"},{"location":"guide/macOS/#available-images","title":"Available images","text":"<p>Cirrus CI is using Tart virtualization for running macOS Virtual Machines on Apple Silicon. Cirrus CI Cloud only allows <code>ghcr.io/cirruslabs/macos-runner:sonoma</code> image which contains the last 3 versions of Xcode.</p> <p>Underlying Orchestration Technology</p> <p>Under the hood Cirrus CI is using Cirrus CI's own Persistent Workers. See more details in out blog post.</p>"},{"location":"guide/notifications/","title":"Notifications","text":"<p>Cirrus CI itself doesn't have built-in mechanism to send notifications but, since Cirrus CI is following best practices of integrating with GitHub, it's possible to configure a GitHub action that will send any kind of notifications.</p> <p>Here is a full list of curated Cirrus Actions for GitHub including ones to send notifications: cirrus-actions.</p>"},{"location":"guide/notifications/#email-action","title":"Email Action","text":"<p>It's possible to facilitate GitHub Action's own email notification mechanism to send emails about Cirrus CI failures.  To enable it, add the following <code>.github/workflows/email.yml</code> workflow file:</p> <pre><code>on:\n  check_suite:\n    type: ['completed']\n\nname: Email about Cirrus CI failures\njobs:\n  continue:\n    name: After Cirrus CI Failure\n    if: &gt;-\n      github.event.check_suite.app.name == 'Cirrus CI'\n      &amp;&amp; github.event.check_suite.conclusion != 'success'\n      &amp;&amp; github.event.check_suite.conclusion != 'cancelled'\n      &amp;&amp; github.event.check_suite.conclusion != 'skipped'\n      &amp;&amp; github.event.check_suite.conclusion != 'neutral'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: octokit/request-action@v2.x\n        id: get_failed_check_run\n        with:\n          route: GET /repos/${{ github.repository }}/check-suites/${{ github.event.check_suite.id }}/check-runs?status=completed\n          mediaType: '{\"previews\": [\"antiope\"]}'\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - run: |\n          echo \"Cirrus CI ${{ github.event.check_suite.conclusion }} on ${{ github.event.check_suite.head_branch }} branch!\"\n          echo \"SHA ${{ github.event.check_suite.head_sha }}\"\n          echo $MESSAGE\n          echo \"##[error]See $CHECK_RUN_URL for details\" &amp;&amp; false\n        env:\n          CHECK_RUN_URL: ${{ fromJson(steps.get_failed_check_run.outputs.data).check_runs[0].html_url }}\n</code></pre>"},{"location":"guide/persistent-workers/","title":"Persistent Workers","text":""},{"location":"guide/persistent-workers/#persistent-workers","title":"Persistent Workers","text":""},{"location":"guide/persistent-workers/#introduction","title":"Introduction","text":"<p>Cirrus CI pioneered an idea of directly using compute services instead of requiring users to manage their own infrastructure, configuring servers for running CI jobs, performing upgrades, etc. Instead, Cirrus CI just uses APIs of cloud providers to create virtual machines or containers on demand. This fundamental design difference has multiple benefits comparing to more traditional CIs:</p> <ol> <li>Ephemeral environment. Each Cirrus CI task starts in a fresh VM or a container without any state left by previous tasks.</li> <li>Infrastructure as code. All VM versions and container tags are specified in <code>.cirrus.yml</code> configuration file in your Git repository.    For any revision in the past Cirrus tasks can be identically reproduced at any point in time in the future using the exact versions of VMs or container tags specified in <code>.cirrus.yml</code> at the particular revision. Just imagine how difficult it is to do a security release for a 6 months old version if your CI environment independently changes.</li> <li>Predictability and cost efficiency. Cirrus CI uses elasticity of modern clouds and creates VMs and containers on demand    only when they are needed for executing Cirrus tasks and deletes them right after. Immediately scale from 0 to hundreds or    thousands of parallel Cirrus tasks without a need to over provision infrastructure or constantly monitor if your team has reached maximum parallelism of your current CI plan.</li> </ol>"},{"location":"guide/persistent-workers/#what-is-a-persistent-worker","title":"What is a Persistent Worker","text":"<p>For some use cases the traditional CI setup is still useful since not everything is available in the cloud. For example, testing hardware itself or some third party devices that can be attached with wires. For such use cases it makes sense to go with a traditional CI setup: install some binary on the hardware which will constantly pull for new tasks  and will execute them one after another.</p> <p>This is precisely what Persistent Workers for Cirrus CI are: a simple way to run Cirrus tasks beyond cloud!</p>"},{"location":"guide/persistent-workers/#configuration","title":"Configuration","text":"<p>First, create a persistent workers pool for your personal account or a GitHub organization (<code>https://cirrus-ci.com/settings/github/&lt;ORGANIZATION&gt;</code>):</p> <p></p> <p>Once a persistent worker is created, copy registration token of the pool and follow Cirrus CLI guide to configure a host that will be a persistent worker.</p> <p>Once configured, target task execution on a worker by using <code>persistent_worker</code> instance and matching by workers' labels:</p> <pre><code>task:\n  persistent_worker:\n    labels:\n      os: darwin\n      arch: arm64\n  script: echo \"running on-premise\"\n</code></pre> <p>Or remove <code>labels</code> filed if you want to target any worker:</p> <pre><code>task:\n  persistent_worker: {}\n  script: echo \"running on-premise\"\n</code></pre>"},{"location":"guide/persistent-workers/#resource-management","title":"Resource management","text":"<p>By default, Cirrus CI limits task concurrency to 1 task per each worker. To schedule more tasks on a given worker, configure it's <code>resources</code>.</p> <p>Once done, the worker will be considered resource-aware and will be able to execute concurrently either:</p> <ul> <li>one resource-less task (a task without <code>resources:</code> field)</li> <li>multiple resourceful tasks (a task with <code>resources:</code> field) as long worker has resources available for these tasks</li> </ul> <p>Note that <code>labels</code> matching still takes place for both resource-less and resource-aware tasks.</p> <p>So, considering a worker with the following configuration:</p> <pre><code>token: \"[snip]\"\n\nname: \"mac-mini-usb-hub\"\n\nresources:\n  connected-iphones: 4\n  connected-ipads: 2\n</code></pre> <p>It will be able to concurrently execute one of this task:</p> <pre><code>task:\n  name: Test iPhones and iPads\n\n  persistent_worker:\n    resources:\n      connected-iphones: 2\n      connected-ipads: 2\n\n  script: make test\n</code></pre> <p>And two of these:</p> <pre><code>task:\n  name: Test iPhones only\n\n  persistent_worker:\n    resources:\n      connected-iphones: 2\n\n  script: make test\n</code></pre>"},{"location":"guide/persistent-workers/#isolation","title":"Isolation","text":"<p>By default, a persistent worker spawns all the tasks on the same host machine it's being run.</p> <p>However, using the <code>isolation</code> field, a persistent worker can utilize a VM or a container engine to increase the separation between tasks and to unlock the ability to use different operating systems.</p>"},{"location":"guide/persistent-workers/#tart","title":"Tart","text":"<p>To use this isolation type, install the Tart on the persistent worker's host machine.</p> <p>Here's an example of a configuration that will run the task inside of a fresh macOS virtual machine created from a remote <code>ghcr.io/cirruslabs/macos-ventura-base:latest</code> VM image:</p> <pre><code>persistent_worker:\n  isolation:\n    tart:\n      image: ghcr.io/cirruslabs/macos-ventura-base:latest\n      user: admin\n      password: admin\n\ntask:\n  script: system_profiler\n</code></pre> <p>Once the VM spins up, persistent worker will connect to the VM's IP-address over SSH using <code>user</code> and <code>password</code> credentials and run the latest agent version.</p>"},{"location":"guide/persistent-workers/#container","title":"Container","text":"<p>To use this isolation type, install and configure a container engine like Docker or Podman (essentially the ones supported by the Cirrus CLI).</p> <p>Here's an example that runs a task in a separate container with a couple directories from the host machine being accessible:</p> <pre><code>persistent_worker:\n  isolation:\n    container:\n      image: debian:latest\n      cpu: 24\n      memory: 128G\n      volumes:\n        - /path/on/host:/path/in/container\n        - /tmp/persistent-cache:/tmp/cache:ro\n\ntask:\n  script: uname -a\n</code></pre>"},{"location":"guide/programming-tasks/","title":"Programming Tasks in Starlark","text":""},{"location":"guide/programming-tasks/#introduction-into-starlark","title":"Introduction into Starlark","text":"<p>Most commonly, Cirrus tasks are declared in a <code>.cirrus.yml</code> file in YAML format as documented in the Writing Tasks guide.</p> <p>YAML, as a language, is great for declaring simple to moderate configurations, but sometimes just using a declarative language is not enough. One might need some conditional execution or an easy way to generate multiple similar tasks. Most continuous integration services solve this problem by introducing a special domain specific language (DSL) into the existing YAML. In case of Cirrus CI, we have the <code>only_if</code> keyword for conditional execution and <code>matrix</code> modification for generating similar tasks. These options are mostly hacks to work around the declarative nature of YAML where in reality an imperative language would be a better fit. This is why Cirrus CI allows tasks to be configured in Starlark in addition to YAML.</p> <p>Starlark is a procedural programming language similar to Python that originated in the Bazel build tool that is ideal for embedding within systems that want to safely allow user-defined logic. There are a few key differences that made us choose Starlark instead of common alternatives like JavaScript/TypeScript or WebAssembly:</p> <ol> <li>Starlark doesn't require compilation. There's no need to introduce a full-blown compile and deploy process for a few dozen lines of logic.</li> <li>Starlark scripts can be executed instantly on any platform. There is Starlark interpreter written in Go which integrates nicely with the Cirrus CLI and Cirrus CI infrastructure.</li> <li>Starlark has built-in functionality for loading external modules which is ideal for config sharing. See module loading for details.</li> </ol>"},{"location":"guide/programming-tasks/#writing-starlark-scripts","title":"Writing Starlark scripts","text":"<p>Let's start with a trivial <code>.cirrus.star</code> example:</p> <pre><code>def main():\n    return [\n        {\n            \"container\": {\n                \"image\": \"debian:latest\",\n            },\n            \"script\": \"make\",\n        },\n    ]\n</code></pre> <p>With module loading you can re-use other people's code to avoid wasting time writing tasks from scratch. For example, with the official task helpers the example above can be refactored to:</p> <pre><code>load(\"github.com/cirrus-modules/helpers\", \"task\", \"container\", \"script\")\n\ndef main(ctx):\n  return [\n    task(\n      instance=container(\"debian:latest\"),\n      instructions=[script(\"make\")]\n    ),\n  ]\n</code></pre> <p><code>main()</code> needs to return a list of task objects, which will be serialized into YAML like this:</p> <pre><code>task:\n    container:\n      image: debian:latest\n    script: make\n</code></pre> <p>Then the generated YAML is appended to <code>.cirrus.yml</code> (if any) before passing the combined config into the final YAML parser.</p> <p>With Starlark, it's possible to generate parts of the configuration dynamically based on some external conditions:</p> <ul> <li>Parsing files inside the repository to pick up some common settings (for example, parse <code>package.json</code> to see if it contains a <code>lint</code> script and generate a linting task).</li> <li>Making an HTTP request to check the previous build status.</li> </ul> <p>See a video tutorial on how to create a custom Cirrus module:</p>"},{"location":"guide/programming-tasks/#entrypoints","title":"Entrypoints","text":"<p>Different events will trigger execution of different top-level functions in the <code>.cirrus.star</code> file. These functions reserve certain names and will be called with different arguments depending on the event which triggered the execution.</p>"},{"location":"guide/programming-tasks/#main","title":"<code>main()</code>","text":"<p><code>main()</code> is called once a Cirrus CI build is triggered in order to generate additional configuration that will be appended to <code>.cirrus.yml</code> before parsing.</p> <p><code>main</code> function can return a single object or a list of objects which will be automatically serialized into YAML. In case of returning plain text, it will be appended to <code>.cirrus.yml</code> as is.</p> <p>Note that <code>.cirrus.yml</code> configuration file is optional and the whole build can be generated via evaluation of <code>.cirrus.star</code> file. </p> <pre><code>def main():\n    return [\n      {\n        \"container\": {\n          \"image\": \"debian:latest\"\n        },\n        \"script\": \"make test\"\n      },\n      {\n        \"container\": {\n          \"image\": \"debian:latest\"\n        },\n        \"script\": \"make build\"\n      }\n    ]\n</code></pre> <p>Sometimes, you might only need to override a specific global field like <code>env</code> or <code>container</code>. This can be achieved by returning a dictionary:</p> <pre><code>def main():\n  return {\n    \"env\": {\n      \"VARIABLE_NAME\": \"VARIABLE_VALUE\",\n    },\n    \"container\": {\n      \"image\": \"debian:latest\",\n    }\n  }\n</code></pre> <p>Or you can simply emit a string containing the YAML configuration. This allows splitting YAML configuration across multiple files like this:</p> <pre><code>load(\"cirrus\", \"env\", \"fs\")\n\ndef main(ctx):\n  if env.get(\"CIRRUS_TAG\") != None:\n    return fs.read(\".cirrus.release.yml\")\n  if env.get(\"CIRRUS_PR\") != None:\n    return fs.read(\".cirrus.pr.yml\")\n\n  return fs.read(\".cirrus.pr.yml\") + fs.read(\".cirrus.e2e.yml\")\n</code></pre> <p>If you want to return multiple tasks with the same name or a top-level override like <code>env</code>, use the tuple syntax below:</p> <pre><code>def main():\n    return [\n      (\"env\", {\"PARALLEL\": \"yes\"}),\n      (\"container\", {\"image\": \"debian:latest\"}),\n      (\"task\", {\"script\": \"make build\"}),\n      (\"task\", {\"script\": \"make test\"})\n    ]\n</code></pre>"},{"location":"guide/programming-tasks/#hooks","title":"Hooks","text":"<p>It's also possible to execute Starlark scripts on updates to the current build or any of the tasks within the build. Think of it as WebHooks running within Cirrus that don't require any infrastructure on your end.</p> <p>Expected names of Starlark Hook functions in <code>.cirrus.star</code> are <code>on_build_&lt;STATUS&gt;</code> or <code>on_task_&lt;STATUS&gt;</code> respectively. Please refer to Cirrus CI GraphQL Schema for a full list of existing statuses, but most commonly  <code>on_build_failed</code>/<code>on_build_completed</code> and <code>on_task_failed</code>/<code>on_task_completed</code> are used. These functions should expect a single context argument passed by Cirrus Cloud. At the moment hook's context only contains a single field <code>payload</code> containing the same payload as a webhook. </p> <p>One caveat of Starlark Hooks execution is <code>CIRRUS_TOKEN</code> environment variable that contains a token to access Cirrus API. Scope of <code>CIRRUS_TOKEN</code> is restricted to the build associated with that particular hook invocation and allows, for example, to automatically re-run tasks. Here is an example of a Starlark Hook that automatically re-runs a failed task in case a particular transient issue found in logs:</p> <pre><code># load some helpers from an external module \nload(\"github.com/cirrus-modules/graphql\", \"rerun_task_if_issue_in_logs\")\n\ndef on_task_failed(ctx):\n  if \"Test\" not in ctx.payload.data.task.name:\n    return\n  if ctx.payload.data.task.automaticReRun:\n    print(\"Task is already an automatic re-run! Won't even try to re-run it...\")\n    return\n  rerun_task_if_issue_in_logs(ctx.payload.data.task.id, \"Time out\")\n</code></pre>"},{"location":"guide/programming-tasks/#module-loading","title":"Module loading","text":"<p>Module loading is done through the Starlark's <code>load()</code> statement.</p> <p>Besides the ability to load builtins with it, Cirrus can load other <code>.star</code> files from local and remote locations to facilitate code re-use.</p>"},{"location":"guide/programming-tasks/#local","title":"Local","text":"<p>Local loads are relative to the project's root (where <code>.cirrus.star</code> is located):</p> <pre><code>load(\".ci/notify-slack.star\", \"notify_slack\")\n</code></pre>"},{"location":"guide/programming-tasks/#remote-from-git","title":"Remote from Git","text":"<p>To load the default branch of the module from GitHub:</p> <pre><code>load(\"github.com/cirrus-modules/golang\", \"task\", \"container\")\n</code></pre> <p>In the example above, the name of the <code>.star</code> file was not provided, because <code>lib.star</code> is assumed by default. This is equivalent to:</p> <pre><code>load(\"github.com/cirrus-modules/golang/lib.star@main\", \"task\", \"container\")\n</code></pre> <p>You can also specify an exact commit hash instead of the <code>main()</code> branch name to prevent accidental changes.</p> <p>Loading private modules</p> <p>If your organization has private repository called <code>cirrus-modules</code> with installed Cirrus CI, then this repository will be available for loading within repositories of your organization.</p> <p>To load <code>.star</code> files from repositories other than GitHub, add a <code>.git</code> suffix at the end of the repository name, for example:</p> <pre><code>load(\"gitlab.com/fictional/repository.git/validator.star\", \"validate\")\n                                     ^^^^ note the suffix\n</code></pre>"},{"location":"guide/programming-tasks/#builtins","title":"Builtins","text":"<p>Cirrus CLI provides builtins all nested in the <code>cirrus</code> module that greatly extend what can be done with the Starlark alone.</p>"},{"location":"guide/programming-tasks/#fs","title":"<code>fs</code>","text":"<p>These builtins allow for read-only filesystem access.</p> <p>The <code>path</code> argument used in the methods below re-uses the module loader's logic and thus can point to a file/directory:</p> <ul> <li>relative to the project's directory<ul> <li>e.g. <code>.cirrus.yml</code></li> </ul> </li> <li>in a GitHub repository<ul> <li>e.g. <code>github.com/cirruslabs/cirrus-ci-docs/.cirrus.yml@master</code></li> </ul> </li> <li>in remote Git repository<ul> <li>e.g. <code>gitlab.com/fictional/repository.git/.cirrus.yml</code></li> </ul> </li> </ul>"},{"location":"guide/programming-tasks/#fsexistspath","title":"<code>fs.exists(path)</code>","text":"<p>Returns <code>True</code> if <code>path</code> exists and <code>False</code> otherwise.</p>"},{"location":"guide/programming-tasks/#fsisdirpath","title":"<code>fs.isdir(path)</code>","text":"<p>Returns <code>True</code> if <code>path</code> points to a directory and <code>False</code> otherwise.</p>"},{"location":"guide/programming-tasks/#fsreadpath","title":"<code>fs.read(path)</code>","text":"<p>Returns a <code>string</code> with the file contents or <code>None</code> if the file doesn't exist.</p> <p>Note that this is an error to read a directory with <code>fs.read()</code>.</p>"},{"location":"guide/programming-tasks/#fsreaddirdirpath","title":"<code>fs.readdir(dirpath)</code>","text":"<p>Returns a <code>list</code> of <code>string</code>'s with names of the entries in the directory or <code>None</code> if the directory does not exist.</p> <p>Note that this is an error to read a file with <code>fs.readdir()</code>.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"fs\")\n\ndef main(ctx):\n    tasks = base_tasks()\n\n    if fs.exists(\"go.mod\"):\n        tasks += go_tasks()\n\n    return tasks\n</code></pre>"},{"location":"guide/programming-tasks/#is_test","title":"<code>is_test</code>","text":"<p>While not technically a builtin, <code>is_test</code> is a <code>bool</code> that allows Starlark code to determine whether it's running in test environment via Cirrus CLI. This can be useful for limiting the test complexity, e.g. by not making a real HTTP request and mocking/skipping it instead. Read more about module testing in a separate guide in Cirrus CLI repository.</p>"},{"location":"guide/programming-tasks/#env","title":"<code>env</code>","text":"<p>While not technically a builtin, <code>env</code> is dict that contains environment variables.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"env\")\n\ndef main(ctx):\n    tasks = base_tasks()\n\n    if env.get(\"CIRRUS_TAG\") != None:\n        tasks += release_tasks()\n\n    return tasks\n</code></pre>"},{"location":"guide/programming-tasks/#changes_include","title":"<code>changes_include</code>","text":"<p><code>changes_include()</code> is a Starlark alternative to the changesInclude() function commonly found in the YAML configuration files.</p> <p>It takes at least one <code>string</code> with a pattern and returns a <code>bool</code> that represents whether any of the specified patterns matched any of the affected files in the running context.</p> <p>Currently supported contexts:</p> <ul> <li><code>main()</code> entrypoint</li> </ul> <p>Example:</p> <pre><code>load(\"cirrus\", \"changes_include\", \"fs\")\n\ndef main(ctx):\n    result = \"\"\n\n    if changes_include(\"docs/*\"):\n        result += fs.read(\".cirrus.docs.yml\") + \"\\n\"\n\n    if changes_include(\"web/*\"):\n        result += fs.read(\".cirrus.web.yml\") + \"\\n\"\n\n    if changes_include(\"server/*\"):\n        result += fs.read(\".cirrus.server.yml\") + \"\\n\"\n\n    return result\n</code></pre>"},{"location":"guide/programming-tasks/#changes_include_only","title":"<code>changes_include_only</code>","text":"<p><code>changes_include_only()</code> is a Starlark alternative to the changesIncludeOnly() function commonly found in the YAML configuration files.</p> <p>It takes at least one <code>string</code> with a pattern and returns a <code>bool</code> that represents whether any of the specified patterns matched all the affected files in the running context.</p> <p>Currently supported contexts:</p> <ul> <li><code>main()</code> entrypoint</li> </ul> <p>Example:</p> <pre><code>load(\"cirrus\", \"changes_include_only\")\n\ndef main(ctx):\n    # skip if only documentation changed\n    if changes_include_only(\"doc/*\"):\n        return []\n\n    # ...\n</code></pre>"},{"location":"guide/programming-tasks/#http","title":"<code>http</code>","text":"<p>Provides HTTP client implementation with <code>http.get()</code>, <code>http.post()</code> and other HTTP method functions.</p> <p>Refer to the starlib's documentation for more details.</p>"},{"location":"guide/programming-tasks/#hash","title":"<code>hash</code>","text":"<p>Provides cryptographic hashing functions, such as <code>hash.md5()</code>, <code>hash.sha1()</code> and <code>hash.sha256()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>"},{"location":"guide/programming-tasks/#base64","title":"<code>base64</code>","text":"<p>Provides Base64 encoding and decoding functions using <code>base64.encode()</code> and <code>base64.decode()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>"},{"location":"guide/programming-tasks/#json","title":"<code>json</code>","text":"<p>Provides JSON document marshalling and unmarshalling using <code>json.dumps()</code> and <code>json.loads()</code> functions.</p> <p>Refer to the starlib's documentation for more details.</p>"},{"location":"guide/programming-tasks/#yaml","title":"<code>yaml</code>","text":"<p>Provides YAML document marshalling and unmarshalling using <code>yaml.dumps()</code> and <code>yaml.loads()</code> functions.</p> <p>Refer to the starlib's documentation for more details.</p>"},{"location":"guide/programming-tasks/#re","title":"<code>re</code>","text":"<p>Provides regular expression functions, such as <code>findall()</code>, <code>split()</code> and <code>sub()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>"},{"location":"guide/programming-tasks/#zipfile","title":"<code>zipfile</code>","text":"<p><code>cirrus.zipfile</code> module provides methods to read Zip archives.</p> <p>You instantiate a <code>ZipFile</code> object using <code>zipfile.ZipFile(data)</code> function call and then call <code>namelist()</code> and <code>open(filename)</code> methods to retrieve information about archive contents.</p> <p>Refer to the starlib's documentation for more details.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"fs\", \"zipfile\")\n\ndef is_java_archive(path):\n    # Read Zip archive contents from the filesystem\n    archive_contents = fs.read(path)\n    if archive_contents == None:\n        return False\n\n    # Open Zip archive and a file inside of it\n    zf = zipfile.ZipFile(archive_contents)\n    manifest = zf.open(\"META-INF/MANIFEST.MF\")\n\n    # Does the manifest contain the expected version?\n    if \"Manifest-Version: 1.0\" in manifest.read():\n        return True\n\n    return False\n</code></pre>"},{"location":"guide/quick-start/","title":"Quick Start","text":"<p>At the moment Cirrus CI only supports repositories hosted on GitHub. This guide will walk you through the installation process. If you are interested in a support for other code hosting platforms please fill up this form to help us prioritize the support and notify you once the support is available.</p> <p>Start by configuring the Cirrus CI application from GitHub Marketplace.</p> <p></p> <p>Choose a plan for your personal account or for an organization you have admin writes for.</p> <p></p> <p>GitHub Apps can be installed on all repositories or on repository-by-repository basis for granular access control. For example, Cirrus CI can be installed only on public repositories and will only have access to these public repositories. In contrast, classic OAuth Apps don't have such restrictions.</p> <p></p> <p>Change Repository Access</p> <p>You can always revisit Cirrus CI's repository access settings on your installation page.</p>"},{"location":"guide/quick-start/#post-installation","title":"Post Installation","text":"<p>Once Cirrus CI is installed for a particular repository, you must add either <code>.cirrus.yml</code> configuration or <code>.cirrus.star</code> script to the root of the repository.  The <code>.cirrus.yml</code> defines tasks that will be executed for every build for the repository. </p> <p>For a Node.js project, your <code>.cirrus.yml</code> could look like:</p> amd64arm64 <pre><code>container:\n  image: node:latest\n\ncheck_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn test\n</code></pre> <pre><code>arm_container:\n  image: node:latest\n\ncheck_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn test\n</code></pre> <p>That's all! After pushing a <code>.cirrus.yml</code> a build with all the tasks defined in the <code>.cirrus.yml</code> file will be created.</p> <p>Note: Please check the full guide on configuring Cirrus Tasks and/or check a list of available examples.</p> <p>Zero-config Docker Builds</p> <p>If your repository happened to have a <code>Dockerfile</code> in the root, Cirrus CI will attempt to build it even without a corresponding <code>.cirrus.yml</code> configuration file.</p> <p>You will see all your Cirrus CI builds on cirrus-ci.com once signed in. </p> <p></p> <p>GitHub status checks for each task will appear on GitHub as well.</p> <p></p> <p>Newly created PRs will also get Cirrus CI's status checks.</p> <p></p> <p>Examples</p> <p>Don't forget to check examples page for ready-to-copy examples of some <code>.cirrus.yml</code>  configuration files for different languages and build systems.</p> <p>Life of a build</p> <p>Please check a high level overview of what's happening under the hood when a changed is pushed and this guide to learn more about how to write tasks.</p>"},{"location":"guide/quick-start/#authorization-on-cirrus-ci-web-app","title":"Authorization on Cirrus CI Web App","text":"<p>All builds created by your account can be viewed on Cirrus CI Web App after signing in with your GitHub Account:</p> <p></p> <p>After clicking on <code>Sign In</code> you'll be redirected to GitHub in order to authorize access:</p> <p></p> <p>Note about Act on your behalf</p> <p>Cirrus CI only asks for several kinds of permissions that you can see on your installation page. These permissions are read-only except for write access to checks and commit statuses in order for Cirrus CI to be able to report task statuses via checks or commit statuses.</p> <p>There is a long thread disscussing this weird \"Act on your behalf\" wording here on GitHub's own commuity forum.</p>"},{"location":"guide/quick-start/#enabling-new-repositories-after-installation","title":"Enabling New Repositories after Installation","text":"<p>If you choose initially to allow Cirrus CI to access all of your repositories, all you need to do is push a <code>.cirrus.yml</code> to start building your repository on Cirrus CI.</p> <p>If you only allowed Cirrus CI to access certain repositories, then add your new repository to the list of repositories Cirrus CI has access to via this page, then push a <code>.cirrus.yml</code> to start building on Cirrus CI.</p>"},{"location":"guide/quick-start/#permission-model-for-github-repositories","title":"Permission Model for GitHub Repositories","text":"<p>When a user triggers a build on Cirrus CI by either pushing a change to a repository, creating a PR or a release, Cirrus CI will associate a corresponding user's permissions with the build and tasks within that build. Those permissions are exposed to tasks with <code>CIRRUS_USER_PERMISSIONS</code> environment variable and are mapped to GitHub's collaborator permissions. of the user for the given repository. Only tasks with <code>write</code> and <code>admin</code> permissions will be get decrypted values of the encrypted variables.</p> <p>When working with Cirrus GraphQL API either directly or indirectly through Cirrus CI Web UI, permissions play a key role. Not only one need <code>read</code> permission to view a certain build and tasks of a private repository, but in order to perform any GraphQL mutation one will need at least <code>write</code> permission with a few exceptions:</p> <ul> <li><code>admin</code> permission is required for deleting a repository via <code>RepositoryDeleteMutation</code>.</li> <li><code>admin</code> permission is required for creating API access tokens via <code>GenerateNewOwnerAccessTokenMutation</code> and <code>GenerateNewScopedAccessTokenMutation</code>.</li> </ul> <p>Note that for public repositories <code>none</code> collaborator permission is mapped to <code>read</code> in order to give public view access to anyone.</p>"},{"location":"guide/supported-computing-services/","title":"Computing Services","text":"<p>For every task Cirrus CI starts a new Virtual Machine or a new Docker Container on a given compute service. Using a new VM or a new Docker Container each time for running tasks has many benefits:</p> <ul> <li>Atomic changes to an environment where tasks are executed. Everything about a task is configured in <code>.cirrus.yml</code> file, including   VM image version and Docker Container image version. After committing changes to <code>.cirrus.yml</code> not only new tasks will use the new environment,   but also outdated branches will continue using the old configuration.</li> <li>Reproducibility. Fresh environment guarantees no corrupted artifacts or caches are presented from the previous tasks.</li> <li>Cost efficiency. Most compute services are offering per-second pricing which makes them ideal for using with Cirrus CI.   Also each task for repository can define ideal amount of CPUs and Memory specific for a nature of the task. No need to manage   pools of similar VMs or try to fit workloads within limits of a given Continuous Integration systems.</li> </ul> <p>To be fair there are of course some disadvantages of starting a new VM or a container for every task:</p> <ul> <li>Virtual Machine Startup Speed. Starting a VM can take from a few dozen seconds to a minute or two depending on a cloud provider and   a particular VM image. Starting a container on the other hand just takes a few hundred milliseconds! But even a minute   on average for starting up VMs is not a big inconvenience in favor of more stable, reliable and more reproducible CI.</li> <li>Cold local caches for every task execution. Many tools tend to store some caches like downloaded dependencies locally   to avoid downloading them again in future. Since Cirrus CI always uses fresh VMs and containers such local caches will always   be empty. Performance implication of empty local caches can be avoided by using Cirrus CI features like   built-in caching mechanism. Some tools like Gradle can   even take advantages of built-in HTTP cache!</li> </ul> <p>Please check the list of currently supported cloud compute services below. In case you have your own hardware, please take a look at Persistent Workers, which allow connecting anything to Cirrus CI.</p>"},{"location":"guide/supported-computing-services/#google-cloud","title":"Google Cloud","text":"<p>Cirrus CI can schedule tasks on several Google Cloud Compute services. In order to interact with Google Cloud APIs Cirrus CI needs permissions. Creating a service account is a common way to safely give granular access to parts of Google Cloud Projects.</p> <p>Isolation</p> <p>We do recommend to create a separate Google Cloud project for running CI builds to make sure tests are isolated from production data. Having a separate project also will show how much money is spent on CI and how efficient Cirrus CI is </p> <p>Once you have a Google Cloud project for Cirrus CI please create a service account by running the following command:</p> <pre><code>gcloud iam service-accounts create cirrus-ci \\\n    --project $PROJECT_ID\n</code></pre> <p>Depending on a compute service Cirrus CI will need different roles assigned to the service account. But Cirrus CI will always need permissions to refresh it's token, generate pre-signed URLs (for the artifacts upload/download to work) and be able to view monitoring:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/iam.serviceAccountTokenCreator\n</code></pre> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/monitoring.viewer\n</code></pre> <p>Cirrus CI uses Google Cloud Storage to store logs and caches. In order to give Google Cloud Storage permissions to the service account please run:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/storage.admin\n</code></pre> <p>Default Logs Retentions Period</p> <p>By default Cirrus CI will store logs and caches for 90 days but it can be changed by manually configuring a lifecycle rule for a Google Cloud Storage bucket that Cirrus CI is using.</p>"},{"location":"guide/supported-computing-services/#authorization","title":"Authorization","text":"<p>Now we have a service account that Cirrus CI can use! It's time to let Cirrus CI know about the service account to use. There are two options:</p> <ol> <li>Creating a static credentials file for your service account. Same as you might do for using    with <code>gcloud</code>.</li> <li>Configure Cirrus CI as workload identity provider. This way Cirrus CI will be able    to acquire a temporary credentials for each task</li> </ol>"},{"location":"guide/supported-computing-services/#credentials-key-file","title":"Credentials Key File","text":"<p>A private key can be created by running the following command:</p> <pre><code>gcloud iam service-accounts keys create service-account-credentials.json \\\n  --iam-account cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com\n</code></pre> <p>At last create an encrypted variable from contents of <code>service-account-credentials.json</code> file and add it to the top of <code>.cirrus.yml</code> file:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n</code></pre> <p>Now Cirrus CI can store logs and caches in Google Cloud Storage for tasks scheduled on either GCE or GKE. Please check following sections with additional instructions about Compute Engine or Kubernetes Engine.</p> <p>Supported Regions</p> <p>Cirrus CI currently supports following GCP regions: <code>us-central1</code>, <code>us-east1</code>, <code>us-east4</code>, <code>us-west1</code>, <code>us-west2</code>, <code>europe-west1</code>, <code>europe-west2</code>, <code>europe-west3</code> and <code>europe-west4</code>.</p> <p>Please contact support if you are interested in support for other regions.</p>"},{"location":"guide/supported-computing-services/#workload-identity-federation","title":"Workload Identity Federation","text":"<p>By configuring Cirrus CI as an identity provider, Cirrus CI will be able to acquire temporary access tokens on-demand for each task. Please read Google Cloud documentation to learn more about security and other benefits of using a workload identity provider.</p> <p>Now let's setup Cirrus CI as a workload identity provider:</p> <ol> <li> <p>First, let's make sure the IAM Credentials API is enabled:</p> <pre><code>gcloud services enable iamcredentials.googleapis.com \\\n  --project \"${PROJECT_ID}\"\n</code></pre> </li> <li> <p>Create a Workload Identity Pool:</p> <pre><code>gcloud iam workload-identity-pools create \"ci-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"Continuous Integration\"\n</code></pre> </li> <li> <p>Get the full ID of the Workload Identity Pool:</p> <pre><code>gcloud iam workload-identity-pools describe \"ci-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --format=\"value(name)\"\n</code></pre> <p>Save this value as an environment variable:</p> <pre><code>export WORKLOAD_IDENTITY_POOL_ID=\"...\" # value from above\n</code></pre> </li> <li> <p>Create a Workload Identity Provider in that pool:</p> <pre><code># TODO(developer): Update this value to your GitHub organization.\nexport OWNER=\"organization\" # e.g. \"cirruslabs\"\n\ngcloud iam workload-identity-pools providers create-oidc \"cirrus-oidc\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"ci-pool\" \\\n  --display-name=\"Cirrus CI\" \\\n  --attribute-mapping=\"google.subject=assertion.aud,attribute.owner=assertion.owner,attribute.actor=assertion.repository,attribute.actor_visibility=assertion.repository_visibility,attribute.pr=assertion.pr\" \\\n  --attribute-condition=\"attribute.owner == '$OWNER'\" \\\n  --issuer-uri=\"https://oidc.cirrus-ci.com\"\n</code></pre> <p>The attribute mappings map claims in the Cirrus CI JWT to assertions you can make about the request (like the repository name or repository visibility). In the example above <code>--attribute-condition</code> flag asserts that the provider can be used with any repository of your organization. You can restrict the access further with attributes like <code>repository</code>, <code>repository_visibility</code> and <code>pr</code>.</p> </li> <li> <p>If not yet created, create a Service Account that Cirrus CI will impersonate to manage compute resources and assign it the required roles.</p> </li> <li> <p>Allow authentications from the Workload Identity Provider originating from your organization to impersonate the Service Account created above:</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding \"cirrus-ci@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/${WORKLOAD_IDENTITY_POOL_ID}/attribute.owner/${OWNER}\"\n</code></pre> </li> <li> <p>Extract the Workload Identity Provider resource name:</p> <pre><code>gcloud iam workload-identity-pools providers describe \"cirrus-oidc\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"ci-pool\" \\\n  --format=\"value(name)\"\n</code></pre> <p>Use this value as the <code>workload_identity_provider</code> value in your Cirrus configuration file:</p> <pre><code>gcp_credentials:\n  # todo(developer): replace PROJECT_NUMBER and PROJECT_ID with the actual values\n  workload_identity_provider: projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/ci-pool/providers/cirrus-oidc\n  service_account: cirrus-ci@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre> </li> </ol>"},{"location":"guide/supported-computing-services/#compute-engine","title":"Compute Engine","text":"<p>In order to schedule tasks on Google Compute Engine a service account that Cirrus CI operates via should have a necessary role assigned. It can be done by running a <code>gcloud</code> command:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/compute.admin\n</code></pre> <p>Now tasks can be scheduled on Compute Engine within <code>$PROJECT_ID</code> project by configuring <code>gce_instance</code> something like this:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_family: ubuntu-2204-lts-arm64\n  architecture: arm64 # optional. By default, amd64 is assumed.\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 60\n  use_ssd: true # defaults to false\n  hostname: ubuntu.local\n\ntask:\n  script: ./run-ci.sh\n</code></pre> <p>Specify Machine Type</p> <p>It is possible to specify a predefined machine type via <code>type</code> field:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_family: ubuntu-2204-lts\n  zone: us-central1-a\n  type: n1-standard-8\n  disk: 20\n</code></pre> <p>Specify Image Name</p> <p>It's also possible to specify a concrete image name instead of the periodically rolling image family. Use the <code>image_name</code> field instead of <code>image_family</code>:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_name: ubuntu-2204-jammy-arm64-v20230630\n  architecture: arm64\n</code></pre>"},{"location":"guide/supported-computing-services/#custom-vm-images","title":"Custom VM images","text":"<p>Building an immutable VM image with all necessary software pre-configured is a known best practice with many benefits. It makes sure environment where a task is executed is always the same and that no time is spent on useless work like installing a package over and over again for every single task.</p> <p>There are many ways how one can create a custom image for Google Compute Engine. Please refer to the official documentation. At Cirrus Labs we are using Packer to automate building such images. An example of how we use it can be found in our public GitHub repository.</p>"},{"location":"guide/supported-computing-services/#windows-support","title":"Windows Support","text":"<p>Google Compute Engine support Windows images and Cirrus CI can take full advantages of it by just explicitly specifying platform of an image like this:</p> <pre><code>gce_instance:\n  image_project: windows-cloud\n  image_family: windows-2016-core\n  platform: windows\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 20\n\ntask:\n  script: run-ci.bat\n</code></pre>"},{"location":"guide/supported-computing-services/#freebsd-support","title":"FreeBSD Support","text":"<p>Google Compute Engine support FreeBSD images and Cirrus CI can take full advantages of it by just explicitly specifying platform of an image like this:</p> <pre><code>gce_instance:\n  image_project: freebsd-org-cloud-dev\n  image_family: freebsd-14-0\n  platform: FreeBSD\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 50\n\ntask:\n  script: printenv\n</code></pre>"},{"location":"guide/supported-computing-services/#docker-containers-on-dedicated-vms","title":"Docker Containers on Dedicated VMs","text":"<p>It is possible to run a container directly on a Compute Engine VM with pre-installed Docker. Use the <code>gce_container</code> field to specify a VM image and a Docker container to execute on the VM (<code>gce_container</code> extends <code>gce_instance</code> definition with a few additional fields):</p> <pre><code>gce_container:\n  image_project: my-project\n  image_name: my-custom-ubuntu-with-docker\n  container: golang:latest\n  additional_containers:\n    - name: redis\n      image: redis:3.2-alpine\n      port: 6379\n</code></pre> <p>Note that <code>gce_container</code> always runs containers in privileged mode.</p> <p>If your VM image has Nested Virtualization Enabled it's possible to use KVM from the container by specifying <code>enable_nested_virtualization</code> flag. Here is an example of using KVM-enabled container to run a hardware accelerated Android emulator:</p> <pre><code>gce_container:\n  image_project: my-project\n  image_name: my-custom-ubuntu-with-docker-and-KVM\n  container: ghcr.io/cirruslabs/android-sdk:latest\n  enable_nested_virtualization: true\n  accel_check_script:\n    - sudo chown cirrus:cirrus /dev/kvm\n    - emulator -accel-check\n</code></pre>"},{"location":"guide/supported-computing-services/#instance-scopes","title":"Instance Scopes","text":"<p>By default Cirrus CI will create Google Compute instances without any scopes so an instance can't access Google Cloud Storage for example. But sometimes it can be useful to give some permissions to an instance by using <code>scopes</code> key of <code>gce_instance</code>.  For example, if a particular task builds Docker images and then pushes them to Container Registry, its configuration file can look something like:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n\ngce_instance:\n  image_project: my-project\n  image_name: my-custom-image-with-docker\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 20\n\ntest_task:\n  test_script: ./scripts/test.sh\n\npush_docker_task:\n  depends_on: test\n  only_if: $CIRRUS_BRANCH == \"master\"\n  gce_instance:\n    scopes: cloud-platform\n  push_script: ./scripts/push_docker.sh\n</code></pre>"},{"location":"guide/supported-computing-services/#spot-instances","title":"Spot Instances","text":"<p>Cirrus CI can schedule spot instances with all price benefits and stability risks. But sometimes risks of an instance being preempted at any time can be tolerated. For example <code>gce_instance</code> can be configured to schedule spot instance for non master branches like this:</p> <pre><code>gce_instance:\n  image_project: my-project\n  image_name: my-custom-image-with-docker\n  zone: us-central1-a\n  spot: $CIRRUS_BRANCH != \"master\"\n</code></pre>"},{"location":"guide/supported-computing-services/#kubernetes-engine","title":"Kubernetes Engine","text":"<p>Scheduling tasks on Compute Engine has one big disadvantage of waiting for an instance to start which usually takes around a minute. One minute is not that long but can't compete with hundreds of milliseconds that takes a container cluster on GKE to start a container.</p> <p>To start scheduling tasks on a container cluster we first need to create one using <code>gcloud</code>. Here is a recommended configuration of a cluster that is very similar to what is used for the managed <code>container</code> instances. We recommend creating a cluster with two node pools:</p> <ul> <li><code>default-pool</code> with a single node and no autoscaling for system pods required by Kubernetes.</li> <li><code>workers-pool</code> that will use Compute-Optimized instances   and SSD storage for better performance. This pool also will be able to scale to 0 when there are no tasks to run.</li> </ul> <pre><code>gcloud container clusters create cirrus-ci-cluster \\\n  --autoscaling-profile optimize-utilization \\\n  --zone us-central1-a \\\n  --num-nodes \"1\" \\\n  --machine-type \"e2-standard-2\" \\\n  --disk-type \"pd-standard\" --disk-size \"100\"\n\ngcloud container node-pools create \"workers-pool\" \\\n  --cluster cirrus-ci-cluster \\\n  --zone \"us-central1-a\" \\\n  --num-nodes \"0\" \\\n  --enable-autoscaling --min-nodes \"0\" --max-nodes \"8\" \\\n  --node-taints dedicated=system:PreferNoSchedule \\\n  --machine-type \"c2-standard-30\" \\\n  --disk-type \"pd-ssd\" --disk-size \"500\"\n</code></pre> <p>A service account that Cirrus CI operates via should be assigned with <code>container.admin</code> role that allows to administrate GKE clusters:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/container.admin\n</code></pre> <p>Done! Now after creating <code>cirrus-ci-cluster</code> cluster and having <code>gcp_credentials</code> configured tasks can be scheduled on the newly created cluster like this:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n\ngke_container:\n  image: gradle:jdk8\n  cluster_name: cirrus-ci-cluster\n  location: us-central1-a # cluster zone or region for multi-zone clusters\n  namespace: default # Kubernetes namespace to create pods in\n  cpu: 6\n  memory: 24GB\n  nodeSelectorTerms: # optional\n    - matchExpressions:\n        - key: cloud.google.com/gke-spot\n          operator: In\n          values:\n            - \"true\"\n</code></pre> <p>Using in-memory disk</p> <p>By default Cirrus CI mounts an emptyDir into <code>/tmp</code> path to protect the pod from unnecessary eviction by autoscaler. It is possible to switch emptyDir's medium to use in-memory <code>tmpfs</code> storage instead of a default one by setting <code>use_in_memory_disk</code> field of <code>gke_container</code> to <code>true</code> or any other expression that uses environment variables.</p> <p>Running privileged containers</p> <p>You can run privileged containers on your private GKE cluster by setting <code>privileged</code> field of <code>gke_container</code> to <code>true</code> or any other expression that uses environment variables. <code>privileged</code> field is also available for any additional container.</p> <p>Here is an example of how to run docker-in-docker</p> <pre><code>gke_container:\n  image: my-docker-client:latest\n  cluster_name: my-gke-cluster\n  location: us-west1-c\n  namespace: cirrus-ci\n  additional_containers:\n    - name: docker\n      image: docker:dind\n      privileged: true\n      cpu: 2\n      memory: 6G\n      port: 2375\n</code></pre> <p>For a full example on leveraging this to do docker-in-docker builds on Kubernetes checkout Docker Builds on Kubernetes</p> Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>"},{"location":"guide/supported-computing-services/#aws","title":"AWS","text":"<p>Cirrus CI can schedule tasks on several AWS services. In order to interact with AWS APIs Cirrus CI needs permissions.</p>"},{"location":"guide/supported-computing-services/#configuring-aws-credentials","title":"Configuring AWS Credentials","text":"<p>There are two options to provide access to your infrastructure: via a traditional IAM user or via a more flexible and secure Identity Provider.</p> Permissions <p>A user or a role that Cirrus CI will be using for orchestrating tasks on AWS should at least have access to S3 in order to store logs and cache artifacts. Here is a list of actions that Cirrus CI requires to store logs and artifacts:</p> <pre><code>\"Action\": [\n  \"s3:CreateBucket\",\n  \"s3:GetObject\",\n  \"s3:PutObject\",\n  \"s3:DeleteObject\",\n  \"s3:PutLifecycleConfiguration\",\n  \"s3:PutBucketCORS\",\n  \"s3:PutBucketPolicy\"\n]\n</code></pre>"},{"location":"guide/supported-computing-services/#iam-user-credentials","title":"IAM user credentials","text":"<p>Creating an IAM user for programmatic access is a common way to safely give granular access to parts of your AWS.</p> <p>Once you created a user for Cirrus CI you'll need to provide key id and access key itself. In order to do so please create an encrypted variable with the following content:</p> <pre><code>[default]\naws_access_key_id=...\naws_secret_access_key=...\n</code></pre> <p>Then you'll be able to use the encrypted variable in your <code>.cirrus.yml</code> file like this:</p> <pre><code>aws_credentials: ENCRYPTED[...]\n\ntask:\n  ec2_instance:\n    ...\n\ntask:\n  eks_container:\n    ...\n</code></pre>"},{"location":"guide/supported-computing-services/#cirrus-as-an-openid-connect-identity-provider","title":"Cirrus as an OpenID Connect Identity Provider","text":"<p>By configuring Cirrus CI as an identity provider, Cirrus CI will be able to acquire temporary access tokens on-demand for each task. Please read AWS documentation to learn more about security and other benefits of using a workload identity provider.</p> <p>Now lets setup Cirrus CI as a workload identity provider. Here is a Cloud Formation Template that can configure Cirrus CI as an OpenID Connect Identity Provider. Please be extra careful and review this template, specifically pay attention to the condition that asserts claims <code>CIRRUS_OIDC_TOKEN</code> has.</p> <pre><code>Parameters:\n  GitHubOrg:\n    Type: String\n  OIDCProviderArn:\n    Description: Arn for the Cirrus CI OIDC Provider.\n    Default: \"\"\n    Type: String\n\nConditions:\n  CreateOIDCProvider: !Equals\n    - !Ref OIDCProviderArn\n    - \"\"\n\nResources:\n  Role:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRoleWithWebIdentity\n            Principal:\n              Federated: !If\n                - CreateOIDCProvider\n                - !Ref CirrusOidc\n                - !Ref OIDCProviderArn\n            Condition:\n              StringLike:\n                oidc.cirrus-ci.com:sub: !Sub repo:github:${GitHubOrg}/* # (1)\n\n  CirrusOidc:\n    Type: AWS::IAM::OIDCProvider\n    Condition: CreateOIDCProvider\n    Properties:\n      Url: https://oidc.cirrus-ci.com\n      ClientIdList:\n        - sts.amazonaws.com\n      ThumbprintList: # https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc_verify-thumbprint.html\n        - 46b4c330c67f24d6f11b5753394ecbe1e479cf29\n\nOutputs:\n  Role:\n    Value: !GetAtt Role.Arn\n</code></pre> <ol> <li> <p> this example template only checks that <code>CIRRUS_OIDC_TOKEN</code> comes from any repository under your organization.     If you are planning to use AWS compute services only for private repositories you should change this condition to:</p> <pre><code>Condition:\n  StringLike:\n    oidc.cirrus-ci.com:sub: !Sub repo:github:${GitHubOrg}/*\n    oidc.cirrus-ci.com:repository_visibility: private\n</code></pre> <p>Additionally, if you are planning to access production services from within your CI tasks, please create a separate role with even stricter asserts for additional security. The same <code>CIRRUS_OIDC_TOKEN</code> can be used to acquire tokens for multiple roles.</p> </li> </ol> <p>The output of running the template will a role that can be used in <code>aws_credentials</code> in your <code>.cirrus.yml</code> configuration:</p> <pre><code>aws_credentials:\n  role_arn: arn:aws:iam::123456789:role/CirrusCI-Role-Something-Something\n  role_session_name: cirrus # an identifier for the assumed role session\n  region: us-east-2 # region to use for calling the STS\n</code></pre> <p>Note that you'll need to add permissions required for Cirrus to that role.</p>"},{"location":"guide/supported-computing-services/#ec2","title":"EC2","text":"<p>In order to schedule tasks on EC2 please make sure that IAM user or OIDC role that Cirrus CI is using has following permissions:</p> <pre><code>\"Action\": [\n  \"ec2:CreateTags\",\n  \"ec2:DescribeInstances\",\n  \"ec2:DescribeImages\",\n  \"ec2:RunInstances\",\n  \"ec2:TerminateInstances\",\n  \"ssm:GetParameters\"\n]\n</code></pre> <p>Now tasks can be scheduled on EC2 by configuring <code>ec2_instance</code> something like this:</p> <pre><code>task:\n  ec2_instance:\n    image: ami-0a047931e1d42fdb3\n    type: t2.micro\n    region: us-east-1\n    subnet_ids: # optional, list of subnets from your default VPC to randomly choose from for scheduling the instance\n      - ...\n    subnet_filters: # optional, map of filters to use for DescribeSubnets API call. Note to make sure Cirrus is given `ec2:DescribeSubnets`\n      - name: tag:Name\n        values:\n          - subnet1\n          - subnet2\n    architecture: arm64 # defaults to amd64\n    spot: true # defaults to false\n    block_device_mappings: # empty by default\n      - device_name: /dev/sdg\n        ebs:\n          volume_size: 100 # to increase the size of the root volume\n      - device_name: /dev/sda1\n        virtual_name: ephemeral0 # to add an ephemeral disk for supported instances\n      - device_name: /dev/sdj\n        ebs:\n          snapshot_id: snap-xxxxxxxx\n  script: ./run-ci.sh\n</code></pre>"},{"location":"guide/supported-computing-services/#ami-resolution-options","title":"AMI resolution options","text":"<p>Value for the <code>image</code> field of <code>ec2_instance</code> can be just the image id in a format of <code>ami-*</code> but there two more convenient options when Cirrus will do image id resolutions for you:</p>"},{"location":"guide/supported-computing-services/#aws-system-manager","title":"AWS System Manager","text":"<pre><code>ec2_task:\n  ec2_instance:\n    image: /aws/service/ecs/optimized-ami/amazon-linux-2/arm64/recommended\n    architecture: arm64\n    region: us-east-2\n    type: a1.metal\n</code></pre> <p>In that case Cirrus will run analogue of:</p> <pre><code>aws ssm get-parameters --names /aws/service/ecs/optimized-ami/amazon-linux-2/arm64/recommended\n</code></pre> <p>to figure out the ami right before scheduling the instance. Please make use AMI user or role has <code>ssm:GetParameters</code> permissions.</p>"},{"location":"guide/supported-computing-services/#image-filtering","title":"Image filtering","text":"<pre><code>ec2_task:\n  ec2_instance:\n    image: ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\n    image_owners: # optional, defaults to self, amazon and aws-marketplace\n      - aws-marketplace\n    architecture: arm64\n    region: us-east-2\n    type: a1.metal\n</code></pre> <p>In that case Cirrus will run analogue of:</p> <pre><code>aws ec2 describe-images --filters \"Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"\n</code></pre> <p>to figure out the ami right before scheduling the instance (Cirrus will pick the freshest AMI from the list based on creation date). Please make use AMI user or role has <code>ec2:DescribeImages</code> permissions.</p>"},{"location":"guide/supported-computing-services/#eks","title":"EKS","text":"<p>Please follow instructions on how to create a EKS cluster and add workers nodes to it. And don't forget to add necessary permissions for the IAM user or OIDC role that Cirrus CI is using:</p> <pre><code>\"Action\": [\n  \"iam:PassRole\",\n  \"eks:DescribeCluster\",\n  \"ecr:DescribeImages\",\n]\n</code></pre> <p>To verify that Cirrus CI will be able to communicate with your cluster please make sure that if you are locally logged in as the user that Cirrus CI acts as you can successfully run the following commands and see your worker nodes up and running:</p> <pre><code>$: aws sts get-caller-identity\n{\n    \"UserId\": \"...\",\n    \"Account\": \"...\",\n    \"Arn\": \"USER_USED_BY_CIRRUS_CI\"\n}\n$: aws eks --region $REGION update-kubeconfig --name $CLUSTER_NAME\n$: kubectl get nodes\n</code></pre> <p>EKS Access Denied</p> <p>If you have an issue with accessing your EKS cluster via <code>kubectl</code>, most likely you did not create the cluster with the user that Cirrus CI is using. The easiest way to do so is to create the cluster through AWS CLI with the following command:</p> <pre><code>$: aws sts get-caller-identity\n{\n    \"UserId\": \"...\",\n    \"Account\": \"...\",\n    \"Arn\": \"USER_USED_BY_CIRRUS_CI\"\n}\n$: aws eks --region $REGION \\\n    create-cluster --name cirrus-ci \\\n    --role-arn ... \\\n    --resources-vpc-config subnetIds=...,securityGroupIds=...\n</code></pre> <p>Now tasks can be scheduled on EKS by configuring <code>eks_container</code> something like this:</p> <pre><code>task:\n  eks_container:\n    image: node:latest\n    region: us-east-1\n    cluster_name: cirrus-ci\n    nodeSelectorTerms: # optional\n      - matchExpressions:\n        - key: eks.amazonaws.com/capacityType\n          operator: In\n          values:\n            - SPOT\n  script: ./run-ci.sh\n</code></pre> <p>S3 Access for Caching</p> <p>Please add <code>AmazonS3FullAccess</code> policy to the role used for creation of EKS workers (same role you put in <code>aws-auth-cm.yaml</code> when enabled worker nodes to join the cluster).</p> Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>"},{"location":"guide/supported-computing-services/#azure","title":"Azure","text":"<p>Cirrus CI can schedule tasks on several Azure services. In order to interact with Azure APIs Cirrus CI needs permissions. First, please choose a subscription you want to use for scheduling CI tasks. Navigate to the Subscriptions blade within the Azure Portal and save <code>$SUBSCRIPTION_ID</code> that we'll use below for setting up a service principle.</p> <p>Creating a service principal is a common way to safely give granular access to parts of Azure:</p> <pre><code>az ad sp create-for-rbac --name CirrusCI --sdk-auth \\\n  --scopes \"/subscriptions/$SUBSCRIPTION_ID\"\n</code></pre> <p>Command above will create a new service principal and will print something like:</p> <pre><code>{\n  \"clientId\": \"...\",\n  \"clientSecret\": \"...\",\n  \"subscriptionId\": \"...\",\n  \"tenantId\": \"...\",\n  ...\n}\n</code></pre> <p>Please also remember <code>clientId</code> from the JSON as <code>$CIRRUS_CLIENT_ID</code>. It will be used later for configuring blob storage access.</p> <p>Please create an encrypted variable from this output and add it to the top of <code>.cirrus.yml</code> file:</p> <pre><code>azure_credentials: ENCRYPTED[qwerty239abc]\n</code></pre> <p>You also need to create a resource group that Cirrus CI will use for scheduling tasks:</p> <pre><code>az group create --location eastus --name CirrusCI\n</code></pre> <p>Please also allow the newly created CirrusCI principle to access blob storage in order to manage logs and caches.</p> <pre><code>az role assignment create \\\n    --role \"Storage Blob Data Contributor\" \\\n    --assignee $CIRRUS_CLIENT_ID \\\n    --scope \"/subscriptions/$SUBSCRIPTION_ID/resourceGroups/CirrusCI\"\n</code></pre> <p>Now Cirrus CI can interact with Azure APIs.</p>"},{"location":"guide/supported-computing-services/#azure-container-instances","title":"Azure Container Instances","text":"<p>Azure Container Instances (ACI) is an ideal candidate for running modern CI workloads. ACI allows just to run Linux and Windows containers without thinking about underlying infrastructure.</p> <p>Once <code>azure_credentials</code> is configured as described above, tasks can be scheduled on ACI by configuring <code>aci_instance</code> like this:</p> <pre><code>azure_container_instance:\n  image: cirrusci/windowsservercore:2016\n  resource_group: CirrusCI\n  region: westus\n  platform: windows\n  cpu: 4\n  memory: 12G\n</code></pre> <p>About Docker Images to use with ACI</p> <p>Linux-based images are usually pretty small and doesn't require much tweaking. For Windows containers ACI recommends to follow a few basic tips in order to reduce startup time.</p>"},{"location":"guide/supported-computing-services/#oracle-cloud","title":"Oracle Cloud","text":"<p>Cirrus CI can schedule tasks on several Oracle Cloud services. In order to interact with OCI APIs Cirrus CI needs permissions. Please create a user that Cirrus CI will behalf on:</p> <pre><code>oci iam user create --name cirrus --description \"Cirrus CI Orchestrator\"\n</code></pre> <p>Please configure the <code>cirrus</code> user to be able to access storage, launch instances and have access to Kubernetes clusters. The easiest way is to add <code>cirrus</code> user to <code>Administrators</code> group, but it's not as secure as a granular access configuration.</p> <p>By default, for every repository you'll start using Cirrus CI with, Cirrus will create a bucket with 90 days lifetime policy. In order to allow Cirrus to configure lifecycle policies please add the following policy as described in the documentation. Here is an example of the policy for <code>us-ashburn-1</code> region:</p> <pre><code>Allow service objectstorage-us-ashburn-1 to manage object-family in tenancy\n</code></pre> <p>Once you created and configured <code>cirrus</code> user you'll need to provide its API key. Once you generate an API key you should get a <code>*.pem</code> file with the private key that will be used by Cirrus CI.</p> <p>Normally your config file for local use looks like this:</p> <pre><code>[DEFAULT]\nuser=ocid1.user.oc1..XXX\nfingerprint=11:22:...:99\ntenancy=ocid1.tenancy.oc1..YYY\nregion=us-ashburn-1\nkey_file=&lt;path to your *.pem private keyfile&gt;\n</code></pre> <p>For Cirrus to use, you'll need to use a different format:</p> <pre><code>&lt;user value&gt;\n&lt;fingerprint value&gt;\n&lt;tenancy value&gt;\n&lt;region value&gt;\n&lt;content of your *.pem private keyfile&gt;\n</code></pre> <p>This way you'll be able to create a single encrypted variable with the contents of the Cirrus specific credentials above.</p> <pre><code>oracle_credentials: ENCRYPTED[qwerty239abc]\n</code></pre>"},{"location":"guide/supported-computing-services/#kubernetes-cluster","title":"Kubernetes Cluster","text":"<p>Please create a Kubernetes cluster and make sure Kubernetes API Public Endpoint is enabled for the cluster so Cirrus can access it. Then copy cluster id which can be used in configuring <code>oke_container</code>:</p> <pre><code>task:\n  oke_container:\n    cluster_id: ocid1.cluster.oc1.iad.xxxxxx\n    image: golang:latest\n    nodeSelectorTerms: # optional\n      - matchExpressions:\n        - key: kubernetes.io/arch\n          operator: In\n          values:\n            - arm64\n  script: ./run-ci.sh\n</code></pre> <p>Ampere A1 Support</p> <p>The cluster can utilize Oracle's Ampere A1 Arm instances in order to run <code>arm64</code> CI workloads!</p> Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>"},{"location":"guide/tips-and-tricks/","title":"Configuration Tips and Tricks","text":""},{"location":"guide/tips-and-tricks/#custom-clone-command","title":"Custom Clone Command","text":"<p>By default, Cirrus CI uses a Git client implemented purely in Go to perform a clone of a single branch with full Git history and no tags. It is possible to control clone depth via <code>CIRRUS_CLONE_DEPTH</code> environment variable.</p> <p>Customizing clone behavior is a simple as overriding <code>clone_script</code>. For example, here an override to use a pre-installed Git client (if your build environment has it) to do a shallow clone of a single branch:</p> <pre><code>task:\n  clone_script: |\n    if [ -z \"$CIRRUS_PR\" ]; then\n      git clone --recursive --branch=$CIRRUS_BRANCH https://x-access-token:${CIRRUS_REPO_CLONE_TOKEN}@github.com/${CIRRUS_REPO_FULL_NAME}.git $CIRRUS_WORKING_DIR\n      git reset --hard $CIRRUS_CHANGE_IN_REPO\n    else\n      git clone --recursive https://x-access-token:${CIRRUS_REPO_CLONE_TOKEN}@github.com/${CIRRUS_REPO_FULL_NAME}.git $CIRRUS_WORKING_DIR\n      git fetch origin pull/$CIRRUS_PR/head:pull/$CIRRUS_PR\n      git reset --hard $CIRRUS_CHANGE_IN_REPO\n    fi\n  # ...\n</code></pre> <p>In addition if you need to use Git tags, set <code>CIRRUS_CLONE_TAGS</code> environment variable to <code>true</code>:</p> <pre><code>build_task:\n  environment:\n    CIRRUS_CLONE_TAGS: true\n  fetch_deps_script: ...\n</code></pre> <p>You can also use the script below, but note that it requires <code>git</code> to be present in the container:</p> <pre><code>task:\n  git_tags_script: git fetch --tags\n</code></pre> <p><code>go-git</code> benefits</p> <p>Using <code>go-git</code> made it possible not to require a pre-installed Git from an execution environment. For example, most of <code>alpine</code>-based containers don't have Git pre-installed. Because of <code>go-git</code> you can even use distroless containers with Cirrus CI, which don't even have an Operating System.</p>"},{"location":"guide/tips-and-tricks/#sharing-configuration-between-tasks","title":"Sharing configuration between tasks","text":"<p>You can use YAML aliases to share configuration options between multiple tasks. For example, here is a 2-task build which only runs for \"master\", PRs and tags, and installs some framework:</p> <pre><code># Define a node anywhere in YAML file to create an alias. Make sure the name doesn't clash with an existing keyword.\nregular_task_template: &amp;REGULAR_TASK_TEMPLATE\n  only_if: $CIRRUS_BRANCH == 'master' || $CIRRUS_TAG != '' || $CIRRUS_PR != ''\n  env:\n    FRAMEWORK_PATH: \"${HOME}/framework\"\n  install_framework_script: curl https://example.com/framework.tar | tar -C \"${FRAMEWORK_PATH}\" -x\n\ntask:\n  # This operator will insert REGULAR_TASK_TEMPLATE at this point in the task node.\n  &lt;&lt; : *REGULAR_TASK_TEMPLATE\n  name: linux\n  container:\n    image: alpine:latest\n  test_script: ls \"${FRAMEWORK_PATH}\"\n\ntask:\n  &lt;&lt; : *REGULAR_TASK_TEMPLATE\n  name: osx\n  macos_instance:\n    image: catalina-xcode\n  test_script: ls -w \"${FRAMEWORK_PATH}\"\n</code></pre>"},{"location":"guide/tips-and-tricks/#long-lines-in-configuration-file","title":"Long lines in configuration file","text":"<p>If you like your YAML file to fit on your screen, and some commands are just too long, you can split them across multiple lines. YAML supports a variety of options to do that, for example here's how you can split ENCRYPTED values:</p> <pre><code>  env:\n    GOOGLE_APPLICATION_CREDENTIALS_DATA: \"ENCRYPTED\\\n      [3287dbace8346dfbe98347d1954eca923487fd8ea7251983\\\n      cb6d5edabdf6fe5abd711238764cbd6efbde6236abd6f274]\"\n</code></pre>"},{"location":"guide/tips-and-tricks/#setting-environment-variables-from-scripts","title":"Setting environment variables from scripts","text":"<p>Even through most of the time you can configure environment variables via <code>env</code>, there are cases when a variable value is obtained only when the task is already running.</p> <p>Normally you'd use <code>export</code> for that, but since each script instruction is executed in a separate shell, the exported variables won't propagate to the next instruction.</p> <p>However, there's a simple solution: just write your variables in a <code>KEY=VALUE</code> format to the file referenced by the <code>CIRRUS_ENV</code> environment variable.</p> <p>Here's a simple example:</p> <pre><code>task:\n  get_date_script: echo \"MEMOIZED_DATE=$(date)\" &gt;&gt; $CIRRUS_ENV\n  show_date_script: echo $MEMOIZED_DATE\n</code></pre>"},{"location":"guide/windows/","title":"Windows Containers","text":""},{"location":"guide/windows/#windows-containers","title":"Windows Containers","text":"<p>It is possible to run Windows Containers like how one can run Linux containers on Cirrus Cloud Windows Cluster.  To use Windows, add <code>windows_container</code> instead of <code>container</code> in <code>.cirrus.yml</code> files:</p> <pre><code>windows_container:\n  image: cirrusci/windowsservercore:2019\n\ntask:\n  script: ...\n</code></pre> <p>Cirrus CI will execute scripts instructions like Batch scripts.</p>"},{"location":"guide/windows/#os-versions","title":"OS Versions","text":"<p>Cirrus CI assumes that the container image's host OS is Windows Server 2019. Cirrus CI used to support <code>1709</code> and <code>1803</code> versions, but they are deprecated as of April 2021.</p> <pre><code>windows_container:\n  image: cirrusci/windowsservercore:2019\n\nwindows_task:\n  install_script: choco install -y ...\n  ...\n</code></pre>"},{"location":"guide/windows/#powershell-support","title":"PowerShell support","text":"<p>By default Cirrus CI agent executed scripts using <code>cmd.exe</code>. It is possible to override default shell executor by providing <code>CIRRUS_SHELL</code> environment variable:</p> <pre><code>env:\n  CIRRUS_SHELL: powershell\n</code></pre> <p>It is also possible to use PowerShell scripts inline inside of a script instruction by prefixing it with <code>ps</code>:</p> <pre><code>windows_task:\n  script:\n    - ps: Get-Location\n</code></pre> <p><code>ps: COMMAND</code> is just a syntactic sugar which transforms it to:</p> <pre><code>powershell.exe -NoLogo -EncodedCommand base64(COMMAND)\n</code></pre>"},{"location":"guide/windows/#environment-variables","title":"Environment Variables","text":"<p>Some software installed with Chocolatey would update <code>PATH</code> environment variable in system settings and suggest using <code>refreshenv</code> to pull those changes into the current environment. Unfortunately, using <code>refreshenv</code> will overwrite any environment variables set in Cirrus CI configuration with system-configured defaults. We advise to make necessary changes using <code>env</code> and <code>environment</code> instead of using <code>refreshenv</code> command in scripts.</p>"},{"location":"guide/windows/#chocolatey","title":"Chocolatey","text":"<p>All <code>cirrusci/*</code> Windows containers like <code>cirrusci/windowsservercore:2016</code> have Chocolatey pre-installed. Chocolatey is a package manager for Windows which supports unattended installs of software, useful on headless machines.</p>"},{"location":"guide/writing-tasks/","title":"Writing Tasks","text":"<p>A <code>task</code> defines a sequence of instructions to execute and an execution environment to execute these instructions in. Let's see a line-by-line example of a <code>.cirrus.yml</code> configuration file first:</p> amd64arm64 <pre><code>test_task:\n  container:\n    image: openjdk:latest\n  test_script: ./gradlew test\n</code></pre> <pre><code>test_task:\n  arm_container:\n    image: openjdk:latest\n  test_script: ./gradlew test\n</code></pre> <p>The example above defines a single task that will be scheduled and executed on the Linux Cluster using the <code>openjdk:latest</code> Docker image. Only one user-defined script instruction to run <code>./gradlew test</code> will be executed. Not that complex, right?</p> <p>Please read the topics below if you want better understand what's going on in a more complex <code>.cirrus.yml</code> configuration file, such as this:</p> amd64arm64 <pre><code>task:\n  container:\n    image: node:latest # (1)\n\n  node_modules_cache: # (2)\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n\n  matrix: # (3)\n    - name: Lint\n      skip: !changesInclude('.cirrus.yml', '**.{js,ts}') # (4)\n      lint_script: yarn run lint\n    - name: Test\n      container:\n        matrix: # (5)\n          - image: node:latest\n          - image: node:lts\n      test_script: yarn run test\n    - name: Publish\n      depends_on:\n        - Lint\n        - Test\n      only_if: $BRANCH == \"master\" # (6)\n      publish_script: yarn run publish\n</code></pre> <ol> <li>Use any Docker image from public or private registries</li> <li>Use cache instruction to persist folders based on an arbitrary <code>fingerprint_script</code>.</li> <li>Use <code>matrix</code> modification to produce many similar tasks.</li> <li>See what kind of files were changes and skip tasks that are not applicable.    See <code>changesInclude</code> and <code>changesIncludeOnly</code> documentation for details.</li> <li>Use nested <code>matrix</code> modification to produce even more tasks.</li> <li>Completely exclude tasks from execution graph by any custom condition.</li> </ol> <pre><code>task:\n  arm_container:\n    image: node:latest # (1)\n\n  node_modules_cache: # (2)\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n\n  matrix: # (3)\n    - name: Lint\n      skip: !changesInclude('.cirrus.yml', '**.{js,ts}') # (4)\n      lint_script: yarn run lint\n    - name: Test\n      arm_container:\n        matrix: # (5)\n          - image: node:latest\n          - image: node:lts\n      test_script: yarn run test\n    - name: Publish\n      depends_on:\n        - Lint\n        - Test\n      only_if: $BRANCH == \"master\" # (6)\n      publish_script: yarn run publish\n</code></pre> <ol> <li>Use any Docker image from public or private registries</li> <li>Use cache instruction to persist folders based on an arbitrary <code>fingerprint_script</code>.</li> <li>Use <code>matrix</code> modification to produce many similar tasks.</li> <li>See what kind of files were changes and skip tasks that are not applicable.    See <code>changesInclude</code> and <code>changesIncludeOnly</code> documentation for details.</li> <li>Use nested <code>matrix</code> modification to produce even more tasks.</li> <li>Completely exclude tasks from execution graph by any custom condition.</li> </ol> <p>Task Naming</p> <p>To name a task one can use the <code>name</code> field. <code>foo_task</code> syntax is a syntactic sugar. Separate name field is very useful when you want to have a rich task name:</p> <pre><code>task:\n  name: Tests (macOS)\n  ...\n</code></pre> <p>Note: instructions within a task can only be named via a prefix (e.g. <code>test_script</code>).</p> <p>Visual Task Creation for Beginners</p> <p>If you are just getting started and prefer a more visual way of creating tasks, there is a third-party Cirrus CI Configuration Builder for generating YAML config that might be helpful.</p>"},{"location":"guide/writing-tasks/#execution-environment","title":"Execution Environment","text":"<p>In order to specify where to execute a particular task you can choose from a variety of options by defining one of the following fields for a <code>task</code>:</p> Field Name Managed by Description <code>container</code> us Linux Docker Container <code>arm_container</code> us Linux Arm Docker Container <code>windows_container</code> us Windows Docker Container <code>macos_instance</code> us macOS Virtual Machines <code>freebsd_instance</code> us FreeBSD Virtual Machines <code>compute_engine_instance</code> us Full-fledged custom VM <code>persistent_worker</code> you Use any host on any platform and architecture <code>gce_instance</code> you Linux, Windows and FreeBSD Virtual Machines in your GCP project <code>gke_container</code> you Linux Docker Containers on private GKE cluster <code>ec2_instance</code> you Linux Virtual Machines in your AWS <code>eks_instance</code> you Linux Docker Containers on private EKS cluster <code>azure_container_instance</code> you Linux and Windows Docker Container on Azure <code>oke_instance</code> you Linux x86 and Arm Containers on Oracle Cloud"},{"location":"guide/writing-tasks/#supported-instructions","title":"Supported Instructions","text":"<p>Each task is essentially a collection of instructions that are executed sequentially. The following instructions are supported:</p> <ul> <li><code>script</code> instruction to execute a script.</li> <li><code>background_script</code> instruction to execute a script in a background.</li> <li><code>cache</code> instruction to persist files between task runs.</li> <li><code>artifacts</code> instruction to store and expose files created via a task.</li> <li><code>file</code> instruction to create a file from an environment variable.</li> </ul>"},{"location":"guide/writing-tasks/#script-instruction","title":"Script Instruction","text":"<p>A <code>script</code> instruction executes commands via <code>shell</code> on Unix or <code>batch</code> on Windows. A <code>script</code> instruction can be named by adding a name as a prefix. For example <code>test_script</code> or <code>my_very_specific_build_step_script</code>. Naming script instructions helps gather more granular information about task execution. Cirrus CI will use it in future to auto-detect performance regressions.</p> <p>Script commands can be specified as a single string value or a list of string values in a <code>.cirrus.yml</code> configuration file like in the example below:</p> <pre><code>check_task:\n  compile_script: gradle --parallel classes testClasses\n  check_script:\n    - echo \"Here comes more than one script!\"\n    - printenv\n    - gradle check\n</code></pre> <p>Note: Each script instruction is executed in a newly created process, therefore environment variables are not preserved between them.</p> Execution on Windows <p>When executed on Windows via <code>batch</code>, Cirrus Agent will wrap each line of the script in a <code>call</code> so it's possible to fail fast upon first line exiting with non-zero exit code.</p> <p>To avoid this \"syntactic sugar\" just create a script file and execute it.</p>"},{"location":"guide/writing-tasks/#background-script-instruction","title":"Background Script Instruction","text":"<p>A <code>background_script</code> instruction is absolutely the same as <code>script</code> instruction but Cirrus CI won't wait for the script to finish and will continue execution of further instructions.</p> <p>Background scripts can be useful when something needs to be executed in the background. For example, a database or some emulators. Traditionally the same effect is achieved by adding <code>&amp;</code> to a command like <code>$: command &amp;</code>. Problem here is that logs from <code>command</code> will be mixed into regular logs of the following commands. By using background scripts not only logs will be properly saved and displayed, but also <code>command</code> itself will be properly killed in the end of a task.</p> <p>Here is an example of how <code>background_script</code> instruction can be used to run an android emulator:</p> <pre><code>android_test_task:\n  start_emulator_background_script: emulator -avd test -no-audio -no-window\n  wait_for_emulator_to_boot_script: adb wait-for-device\n  test_script: gradle test\n</code></pre>"},{"location":"guide/writing-tasks/#cache-instruction","title":"Cache Instruction","text":"<p>A <code>cache</code> instruction allows to persist a folder and reuse it during the next execution of the task. A <code>cache</code> instruction can be named the same way as <code>script</code> instruction.</p> <p>Here is an example:</p> amd64arm64 <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    reupload_on_changes: false # since there is a fingerprint script\n    fingerprint_script:\n      - echo $CIRRUS_OS\n      - node --version\n      - cat package-lock.json\n    populate_script: \n      - npm install\n  test_script: npm run test\n</code></pre> <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    reupload_on_changes: false # since there is a fingerprint script\n    fingerprint_script:\n      - echo $CIRRUS_OS\n      - node --version\n      - cat package-lock.json\n    populate_script: \n      - npm install\n  test_script: npm run test\n</code></pre> <p>Either <code>folder</code> or a <code>folders</code> field (with a list of folder paths) is required and they tell the agent which folder paths to cache.</p> <p>Folder paths should be generally relative to the working directory (e.g. <code>node_modules</code>), with the exception of when only a single folder specified. In this case, it can be also an absolute path (<code>/usr/local/bundle</code>).</p> <p>Folder paths can contain a \"glob\" pattern to cache multiple files/folders within a working directory (e.g. <code>**/node_modules</code> will cache every <code>node_modules</code> folder within the working directory).</p> <p>A <code>fingerprint_script</code> and <code>fingerprint_key</code> are optional fields that can specify either:</p> <ul> <li>a script, the output of which will be hashed and used as a key for the given cache:</li> </ul> <pre><code>node_modules_cache:\n  folder: node_modules\n  fingerprint_script: cat yarn.lock\n</code></pre> <ul> <li>a final cache key:</li> </ul> <pre><code>node_modules_cache:\n  folder: node_modules\n  fingerprint_key: 2038-01-20\n</code></pre> <p>These two fields are mutually exclusive. By default, the task name is used as a fingerprint value.</p> <p>After the last <code>script</code> instruction for the task succeeds, Cirrus CI will calculate checksum of the cached folder (note that it's unrelated to <code>fingerprint_script</code> or <code>fingerprint_key</code> fields) and re-upload the cache if it finds any changes. To avoid a time-costly re-upload, remove volatile files from the cache (for example, in the last <code>script</code> instruction of a task).</p> <p><code>populate_script</code> is an optional field that can specify a script that will be executed to populate the cache. <code>populate_script</code> should create the <code>folder</code> if it doesn't exist before the <code>cache</code> instruction. If your dependencies are updated often, please pay attention to <code>fingerprint_script</code> and make sure it will produce different outputs for different versions of your dependency (ideally just print locked versions of dependencies).</p> <p><code>reupload_on_changes</code> is an optional field that can specify whether Cirrus Agent should check if  contents of cached <code>folder</code> have changed during task execution and re-upload a cache entry in case of any changes. If <code>reupload_on_changes</code> option is not set explicitly then it will be set to <code>false</code> if <code>fingerprint_script</code> or <code>fingerprint_key</code> is presented and <code>true</code> otherwise. Cirrus Agent will detect additions, deletions and modifications of any files under specified <code>folder</code>. All of the detected changes will be logged under <code>Upload '$CACHE_NAME' cache</code> instructions for easier debugging of cache invalidations.</p> <p><code>optimistically_restore_on_miss</code> is an optional field that can specify whether Cirrus Agent should restore a cache even if it doesn't exist for a given <code>fingerprint_script</code> or <code>fingerprint_key</code>. In this case, a most recent cache entry will be optimistically located and used instead. Note that the optimistic lookup is performed based on cache instruction name, so please make sure they are unique across different platforms and architectures if contents of the cache depend on platform or architecture.</p> <p>That means the only difference between the example above and below is that <code>yarn install</code> will always be executed in the example below where in the example above only when <code>yarn.lock</code> has changes.</p> amd64arm64 <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n  install_script: yarn install\n  test_script: yarn run test\n</code></pre> <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n  install_script: yarn install\n  test_script: yarn run test\n</code></pre> <p>Caching for Pull Requests</p> <p>Tasks for PRs upload caches to a separate caching namespace to not interfere with caches used by other tasks. But such PR tasks can read all caches even from the main caching namespace for a repository.</p> <p>Scope of cached artifacts</p> <p>Cache artifacts are shared between tasks, so two caches with the same name on e.g. Linux containers and macOS VMs will share the same set of files. This may introduce binary incompatibility between caches. To avoid that, add <code>echo $CIRRUS_OS</code> into <code>fingerprint_script</code> or use <code>$CIRRUS_OS</code> in <code>fingerprint_key</code>, which will distinguish caches based on OS.</p>"},{"location":"guide/writing-tasks/#manual-cache-upload","title":"Manual cache upload","text":"<p>Normally caches are uploaded at the end of the task execution. However, you can override the default behavior and upload them earlier.</p> <p>To do this, use the <code>upload_caches</code> instruction, which uploads a list of caches passed to it once executed:</p> amd64arm64 <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n  upload_caches:\n    - node_modules\n  install_script: yarn install\n  test_script: yarn run test\n  pip_cache:\n    folder: ~/.cache/pip\n</code></pre> <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n  upload_caches:\n    - node_modules\n  install_script: yarn install\n  test_script: yarn run test\n  pip_cache:\n    folder: ~/.cache/pip\n</code></pre> <p>Note that <code>pip</code> cache won't be uploaded in this example: using <code>upload_caches</code> disables the default behavior where all caches are automatically uploaded at the end of the task, so if you want to upload <code>pip</code> cache too, you'll have to either:</p> <ul> <li>extend the list of uploaded caches in the first <code>upload_caches</code> instruction</li> <li>insert a second <code>upload_caches</code> instruction that specifically targets <code>pip</code> cache</li> </ul>"},{"location":"guide/writing-tasks/#artifacts-instruction","title":"Artifacts Instruction","text":"<p>An <code>artifacts</code> instruction allows to store files and expose them in the UI for downloading later. An <code>artifacts</code> instruction can be named the same way as <code>script</code> instruction and has only one required <code>path</code> field which accepts a glob pattern of files relative to <code>$CIRRUS_WORKING_DIR</code> to store. Right now only storing files under <code>$CIRRUS_WORKING_DIR</code> folder as artifacts is supported with a total size limit of 1G for a free task and with no limit on your own infrastructure.</p> <p>In the example below, Build and Test task produces two artifacts: <code>binaries</code> artifacts with all executables built during a successful task completion and <code>junit</code> artifacts with all test reports regardless of the final task status (more about that you can learn in the next section describing execution behavior).</p> <pre><code>build_and_test_task:\n  # instructions to build and test\n  binaries_artifacts:\n    path: \"build/*\"\n  always:\n    junit_artifacts:\n      path: \"**/test-results/**.xml\"\n      format: junit\n</code></pre> <p>You can either use <code>path</code> or <code>paths</code> field, where the latter expects a list of paths instead of just a scalar.</p> URLs to the artifacts"},{"location":"guide/writing-tasks/#latest-build-artifacts","title":"Latest build artifacts","text":"<p>It is possible to refer to the latest artifacts directly (artifacts of the latest successful build). Use the following link format to download the latest artifact of a particular task:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;/&lt;PATH&gt;\n</code></pre> <p>It is possible to also download an archive of all files within an artifact with the following link:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>By default, Cirrus looks up the latest successful build of the default branch for the repository but the branch name can be customized via <code>?branch=&lt;BRANCH&gt;</code> query parameter.</p>"},{"location":"guide/writing-tasks/#current-build-artifacts","title":"Current build artifacts","text":"<p>It is possible to refer to the artifacts of the current build directly:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/build/&lt;CIRRUS_BUILD_ID&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>Note that if several tasks are uploading artifacts with the same name then the ZIP archive from the above link will contain merged content of all artifacts. It's also possible to refer to an artifact of a particular task within a build by name:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/build/&lt;CIRRUS_BUILD_ID&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>It is also possible to download artifacts given a task id directly:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/task/&lt;CIRRUS_TASK_ID&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>It's also possible to download a particular file of an artifact and not the whole archive by using <code>&lt;ARTIFACTS_NAME&gt;/&lt;PATH&gt;</code> instead of <code>&lt;ARTIFACTS_NAME&gt;.zip</code>.</p>"},{"location":"guide/writing-tasks/#artifact-type","title":"Artifact Type","text":"<p>By default, Cirrus CI will try to guess mimetype of files in artifacts by looking at their extensions. In case when artifacts don't have extensions, it's possible to explicitly set the <code>Content-Type</code> via <code>type</code> field:</p> <pre><code>  my_task:\n    my_dotjar_artifacts:\n      path: build/*.jar\n      type: application/java-archive\n</code></pre> <p>A list of some of the basic types supported can be found here.</p>"},{"location":"guide/writing-tasks/#artifact-parsing","title":"Artifact Parsing","text":"<p>Cirrus CI supports parsing artifacts in order to extract information that can be presented in the UI for a better user experience. Use the <code>format</code> field of an artifact instruction to specify artifact's format (mimetypes):</p> <pre><code>junit_artifacts:\n  path: \"**/test-results/**.xml\"\n  type: text/xml\n  format: junit\n</code></pre> <p>Currently, Cirrus CI supports:</p> <ul> <li>Android Lint Report format</li> <li>GolangCI Lint's JSON format</li> <li>JUnit's XML format<ul> <li>Python's Unittest format</li> </ul> </li> <li>XCLogParser</li> <li>JetBrains Qodana</li> <li>Buf CLI for Protocol Buffers</li> </ul> <p>Please let us know what kind of formats Cirrus CI should support next!</p>"},{"location":"guide/writing-tasks/#file-instruction","title":"File Instruction","text":"<p>A <code>file</code> instruction allows to create a file from either an environment variable or directly from the configuration file. It is especially useful for situations when execution environment doesn't have proper shell to use <code>echo ... &gt;&gt; ...</code> syntax, for example, within scratch Docker containers.</p> <p>Here is an example of how to populate Docker config from an encrypted environment variable:</p> <pre><code>task:\n  environment:\n    DOCKER_CONFIG_JSON: ENCRYPTED[qwerty]\n  docker_config_file:\n    path: /root/.docker/config.json\n    variable_name: DOCKER_CONFIG_JSON\n</code></pre> <p>You can also populate a file directly from the <code>.cirrus.yml</code> configuration file:</p> <pre><code>task:\n  git_config_file:\n    path: /root/.gitconfig\n    from_contents: |\n      [user]\n        name = John Doe\n        email = john@example.com\n</code></pre>"},{"location":"guide/writing-tasks/#execution-behavior-of-instructions","title":"Execution Behavior of Instructions","text":"<p>By default, Cirrus CI executes instructions one after another and stops the overall task execution on the first failure. Sometimes there might be situations when some scripts should always be executed or some debug information needs to be saved on a failure. For such situations the <code>always</code> and <code>on_failure</code> keywords can be used to group instructions.</p> <pre><code>task:\n  test_script: ./run_tests.sh\n  on_failure:\n    debug_script: ./print_additional_debug_info.sh\n  cleanup_script: ./cleanup.sh # failure here will not trigger `on_failure` instruction above\n  always:\n    test_reports_script: ./print_test_reports.sh\n</code></pre> <p>In the example above, <code>print_additional_debug_info.sh</code> script will be executed only on failures of <code>test_script</code> to output some additional debug information. <code>print_test_reports.sh</code> on the other hand will be executed both on successful and failed runs to print test reports (test reports are always useful! ).</p> <p>Sometimes, a complex task might exceed the pre-defined timeout, and it might not be clear why. In this case, the <code>on_timeout</code> execution behavior, which has an extra time budget of 5 minutes might be useful:</p> <pre><code>task:\n  integration_test_script: ./run_integration_tests.sh\n  on_timeout:\n    cleanup_script: ./cleanup.sh\n</code></pre>"},{"location":"guide/writing-tasks/#environment-variables","title":"Environment Variables","text":"<p>Environment variables can be configured under the <code>env</code> or <code>environment</code> keywords in <code>.cirrus.yml</code> files. Here is an example:</p> <pre><code>echo_task:\n  env:\n    FOO: Bar\n  echo_script: echo $FOO\n</code></pre> <p>You can reference other environment variables using <code>$VAR</code>, <code>${VAR}</code> or <code>%VAR%</code> syntax:</p> <pre><code>custom_path_task:\n  env:\n    SDK_ROOT: ${HOME}/sdk\n    PATH: ${SDK_ROOT}/bin:${PATH}\n  custom_script: sdktool install\n</code></pre> <p>Environment variables may also be set at the root level of <code>.cirrus.yml</code>. In that case, they will be merged with each task's individual environment variables, but the task level variables always take precedence. For example:</p> <pre><code>env:\n  PATH: /sdk/bin:${PATH}\n\necho_task:\n  env:\n    PATH: /opt/bin:${PATH}\n  echo_script: echo $PATH\n</code></pre> <p>Will output <code>/opt/bin:/usr/local/bin:/usr/bin</code> or similar, but will not include <code>/sdk/bin</code> because this root level setting is ignored.</p> <p>Also some default environment variables are pre-defined:</p> Name Value / Description CI true CIRRUS_CI true CI_NODE_INDEX Index of the current task within <code>CI_NODE_TOTAL</code> tasks CI_NODE_TOTAL Total amount of unique tasks for a given <code>CIRRUS_BUILD_ID</code> build CONTINUOUS_INTEGRATION <code>true</code> CIRRUS_API_CREATED <code>true</code> if the current build was created through the API. CIRRUS_BASE_BRANCH Base branch name if current build was triggered by a PR. For example <code>master</code> CIRRUS_BASE_SHA Base SHA if current build was triggered by a PR CIRRUS_BRANCH Branch name. For example <code>my-feature</code> CIRRUS_BUILD_ID Unique build ID CIRRUS_CHANGE_IN_REPO Git SHA CIRRUS_CHANGE_MESSAGE Commit message or PR title and description, depending on trigger event (Non-PRs or PRs respectively). CIRRUS_CHANGE_TITLE First line of <code>CIRRUS_CHANGE_MESSAGE</code> CIRRUS_CPU Amount of CPUs requested by the task. <code>CIRRUS_CPU</code> value is integer and rounded up for tasks that requested non-interger amount of CPUs. CIRRUS_CRON Cron Build name configured in the repository settings if this build was triggered by Cron. For example, <code>nightly</code>. CIRRUS_DEFAULT_BRANCH Default repository branch name. For example <code>master</code> CIRRUS_DOCKER_CONTEXT Docker build's context directory to use for Dockerfile as a CI environment. Defaults to project's root directory. CIRRUS_LAST_GREEN_BUILD_ID The build id of the last successful build on the same branch at the time of the current build creation. CIRRUS_LAST_GREEN_CHANGE Corresponding to <code>CIRRUS_LAST_GREEN_BUILD_ID</code> SHA (used in <code>changesInclude</code> and <code>changesIncludeOnly</code> functions). CIRRUS_PR PR number if current build was triggered by a PR. For example <code>239</code>. CIRRUS_PR_DRAFT <code>true</code> if current build was triggered by a Draft PR. CIRRUS_PR_TITLE Title of a corresponding PR if any. CIRRUS_PR_BODY Body of a corresponding PR if any. CIRRUS_PR_LABELS comma separated list of PR's labels if current build was triggered by a PR. CIRRUS_TAG Tag name if current build was triggered by a new tag. For example <code>v1.0</code> CIRRUS_OIDC_TOKEN OpenID Connect Token issued by <code>https://oidc.cirrus-ci.com</code> with audience set to <code>https://cirrus-ci.com/github/$CIRRUS_REPO_OWNER</code> (can be changed via <code>$CIRRUS_OIDC_TOKEN_AUDIENCE</code>). Please refer to a dedicated section below for in-depth details. CIRRUS_OS, OS Host OS. Either <code>linux</code>, <code>windows</code> or <code>darwin</code>. CIRRUS_TASK_NAME Task name CIRRUS_TASK_NAME_ALIAS Task name <code>alias</code> if any. CIRRUS_TASK_ID Unique task ID CIRRUS_RELEASE GitHub Release id if current tag was created for a release. Handy for uploading release assets. CIRRUS_REPO_CLONE_TOKEN Temporary GitHub access token to perform a clone. CIRRUS_REPO_NAME Repository name. For example <code>my-project</code> CIRRUS_REPO_OWNER Repository owner (an organization or a user). For example <code>my-organization</code> CIRRUS_REPO_FULL_NAME Repository full name/slug. For example <code>my-organization/my-project</code> CIRRUS_REPO_CLONE_URL URL used for cloning. For example <code>https://github.com/my-organization/my-project.git</code> CIRRUS_USER_COLLABORATOR <code>true</code> if a user initialized a build is already a contributor to the repository. <code>false</code> otherwise. CIRRUS_USER_PERMISSION <code>admin</code>, <code>write</code>, <code>read</code> or <code>none</code>. CIRRUS_HTTP_CACHE_HOST Host and port number on which local HTTP cache can be accessed on. GITHUB_CHECK_SUITE_ID Monotonically increasing id of a corresponding GitHub Check Suite which caused the Cirrus CI build. CIRRUS_ENV Path to a file, by writing to which you can set task-wide environment variables. CIRRUS_ENV_SENSITIVE Set to <code>true</code> to mask all variable values written to the <code>CIRRUS_ENV</code> file in the console output"},{"location":"guide/writing-tasks/#behavioral-environment-variables","title":"Behavioral Environment Variables","text":"<p>And some environment variables can be set to control behavior of the Cirrus CI Agent:</p> Name Default Value Description CIRRUS_AGENT_EXPOSE_SCRIPTS_OUTPUTS not set If set, instructs Cirrus Agent to stream scripts outputs to the console as well as Cirrus API. Useful in case your Kubernetes cluster has logging collection enabled. CIRRUS_CLI_VERSION not set Cirrus CLI version to use when running the agent. If not set, the latest release will be used. CIRRUS_CLONE_DEPTH <code>0</code> which will reflect in a full clone of a single branch Clone depth. CIRRUS_CLONE_TAGS <code>false</code> which will not fetch any tags Whether to fetch git tags. CIRRUS_CLONE_SUBMODULES <code>false</code> Set to <code>true</code> to clone submodules recursively. CIRRUS_LOG_TIMESTAMP <code>false</code> Indicate Cirrus Agent to prepend timestamp to each line of logs. CIRRUS_OIDC_TOKEN_AUDIENCE not set Allows to override <code>aud</code> claim for <code>CIRRUS_OIDC_TOKEN</code>. CIRRUS_SHELL <code>sh</code> on Linux/macOS/FreeBSD and <code>cmd.exe</code> on Windows. Set to <code>direct</code> to execute each script directly without wrapping the commands in a shell script. Shell that Cirrus CI uses to execute scripts. By default <code>sh</code> is used. CIRRUS_VOLUME <code>/tmp</code> Defines a path for a temporary volume to be mounted into instances running in a Kubernetes cluster. This volume is mounted into all additional containers and is persisted between steps of a <code>pipe</code>. CIRRUS_WORKING_DIR <code>cirrus-ci-build</code> folder inside of a system's temporary folder Working directory where Cirrus CI executes builds. Default to <code>cirrus-ci-build</code> folder inside of a system's temporary folder. CIRRUS_ESCAPING_PROCESSES not set Set this variable to prevent the agent from terminating the processes spawned in each non-background instruction after that instruction ends. By default, the agent tries it's best to garbage collect these processes and their standard input/output streams. It's generally better to use a Background Script Instruction instead of this variable to achieve the same effect. CIRRUS_WINDOWS_ERROR_MODE not set Set this value to force all processes spawned by the agent to call the equivalent of <code>SetErrorMode()</code> with the provided value (for example, <code>0x8001</code>) before beginning their execution. CIRRUS_VAULT_URL not set Address of the Vault server expressed as a URL and port (for example, <code>https://vault.example.com:8200/</code>), see HashiCorp Vault Support. CIRRUS_VAULT_NAMESPACE not set A Vault Enterprise Namespace to use when authenticating and reading secrets from Vault. CIRRUS_VAULT_AUTH_PATH <code>jwt</code> Alternative auth method mount point, in case it was mounted to a non-default path. CIRRUS_VAULT_ROLE not set"},{"location":"guide/writing-tasks/#internals-of-openid-connect-tokens","title":"Internals of OpenID Connect tokens","text":"<p>OpenID Connect is a very powerful mechanism that allows two independent systems establish trust without sharing any secrets. In the core of OpenID Connect is a simple JWT token that is signed by a trusted party (in our case it's Cirrus CI). Then the second system can be configured to trust such <code>CIRRUS_OIDC_TOKEN</code>s signed by Cirrus CI. For examples please check Vault Integration, Google Cloud Integration and AWS Integration.</p> <p>Once such external system receives a request authenticated with <code>CIRRUS_OIDC_TOKEN</code> it can verify the signature of the token via publicly available keys. Then it can extract claims from the token to make necessary assertions. Properly configuring assertions of such claims is crucial for secure integration with OIDC. Let's take a closer look at claims that are available through a payload of a <code>CIRRUS_OIDC_TOKEN</code>:</p> <pre><code>task:\n  container:\n    image: alpine:latest\n  install_script: apk add jq\n  payload_script: echo $CIRRUS_OIDC_TOKEN | jq -R 'split(\".\") | .[1] | @base64d | fromjson'\n</code></pre> <p>The above task will print out payload of a <code>CIRRUS_OIDC_TOKEN</code> that contains claims from the configuration that can be used for assertions.</p> <pre><code>{\n  // Reserved Claims https://openid.net/specs/draft-jones-json-web-token-07.html#rfc.section.4.1 \n  \"iss\": \"https://oidc.cirrus-ci.com\",\n  \"aud\": \"https://cirrus-ci.com/github/cirruslabs\", // can be changed via $CIRRUS_OIDC_TOKEN_AUDIENCE\n  \"sub\": \"repo:github:cirruslabs/cirrus-ci-docs\",\n  \"nbf\": ...,\n  \"exp\": ...,\n  \"iat\": ...,\n  \"jti\": \"...\",\n  // Cirrus Added Claims\n  \"platform\": \"github\", // Currently only GitHub is supported but more platforms will be added in the future\n  \"owner\": \"cirruslabs\", // Unique organization or username on the platform\n  \"owner_id\": \"29414678\", // Internal ID of the organization or user on the platform\n  \"repository\": \"cirrus-ci-docs\", // Repository name\n  \"repository_visibility\": \"public\", // either public or private\n  \"repository_id\": \"5730634941071360\", // Internal Cirrus CI ID of the repository\n  \"build_id\": \"1234567890\", // Internal Cirrus CI ID of the build. Same as $CIRRUS_BUILD_ID\n  \"branch\": \"fkorotkov-patch-2\", // Git branch name. Same as $CIRRUS_BRANCH\n  \"change_in_repo\": \"e6e989d4792a678b697a9f17a787761bfefb52d0\", // Git commit SHA. Same as $CIRRUS_CHANGE_IN_REPO\n  \"pr\": \"123\", // Pull request number if a build was triggered by a PR. Same as $CIRRUS_PR\n  \"pr_draft\": \"false\", // Whether the pull request is a draft. Same as $CIRRUS_PR_DRAFT\n  \"pr_labels\": \"\", // Comma-separated list of labels of the pull request. Same as $CIRRUS_PR_LABELS\n  \"tag\": \"1.0.0\", // Git tag name if a build was triggered by a tag creation. Same as $CIRRUS_TAG\n  \"task_id\": \"987654321\", // Internal Cirrus CI ID of the task. Same as $CIRRUS_TASK_ID\n  \"task_name\": \"main\", // Name of the task. Same as $CIRRUS_TASK_NAME\n  \"task_name_alias\": \"main\", // Optional name alias of the task. Same as $CIRRUS_TASK_NAME_ALIAS\n  \"user_collaborator\": \"true\", // Whether the user is a collaborator of the repository. Same as $CIRRUS_USER_COLLABORATOR\n  \"user_permission\": \"admin\", // Permission level of the user in the repository. Same as $CIRRUS_USER_PERMISSION\n}\n</code></pre> <p>Please use the above claims to configure assertions in your external system. For example, you can assert that only tokens for specific branches can retrieve secrets for deploying to production.</p>"},{"location":"guide/writing-tasks/#encrypted-variables","title":"Encrypted Variables","text":"<p>It is possible to add encrypted variables to a <code>.cirrus.yml</code> file. These variables are decrypted only in builds for commits and pull requests that are made by users with <code>write</code> permission or approved by them.</p> <p>In order to encrypt a variable go to repository's settings page via clicking settings icon  on a repository's main page (for example <code>https://cirrus-ci.com/github/my-organization/my-repository</code>) and follow instructions.</p> <p>Warning</p> <p>Only users with <code>WRITE</code> permissions can add encrypted variables to a repository.</p> <p>Multi-Line Encrypted Variables</p> <p>Please avoid using encrypted variables that consist of multiple lines as they won't be masked (replaced by <code>HIDDEN-BY-CIRRUS-CI</code>) correctly due to the line-buffered nature of the task's instructions output.</p> <p>An encrypted variable will be presented in a form like <code>ENCRYPTED[qwerty239abc]</code> which can be safely committed to <code>.cirrus.yml</code> file:</p> <pre><code>publish_task:\n  environment:\n    AUTH_TOKEN: ENCRYPTED[qwerty239abc]\n  script: ./publish.sh\n</code></pre> <p>Cirrus CI encrypts variables with a unique per-repository 256-bit encryption key, so that forks and even repositories within the same organization cannot re-use them. <code>qwerty239abc</code> from the example above is NOT the content of your encrypted variable, it's just an internal ID. No one can brute force your secrets from such ID. In addition, Cirrus CI doesn't know a relation between an encrypted variable and a repository for which the encrypted variable was created.</p> Organization Level Encrypted Variables <p>Sometimes there might be secrets that are used in almost all repositories of an organization. For example, credentials to a compute service where tasks will be executed. In order to create such sharable encrypted variable go to organization's settings page via clicking settings icon  on an organization's main page (for example <code>https://cirrus-ci.com/github/my-organization</code>) and follow instructions in Organization Level Encrypted Variables section.</p> Encrypted Variable for Cloud Credentials <p>In case you use integration with one of supported computing services, an encrypted variable used to store credentials that Cirrus is using to communicate with the computing service won't be decrypted if used in environment variables. These credentials have too many permissions for most of the cases, please create separate credentials with the minimum needed permissions for your specific case.</p> <pre><code>gcp_credentials: SECURED[!qwerty]\n\nenv:\n  CREDENTIALS: SECURED[!qwerty] # won't be decrypted in any case\n</code></pre> Skipping Task in Forked Repository <p>In forked repository the decryption of variable fails, which causes failure of task depending on it. To avoid this by default, make the sensitive task conditional:</p> <pre><code>task:\n  name: Task requiring decrypted variables\n  only_if: $CIRRUS_REPO_OWNER == 'my-organization'\n  ...\n</code></pre> <p>Owner of forked repository can re-enable the task, if they have the required sensitive data, by encrypting the variable by themselves and editing both the encrypted variable and repo-owner condition in the <code>.cirrus.yml</code> file.</p>"},{"location":"guide/writing-tasks/#hashicorp-vault-support","title":"HashiCorp Vault support","text":"<p>In addition to using Cirrus CI for managing secrets, it is possible to retrieve secrets from HashiCorp Vault.</p> <p>You will need to configure a JWT authentication method and point it to the Cirrus CI's OIDC discovery URL: <code>https://oidc.cirrus-ci.com</code>.</p> <p>This ensures that a cryptographic JWT token (<code>CIRRUS_OIDC_TOKEN</code>) that each Cirrus CI's task get assigned will be verified by your Vault installation.</p> <p>From the Cirrus CI's side, use the <code>CIRRUS_VAULT_URL</code> environment variable to point Cirrus Agent at your vault and configure other Vault-specific variables, if needed. Note that it's not required for <code>CIRRUS_VAULT_URL</code> to be publicly available since Cirrus CI can orchestrate tasks on your infrastructure. Only Cirrus Agent executing a task from within an execution environment needs access to your Vault.</p> <p>Once done, you will be able to use the <code>VAULT[path/to/secret selector]</code> syntax to retrieve a version 2 secret, for example:</p> <pre><code>publish_task:\n  environment:\n    AUTH_TOKEN: VAULT[secret/data/github data.token]\n  script: ./publish.sh\n</code></pre> <p>The path is exactly the one you are familiar from invoking Vault CLI like <code>vault read ...</code>, and the selector is a simply dot-delimited list of fields to query in the output.</p> <p>Caching of Vault secrets</p> <p>Note that all <code>VAULT[...]</code> invocations cache the retrieved secrets on a per-path basis by default. Caching happens within a single task execution and is not shared between several tasks using the same secret.</p> <p>To disable caching, use <code>VAULT_NOCACHE[...]</code> instead of <code>VAULT[...]</code>.</p> <p>Multi-Line Vault Secrets</p> <p>Please avoid referencing Vault secrets that consist of multiple lines as they won't be masked (replaced by <code>HIDDEN-BY-CIRRUS-CI</code>) correctly due to the line-buffered nature of the task's instructions output.</p> <p>Mixing of <code>VAULT[...]</code> and <code>VAULT_NOCACHE[...]</code> on the same path</p> <p>Using both <code>VAULT[...]</code> and <code>VAULT_NOCACHE[...]</code> on the same path is not recommended because the order in which these invocations are processed is not deterministic.</p>"},{"location":"guide/writing-tasks/#cron-builds","title":"Cron Builds","text":"<p>It is possible to configure invocations of re-occurring builds via the well-known Cron expressions. Cron builds can be configured on a repository's settings page (not in <code>.cirrus.yml</code>).</p> <p>It's possible to configure several cron builds with unique <code>names</code> which will be available via <code>CIRRUS_CRON</code> environment variable. Each cron build should specify branch to trigger new builds for and a cron expression compatible with Quartz. You can use this generator to generate/validate your expressions.</p> <p>Note: Cron Builds are timed with the UTC timezone.</p>"},{"location":"guide/writing-tasks/#matrix-modification","title":"Matrix Modification","text":"<p>Sometimes it's useful to run the same task against different software versions. Or run different batches of tests based on an environment variable. For cases like these, the <code>matrix</code> modifier comes very handy. It's possible to use <code>matrix</code> keyword only inside of a particular task to have multiple tasks based on the original one. Each new task will be created from the original task by replacing the whole <code>matrix</code> YAML node with each <code>matrix</code>'s children separately.</p> <p>Let check an example of a <code>.cirrus.yml</code>:</p> amd64arm64 <pre><code>test_task:\n  container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  test_script: yarn run test\n</code></pre> <pre><code>test_task:\n  arm_container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  test_script: yarn run test\n</code></pre> <p>Which will be expanded into:</p> amd64arm64 <pre><code>test_task:\n  container:\n    image: node:latest\n  test_script: yarn run test\n\ntest_task:\n  container:\n    image: node:lts\n  test_script: yarn run test\n</code></pre> <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  test_script: yarn run test\n\ntest_task:\n  arm_container:\n    image: node:lts\n  test_script: yarn run test\n</code></pre> <p>Tip</p> <p>The <code>matrix</code> modifier can be used multiple times within a task.</p> <p>The <code>matrix</code> modification makes it easy to create some pretty complex testing scenarios like this:</p> amd64arm64 <pre><code>task:\n  container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script:\n      - node --version\n      - cat yarn.lock\n    populate_script: yarn install\n  matrix:\n    - name: Build\n      build_script: yarn build\n    - name: Test\n      test_script: yarn run test\n</code></pre> <pre><code>task:\n  arm_container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script:\n      - node --version\n      - cat yarn.lock\n    populate_script: yarn install\n  matrix:\n    - name: Build\n      build_script: yarn build\n    - name: Test\n      test_script: yarn run test\n</code></pre> <p>Another example showing how to create 2 tasks with different environment variables:</p> <pre><code>task:\n  name: matrixdemo\n  container:\n    image: alpine:3.20\n  env:\n    matrix:\n      FIRST_VAR: foo\n      FIRST_VAR: bar\n      FIRST_VAR: baz\n    matrix:\n      SECOND_VAR: alpha\n      SECOND_VAR: bravo\n  print_script:\n    - echo \"FIRST_VAR value is $FIRST_VAR\"\n    - echo \"SECOND_VAR value is $SECOND_VAR\"\n</code></pre> <p>The above will generate 6 tasks.</p> <p>And yet another example, showing how to granularly control the environment variables for each matrix:</p> <pre><code>task:\n  name: matrixdemo\n  container:\n    image: ubuntu:24.04\n  env:\n    # Android Command-line tools were obtained with https://stackoverflow.com/a/78890086/7009800\n    matrix:\n      - ANDROID_CLT_VERSION: 9123335 # v8, latest compatible with JDK 8+\n        JDK_PACKAGE: openjdk-8-jdk\n      - ANDROID_CLT_VERSION: 9862592 # v10, latest compatible with JDK 11+\n        JDK_PACKAGE: openjdk-11-jdk\n      - ANDROID_CLT_VERSION: 11479570 # v13, latest compatible with JDK 17+\n        JDK_PACKAGE: openjdk-17-jdk\n      - ANDROID_CLT_VERSION: 11479570 # v13, latest compatible with JDK 17+\n        JDK_PACKAGE: openjdk-21-jdk\n      info_script:\n        - echo \"Building Android SDK with Android Command-line tools $ANDROID_CLT_VERSION and JDK $JDK_PACKAGE\"\n</code></pre> <p>The above will generate 4 tasks.</p>"},{"location":"guide/writing-tasks/#task-execution-dependencies","title":"Task Execution Dependencies","text":"<p>Sometimes it might be very handy to execute some tasks only after successful execution of other tasks. For such cases it is possible to specify task names that a particular task depends. Use <code>depends_on</code> keyword to define dependencies:</p> amd64arm64 <pre><code>container:\n  image: node:latest\n\nlint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  depends_on:\n    - test\n    - lint\n  script: yarn run publish\n</code></pre> <pre><code>arm_container:\n  image: node:latest\n\nlint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  depends_on:\n    - test\n    - lint\n  script: yarn run publish\n</code></pre> Task Names and Aliases <p>It is possible to specify the task's name via the <code>name</code> field. <code>lint_task</code> syntax is a syntactic sugar that will be expanded into:</p> <pre><code>task:\n  name: lint\n  ...\n</code></pre> <p>Names can be also pretty complex:</p> <pre><code>task:\n  name: Test Shard $TESTS_SPLIT\n  env:\n    matrix:\n      TESTS_SPLIT: 1/3\n      TESTS_SPLIT: 2/2\n      TESTS_SPLIT: 3/3\n  tests_script: ./.ci/tests.sh\n\ndeploy_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  depends_on:\n    - Test Shard 1/3\n    - Test Shard 2/3\n    - Test Shard 3/3\n  script: ./.ci/deploy.sh\n  ...\n</code></pre> <p>Complex task names make it difficult to list and maintain all of such task names in your <code>depends_on</code> field. In order to  make it simpler you can use the <code>alias</code> field to have a short simplified name for several tasks to use in <code>depends_on</code>.</p> <p>Here is a modified version of an example above that leverages the <code>alias</code> field:</p> <pre><code>task:\n  name: Test Shard $TESTS_SPLIT\n  alias: Tests\n  env:\n    matrix:\n      TESTS_SPLIT: 1/3\n      TESTS_SPLIT: 2/2\n      TESTS_SPLIT: 3/3\n  tests_script: ./.ci/tests.sh\n\ndeploy_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  depends_on: Tests\n  script: ./.ci/deploy.sh\n</code></pre>"},{"location":"guide/writing-tasks/#conditional-task-execution","title":"Conditional Task Execution","text":"<p>Some tasks are meant to be created only if a certain condition is met. And some tasks can be skipped in some cases. Cirrus CI supports the <code>only_if</code> and <code>skip</code> keywords in order to provide such flexibility:</p> <ul> <li> <p>The <code>only_if</code> keyword controls whether or not a task will be created. For example, you may want to publish only changes   committed to the <code>master</code> branch.   <pre><code>publish_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  script: yarn run publish\n</code></pre></p> </li> <li> <p>The <code>skip</code> keyword allows to skip execution of a task and mark it as successful. For example, you may want to skip linting   if no source files have changed since the last successful run.   <pre><code>lint_task:\n  skip: \"!changesInclude('.cirrus.yml', '**.{js,ts}')\"\n  script: yarn run lint\n</code></pre></p> </li> </ul> <p>Skip CI Completely</p> <p>Just include <code>[skip ci]</code> or <code>[skip cirrus]</code> in the first line or last line of your commit message in order to skip CI execution for a commit completely.</p> <p>If you push multiple commits at the same time, only the last commit message will be checked for <code>[skip ci]</code> or <code>[ci skip]</code>.</p> <p>If you open a PR, PR title will be checked for <code>[skip ci]</code> or <code>[ci skip]</code> instead of the last commit message on the PR branch.</p>"},{"location":"guide/writing-tasks/#supported-operators","title":"Supported Operators","text":"<p>Currently only basic operators like <code>==</code>, <code>!=</code>, <code>=~</code>, <code>!=~</code>, <code>&amp;&amp;</code>, <code>||</code> and unary <code>!</code> are supported in <code>only_if</code> and <code>skip</code> expressions. Environment variables can also be used as usual.</p> <p>Pattern Matching Example</p> <p>Use <code>=~</code> operator for pattern matching.</p> <pre><code>check_aggreement_task:\n  only_if: $CIRRUS_BRANCH =~ 'pull/.*'\n</code></pre> <p>Note that <code>=~</code> operator can match against multiline values (dotall mode) and therefore looking for the exact occurrence of the regular expression so don't forget to use <code>.*</code> around your term for matching it at any position (for example, <code>$CIRRUS_CHANGE_TITLE =~ '.*\\[docs\\].*'</code>).</p>"},{"location":"guide/writing-tasks/#supported-functions","title":"Supported Functions","text":"<p>Currently two functions are supported in the <code>only_if</code> and <code>skip</code> expressions:</p> <ul> <li><code>changesInclude</code> function allows to check which files were changed</li> <li><code>changesIncludeOnly</code> is a more strict version of <code>changesInclude</code>, i.e. it won't evaluate to <code>true</code> if there are changed files other than the ones covered by patterns</li> </ul> <p>These two functions behave differently for PR builds and regular builds:</p> <ul> <li>For PR builds, functions check the list of files affected by the PR.</li> <li>For regular builds, the <code>CIRRUS_LAST_GREEN_CHANGE</code> environment variable   will be used to determine list of affected files between <code>CIRRUS_LAST_GREEN_CHANGE</code> and <code>CIRRUS_CHANGE_IN_REPO</code>.   In case <code>CIRRUS_LAST_GREEN_CHANGE</code> is not available (either it's a new branch or there were no passing builds before),   list of files affected by a commit associated with <code>CIRRUS_CHANGE_IN_REPO</code> environment variable will be used instead.</li> </ul> <p><code>changesInclude</code> function can be very useful for skipping some tasks when no changes to sources have been made since the last successful Cirrus CI build.</p> <pre><code>lint_task:\n  skip: \"!changesInclude('.cirrus.yml', '**.{js,ts}')\"\n  script: yarn run lint\n</code></pre> <p><code>changesIncludeOnly</code> function can be used to skip running a heavyweight task if only documentation was changed, for example:</p> <pre><code>build_task:\n  skip: \"changesIncludeOnly('doc/*')\"\n</code></pre>"},{"location":"guide/writing-tasks/#auto-cancellation-of-tasks","title":"Auto-Cancellation of Tasks","text":"<p>Cirrus CI can automatically cancel tasks in case of new pushes to the same branch. By default, Cirrus CI auto-cancels all tasks for non default branch (for most repositories <code>master</code> branch) but this behavior can be changed by specifying <code>auto_cancellation</code> field:</p> <pre><code>task:\n  auto_cancellation: $CIRRUS_BRANCH != 'master' &amp;&amp; $CIRRUS_BRANCH !=~ 'release/.*'\n  ...\n</code></pre>"},{"location":"guide/writing-tasks/#stateful-tasks","title":"Stateful Tasks","text":"<p>It's possible to tell Cirrus CI that a certain task is stateful and Cirrus CI will use a slightly different scheduling algorithm to minimize chances of such tasks being interrupted. Stateful tasks are intended to use low CPU count. Scheduling times of such stateful tasks might be a bit longer then usual especially for tasks with high CPU requirements.</p> <p>By default, Cirrus CI marks a task as stateful if its name contains one of the following terms: <code>deploy</code>, <code>push</code>, <code>publish</code>,  <code>upload</code> or <code>release</code>. Otherwise, you can explicitly mark a task as stateful via <code>stateful</code> field:</p> <pre><code>task:\n  name: Propagate to Production\n  stateful: true\n  ...\n</code></pre>"},{"location":"guide/writing-tasks/#failure-toleration","title":"Failure Toleration","text":"<p>Sometimes tasks can play a role of sanity checks. For example, a task can check that your library is working with the latest nightly version of some dependency package. It will be great to be notified about such failures but it's not necessary to fail the whole build when a failure occurs. Cirrus CI has the <code>allow_failures</code> keyword which will make a task to not affect the overall status of a build.</p> <pre><code>test_nightly_task:\n  allow_failures: $SOME_PACKAGE_DEPENDENCY_VERSION == 'nightly'\n</code></pre> <p>Skipping Notifications</p> <p>You can also skip posting red statuses to GitHub via <code>skip_notifications</code> field.</p> <pre><code>skip_notifications: $SOME_PACKAGE_DEPENDENCY_VERSION == 'nightly'\n</code></pre> <p>It can help to track potential issues overtime without distracting the main workflow.</p>"},{"location":"guide/writing-tasks/#manual-tasks","title":"Manual tasks","text":"<p>By default a Cirrus CI task is automatically triggered when all its dependency tasks finished successfully. Sometimes though, it can be very handy to trigger some tasks manually, for example, perform a deployment to staging for manual testing upon all automation checks have succeeded. In order change the default behavior please use <code>trigger_type</code> field like this:</p> <pre><code>task:\n  name: \"Staging Deploy\"\n  trigger_type: manual\n  depends_on:\n    - Tests (Unit)\n    - Tests (Ingegration)\n    - Lint\n</code></pre> <p>You'll be able to manually trigger such paused tasks via Cirrus CI Web UI or directly from GitHub Checks page.</p>"},{"location":"guide/writing-tasks/#task-execution-lock","title":"Task Execution Lock","text":"<p>Some CI tasks perform external operations which are required to be executed one at a time. For example, parallel deploys to the same environment is usually a bad idea. In order to restrict parallel execution of a certain task within a repository, you can use <code>execution_lock</code> to specify a task's lock key, a unique string that will be used to make sure that any tasks with the same <code>execution_lock</code> string are executed one at a time. Here is an example of how to make sure deployments  on a specific branch can not run in parallel:</p> <pre><code>task:\n  name: \"Automatic Staging Deploy\"\n  execution_lock: $CIRRUS_BRANCH\n</code></pre> <p>You'll be able to manually trigger such paused tasks via the Cirrus CI Web Dashboard or directly from the commit's <code>checks</code> page on GitHub.</p>"},{"location":"guide/writing-tasks/#required-pr-labels","title":"Required PR Labels","text":"<p>Similar to manual tasks Cirrus CI can pause execution of tasks until a corresponding PR gets labeled. This can be particular useful when you'd like to do an initial review before running all unit and integration tests on every supported platform. Use the <code>required_pr_labels</code> field to specify a list of labels a PR requires to have in order to trigger a task. Here is a simple example of <code>.cirrus.yml</code> config that automatically runs a linting tool but requires <code>initial-review</code> label being presented in order to run tests:</p> <pre><code>lint_task:\n  # ...\n\ntest_task:\n  required_pr_labels: initial-review\n  # ...\n</code></pre> <p>Note: <code>required_pr_labels</code> has no effect on tasks created for non-PR builds.</p> <p>You can also require multiple labels to continue executing the task for even more flexibility:</p> <pre><code>deploy_task:\n  required_pr_labels: \n    - initial-review\n    - ready-for-staging\n  depends_on: build\n  # ...\n</code></pre> <p>In the example above both <code>initial-review</code> and <code>ready-for-staging</code> labels should be presented on a PR in order to perform a deployment via <code>deploy</code> task.</p>"},{"location":"guide/writing-tasks/#http-cache","title":"HTTP Cache","text":"<p>For the most cases regular caching mechanism where Cirrus CI caches a folder is more than enough. But modern build systems like Gradle, Bazel and Pants can take advantage of remote caching. Remote caching is when a build system uploads and downloads intermediate results of a build execution while the build itself is still executing.</p> <p>Cirrus CI agent starts a local caching server and exposes it via <code>CIRRUS_HTTP_CACHE_HOST</code> environments variable. Caching server supports <code>GET</code>, <code>POST</code>, <code>HEAD</code> and <code>DELETE</code> requests to upload, download, check presence and delete artifacts.</p> <p>Info</p> <p>If port <code>12321</code> is available <code>CIRRUS_HTTP_CACHE_HOST</code> will be equal to <code>localhost:12321</code>.</p> <p>For example running the following command:</p> <pre><code>curl -s -X POST --data-binary @myfolder.tar.gz http://$CIRRUS_HTTP_CACHE_HOST/name-key\n</code></pre> <p>...has the same effect as the following caching instruction:</p> <pre><code>name_cache:\n  folder: myfolder\n  fingerprint_key: key\n</code></pre> <p>Info</p> <p>To see how HTTP Cache can be used with Gradle's Build Cache please check this example.</p>"},{"location":"guide/writing-tasks/#additional-containers","title":"Additional Containers","text":"<p>Sometimes one container is not enough to run a CI build. For example, your application might use a MySQL database as a storage. In this case you most likely want a MySQL instance running for your tests.</p> <p>One option here is to pre-install MySQL and use a <code>background_script</code> to start it. This approach has some inconveniences like the need to pre-install MySQL by building a custom Docker container.</p> <p>For such use cases Cirrus CI allows to run additional containers in parallel with the main container that executes a task. Each additional container is defined under <code>additional_containers</code> keyword in <code>.cirrus.yml</code>. Each additional container should have a unique <code>name</code> and specify at least a container <code>image</code>.</p> <p>Normally, you would also specify a <code>port</code> (or <code>ports</code>, if there are many) to instruct the Cirrus CI to configure the networking between the containers and wait for the ports to be available before running the task. Additional containers do not inherit environment variables because they are started before the main task receives it's environment variables.</p> <p>In the example below we use an official MySQL Docker image that exposes the standard MySQL port (3306). Tests will be able to access MySQL instance via <code>localhost:3306</code>.</p> amd64arm64 <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      cpu: 1.0\n      memory: 512Mi\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre> <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      cpu: 1.0\n      memory: 512Mi\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre> <p>Additional container can be very handy in many scenarios. Please check Cirrus CI catalog of examples for more details.</p> Default Resources <p>By default, each additional container will get <code>0.5</code> CPU and <code>512Mi</code> of memory. These values can be configured as usual via <code>cpu</code> and <code>memory</code> fields.</p> Port Mapping <p>It's also possible to map ports of additional containers by using <code>&lt;HOST_PORT&gt;:&lt;CONTAINER_PORT&gt;</code> format for the <code>port</code> field. For example, <code>port: 80:8080</code> will map port <code>8080</code> of the container to be available on local port <code>80</code> within a task.</p> <p>Note: don't use port mapping unless absolutely necessary. A perfect use case is when you have several additional containers which start the service on the same port and there's no easy way to change that. Port mapping limits  the number of places the container can be scheduled and will affect how fast such tasks are scheduled.</p> <p>To specify multiple mappings use the <code>ports</code> field, instead of the <code>port</code>: <pre><code>ports:\n  - 8080\n  - 3306\n</code></pre></p> Overriding Default Command <p>It's also possible to override the default <code>CMD</code> of an additional container via <code>command</code> field:</p> amd64arm64 <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 7777\n      command: mysqld --port 7777\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre> <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 7777\n      command: mysqld --port 7777\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre> Warning <p>Note that <code>additional_containers</code> can be used only with the Linux Clusters, a GKE cluster or a EKS cluster.</p>"},{"location":"guide/writing-tasks/#embedded-badges","title":"Embedded Badges","text":"<p>Cirrus CI provides a way to embed a badge that can represent status of your builds into a ReadMe file or a website.</p> <p>For example, this is a badge for <code>cirruslabs/cirrus-ci-web</code> repository that contains Cirrus CI's front end: </p> <p>In order to embed such a check into a \"read-me\" file or your website, just use a URL to a badge that looks like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg\n</code></pre> <p>If you want a badge for a particular branch, use the <code>?branch=&lt;BRANCH NAME&gt;</code> query parameter (at the end of the URL) like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?branch=&lt;BRANCH NAME&gt;\n</code></pre> <p>By default, Cirrus picks the latest build in a final state for the repository or a particular branch if <code>branch</code> parameter is specified. It's also possible to explicitly set a concrete build to use with <code>?buildId=&lt;BUILD ID&gt;</code> query parameter.</p> <p>If you want a badge for a particular task within the latest finished build, use the <code>?task=&lt;TASK NAME&gt;</code> query parameter (at the end of the URL) like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?task=tests\n</code></pre> <p>You can even pick a specific script instruction within the task with an additional <code>script=&lt;SCRIPT NAME&gt;</code> parameter:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?task=build&amp;script=lint\n</code></pre>"},{"location":"guide/writing-tasks/#badges-in-markdown","title":"Badges in Markdown","text":"<p>Here is how Cirrus CI's badge can be embeded in a Markdown file:</p> <pre><code>[![Build Status](https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg)](https://cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;)\n</code></pre>"},{"location":"guide/writing-tasks/#cctray-xml","title":"CCTray XML","text":"<p>Cirrus CI supports exporting information about the latest repository builds via the CCTray XML format, using the following URL format:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/cctray.xml\n</code></pre> <p>Some tools with support of CCtray are:</p> <ul> <li>CCMenu (macOS Native build status monitor).</li> <li>Barklarm (Open Source multiplatform alarm and build status monitor).</li> <li>Nevergreen (Build radiation service).</li> </ul> <p>Note: for private repositories you'll need to configure access token.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/category/announcement/","title":"announcement","text":""},{"location":"blog/category/macos/","title":"macos","text":""},{"location":"blog/category/terminal/","title":"terminal","text":""},{"location":"blog/category/cirrus-cli/","title":"cirrus-cli","text":""},{"location":"blog/category/workers/","title":"workers","text":""},{"location":"blog/category/github/","title":"github","text":""},{"location":"blog/category/aws/","title":"aws","text":""},{"location":"blog/category/docker/","title":"docker","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/category/announcement/page/2/","title":"announcement","text":""}]}